{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cookbook, we will learn how to use Opsmate to manage knowledge.\n",
    "\n",
    "Notes the knowledge management feature is currently in the early stage of development, the features and the UX are subject to change.\n",
    "At the moment 2 type of data source can be ingested as knowledge:\n",
    "\n",
    "1. Any text based files from your local file system or network-attached storage.\n",
    "2. Any text based files from Github repositories.\n",
    "\n",
    "We use [lancedb](https://lancedb.github.io/lancedb/) as the underlying vector database to store the knowledge. We use lancedb mainly because of the serverless nature of the database where you can use the cloud storage as the backend, which reduces the cost of ownership.\n",
    "\n",
    "Knowledge retrieval is can be achieved via the `KnowledgeRetrival` tool - which is a built-in tool in Opsmate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment variablebased configuration Options\n",
    "\n",
    "### FS_EMBEDDINGS_CONFIG\n",
    "\n",
    "This is a JSON key-value pair where the key is the path to the directory to be ingested and the value is the glob pattern to match the files to be ingested.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "```\n",
    "FS_EMBEDDINGS_CONFIG='{\"./docs/cookbooks\": \"*.md\"}'\n",
    "```\n",
    "\n",
    "This will ingest all the markdown files in the `./docs/cookbooks` directory.\n",
    "\n",
    "### GITHUB_EMBEDDINGS_CONFIG\n",
    "\n",
    "This is a JSON key-value pair where the key is the `owner/repo:optional[branch]` and the value is the glob pattern to match the files to be ingested.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "```\n",
    "GITHUB_EMBEDDINGS_CONFIG='{\"opsmate/opsmate\": \"*.md\", \"kubernetes/kubernetes:test-branch\": \"*.txt\"}'\n",
    "```\n",
    "\n",
    "In the example above, the first entry will ingest all the markdown files in the `opsmate/opsmate` repository. The second entry will ingest all the text files in the `kubernetes/kubernetes` repository on the `test-branch` branch.\n",
    "\n",
    "If the branch is not specified, it will default to `main`.\n",
    "\n",
    ":important: The Github token is required to be set in the environment variable `GITHUB_TOKEN`.\n",
    "\n",
    "\n",
    "### EMBEDDING_REGISTRY_NAME and EMBEDDING_MODEL_NAME\n",
    "\n",
    "`EMBEDDING_REGISTRY_NAME` is the name of the embedding registry to use. It is default to `openai`.\n",
    "\n",
    "`EMBEDDING_MODEL_NAME` is the name of the embedding model to use. It is default to `text-embedding-ada-002`.\n",
    "\n",
    "LanceDB supports wide range of embedding models, you can refer to the [lancedb embedding documentation](https://lancedb.github.io/lancedb/embeddings/default_embedding_functions/#text-embedding-functions) for more details.\n",
    "\n",
    "### EMBEDDINGS_DB_PATH\n",
    "\n",
    "EMBEDDINGS_DB_PATH is the path to the lancedb database. It is default to `~/.data/opsmate-embeddings`.\n",
    "\n",
    "Right now it is defaulted to the local file system, but there are wide range of storage options supported by lancedb, you can refer to the [lancedb storage documentation](https://lancedb.github.io/lancedb/concepts/storage/) for more details. In the documentation it provides a very comprehensive diagram to show case the thought process that goes into choosing the right storage backend.\n",
    "\n",
    "**WARNING**: Currently the ingestion chunk size is set to 1000, with overlap set to 0, with recursive text splitter as the default chunking strategy. This is hardcoded right now through environment-variable based configuration, but we will support more flexible configuration in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDK-based data ingestion\n",
    "\n",
    "You can also choose to ingest the knowledge via the SDK which provides greater flexibility in terms of configuration.\n",
    "\n",
    "In the example below, we ingest all the markdown files in the `docs/book/src` directory of the `kubernetes-sigs/kubebuilder` repository to learn about the kubebuilder.\n",
    "\n",
    "Note this is going to take a while to complete and emit a lot of logs so we are not going to run it here.\n",
    "\n",
    "```python\n",
    "from opsmate.ingestions import ingest_from_config\n",
    "from opsmate.libs.config import Config\n",
    "\n",
    "await ingest_from_config(Config(\n",
    "  github_embeddings_config={\n",
    "    \"kubernetes-sigs/kubebuilder:master\": \"./docs/book/src/**/*.md\"\n",
    "  },\n",
    "  categorise=False, # By default we categorise the knowledge into categories for better segmentation, but we disable it here for the sake of speed.\n",
    "))\n",
    "```\n",
    "\n",
    "Once the knowledge of the kubebuilder is ingested, we can use the `KnowledgeRetrieval` tool to provide retrieval augmented generation (RAG):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-01-16 22:33:09\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mrunning knowledge retrieval tool\u001b[0m \u001b[36mquery\u001b[0m=\u001b[35mhow to do env test against a real cluster in kubebuilder using environment variables?\u001b[0m\n",
      "To run integration tests using `envtest` against an existing cluster in Kubebuilder, you can follow these steps utilizing environment variables:\n",
      "\n",
      "1. **Set Environment Variables**: You need to specify several environment variables before executing your tests. Key variables include:\n",
      "   - `USE_EXISTING_CLUSTER`: Set this to `true` to connect to an existing cluster instead of setting up a local one.\n",
      "   - `KUBEBUILDER_ASSETS`: Point this to a directory containing the necessary binaries (`kube-apiserver`, `etcd`, `kubectl`). This should be set if you want the tests to use specific binaries rather than default ones.\n",
      "   - Alternatively, you can set specific paths for the binaries:\n",
      "     - `TEST_ASSET_KUBE_APISERVER`: Path to the `kube-apiserver` binary.\n",
      "     - `TEST_ASSET_ETCD`: Path to the `etcd` binary.\n",
      "     - `TEST_ASSET_KUBECTL`: Path to the `kubectl` binary.\n",
      "\n",
      "2. **Configure the Test Environment**: In your test setup, youâ€™ll need to configure the `envtest` environment to utilize the existing cluster. The following code snippet shows how to do this:\n",
      "   ```go\n",
      "   testEnv = &envtest.Environment{\n",
      "       UseExistingCluster: true,\n",
      "       CRDDirectoryPaths:  []string{filepath.Join(\"..\", \"config\", \"crd\", \"bases\")},\n",
      "   }\n",
      "   ```\n",
      "\n",
      "3. **Starting and Stopping the Test Environment**: After your environment is configured, start the test environment and ensure you handle any errors:\n",
      "   ```go\n",
      "   cfg, err := testEnv.Start()\n",
      "   if err != nil {\n",
      "       log.Fatalf(\"Failed to start test environment: %v\", err)\n",
      "   }\n",
      "   defer testEnv.Stop()\n",
      "   ```\n",
      "\n",
      "4. **Running Your Tests**: Once the environment is set up and started, you can write your test logic and run it against the existing cluster. Any changes to the state of resources in the cluster will be reflected during these tests, allowing for a more realistic test scenario.\n",
      "\n",
      "Be sure to export the necessary environment variables in your terminal or within your test files before executing your tests to ensure they are picked up appropriately.\n"
     ]
    }
   ],
   "source": [
    "from opsmate.tools import KnowledgeRetrieval\n",
    "\n",
    "result = await KnowledgeRetrieval(query=\"how to do env test against a real cluster in kubebuilder using environment variables?\").run()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future capabilities:\n",
    "\n",
    "* Right now the async based knowledge ingestion is fairly naive and is not designed to be run in a distributed and fault-tolerant manner. We need to design a more robust system to support this - Potentially brining in the big gun such as [Celery](https://docs.celeryq.dev/en/stable/) but ideally anything easy to maintain and scale.\n",
    "* We need to support more data source types, such as databases or other API-based data sources.\n",
    "* Currently only text-based files are supported, we need to support more file types, such as images, videos, and other binary data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
