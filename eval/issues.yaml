-   fix_verification:
    -   command: kubectl -n test-namespace-e5f1f get pods -l app=test-app -o jsonpath='{.items[0].status.phase}'
        exit_code: 0
        expected_output: Running
    -   command: kubectl -n test-namespace-e5f1f describe pod -l app=test-app | grep
            'Readiness probe'
        exit_code: 0
        expected_output: 'Readiness probe failed: Get "http://10.1.0.1:80/healthy":
            dial tcp 10.1.0.1:80: connect: connection refused'
    issue_produced_verification:
    -   command: kubectl -n test-namespace-e5f1f get pods -l app=test-app -o jsonpath='{.items[0].status.containerStatuses[0].state.waiting.reason}'
        exit_code: 0
        expected_output: CrashLoopBackOff
    namespace: test-namespace-e5f1f
    question: The pod is not running properly and is in a CrashLoopBackOff state,
        what could be the cause?
    root_cause: The container is running a readiness probe that is failing, causing
        the pod to crash and restart repeatedly. The probe is checking an endpoint
        that does not return a successful response.
    steps_to_create_issue:
    -   description: Create a Deployment with a container that has a failing readiness
            probe
        manifest: "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-deployment\n\
            \  namespace: test-namespace-e5f1f\nspec:\n  replicas: 1\n  selector:\n\
            \    matchLabels:\n      app: test-app\n  template:\n    metadata:\n \
            \     labels:\n        app: test-app\n    spec:\n      containers:\n \
            \     - name: test-container\n        image: httpd:2.4\n        readinessProbe:\n\
            \          httpGet:\n            path: /healthy\n            port: 80\n\
            \          initialDelaySeconds: 2\n          periodSeconds: 5"
