{"query": "A user asked the following question:\nQuestion: What fields do I need to include when writing a Job spec in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Writing a Job spec\nRunbook Content: Writing a Job specAs with all other Kubernetes config, a Job needs `apiVersion`, `kind`, and `metadata` fields.  \nWhen the control plane creates new Pods for a Job, the `.metadata.name` of the\nJob is part of the basis for naming those Pods. The name of a Job must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\nEven when the name is a DNS subdomain, the name must be no longer than 63\ncharacters.  \nA Job also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\n", "relevant_passages": ["What fields do I need to include when writing a Job spec in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: How can I check if my deployment rollback was successful?\nThis is about the following runbook:\nRunbook Title: Rolling Back to a Previous Revision\nRunbook Content: Rolling Back a DeploymentRolling Back to a Previous RevisionFollow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.  \n1. Now you've decided to undo the current rollout and rollback to the previous revision:\n```shell\nkubectl rollout undo deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment rolled back\n```\nAlternatively, you can rollback to a specific revision by specifying it with `--to-revision`:  \n```shell\nkubectl rollout undo deployment/nginx-deployment --to-revision=2\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment rolled back\n```  \nFor more details about rollout related commands, read [`kubectl rollout`](/docs/reference/generated/kubectl/kubectl-commands#rollout).  \nThe Deployment is now rolled back to a previous stable revision. As you can see, a `DeploymentRollback` event\nfor rolling back to revision 2 is generated from Deployment controller.  \n2. Check if the rollback was successful and the Deployment is running as expected, run:\n```shell\nkubectl get deployment nginx-deployment\n```  \nThe output is similar to this:\n```\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           30m\n```\n3. Get the description of the Deployment:\n```shell\nkubectl describe deployment nginx-deployment\n```\nThe output is similar to this:\n```\nName:                   nginx-deployment\nNamespace:              default\nCreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500\nLabels:                 app=nginx\nAnnotations:            deployment.kubernetes.io/revision=4\nkubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\nSelector:               app=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx\nContainers:\nnginx:\nImage:        nginx:1.16.1\nPort:         80/TCP\nHost Port:    0/TCP\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    NewReplicaSetAvailable\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)\nEvents:\nType    Reason              Age   From                   Message\n----    ------              ----  ----                   -------\nNormal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1\nNormal  DeploymentRollback  15s   deployment-controller  Rolled back deployment \"nginx-deployment\" to revision 2\nNormal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0\n```\n", "relevant_passages": ["How can I check if my deployment rollback was successful?"]}
{"query": "A user asked the following question:\nQuestion: What happens to persistent data in a Pod if a container restarts?\nThis is about the following runbook:\nRunbook Title: Storage in Pods {#pod-storage}\nRunbook Content: Resource sharing and communicationStorage in Pods {#pod-storage}A Pod can specify a set of shared storage\n{{< glossary_tooltip text=\"volumes\" term_id=\"volume\" >}}. All containers\nin the Pod can access the shared volumes, allowing those containers to\nshare data. Volumes also allow persistent data in a Pod to survive\nin case one of the containers within needs to be restarted. See\n[Storage](/docs/concepts/storage/) for more information on how\nKubernetes implements shared storage and makes it available to Pods.\n", "relevant_passages": ["What happens to persistent data in a Pod if a container restarts?"]}
{"query": "A user asked the following question:\nQuestion: What happens when I scale down an Elastic Indexed Job?\nThis is about the following runbook:\nRunbook Title: Elastic Indexed Jobs\nRunbook Content: Advanced usageElastic Indexed Jobs{{< feature-state feature_gate_name=\"ElasticIndexedJob\" >}}  \nYou can scale Indexed Jobs up or down by mutating both `.spec.parallelism`\nand `.spec.completions` together such that `.spec.parallelism == .spec.completions`.\nWhen scaling down, Kubernetes removes the Pods with higher indexes.  \nUse cases for elastic Indexed Jobs include batch workloads which require\nscaling an indexed Job, such as MPI, Horovod, Ray, and PyTorch training jobs.\n", "relevant_passages": ["What happens when I scale down an Elastic Indexed Job?"]}
{"query": "A user asked the following question:\nQuestion: What happens to a Pod after it's created?\nThis is about the following runbook:\nRunbook Title: Working with Pods\nRunbook Content: Working with PodsYou'll rarely create individual Pods directly in Kubernetes\u2014even singleton Pods. This\nis because Pods are designed as relatively ephemeral, disposable entities. When\na Pod gets created (directly by you, or indirectly by a\n{{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}), the new Pod is\nscheduled to run on a {{< glossary_tooltip term_id=\"node\" >}} in your cluster.\nThe Pod remains on that node until the Pod finishes execution, the Pod object is deleted,\nthe Pod is *evicted* for lack of resources, or the node fails.  \n{{< note >}}\nRestarting a container in a Pod should not be confused with restarting a Pod. A Pod\nis not a process, but an environment for running container(s). A Pod persists until\nit is deleted.\n{{< /note >}}  \nThe name of a Pod must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostname.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\n", "relevant_passages": ["What happens to a Pod after it's created?"]}
{"query": "A user asked the following question:\nQuestion: Where can I find more details about downward API volumes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}You can read about [`downwardAPI` volumes](/docs/concepts/storage/volumes/#downwardapi).  \nYou can try using the downward API to expose container- or Pod-level information:\n* as [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)\n* as [files in `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)\n", "relevant_passages": ["Where can I find more details about downward API volumes?"]}
{"query": "A user asked the following question:\nQuestion: What is the purpose of the Headless Service in a StatefulSet?\nThis is about the following runbook:\nRunbook Title: Components\nRunbook Content: ComponentsThe example below demonstrates the components of a StatefulSet.  \n```yaml\napiVersion: v1\nkind: Service\nmetadata:\nname: nginx\nlabels:\napp: nginx\nspec:\nports:\n- port: 80\nname: web\nclusterIP: None\nselector:\napp: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nname: web\nspec:\nselector:\nmatchLabels:\napp: nginx # has to match .spec.template.metadata.labels\nserviceName: \"nginx\"\nreplicas: 3 # by default is 1\nminReadySeconds: 10 # by default is 0\ntemplate:\nmetadata:\nlabels:\napp: nginx # has to match .spec.selector.matchLabels\nspec:\nterminationGracePeriodSeconds: 10\ncontainers:\n- name: nginx\nimage: registry.k8s.io/nginx-slim:0.24\nports:\n- containerPort: 80\nname: web\nvolumeMounts:\n- name: www\nmountPath: /usr/share/nginx/html\nvolumeClaimTemplates:\n- metadata:\nname: www\nspec:\naccessModes: [ \"ReadWriteOnce\" ]\nstorageClassName: \"my-storage-class\"\nresources:\nrequests:\nstorage: 1Gi\n```  \n{{< note >}}\nThis example uses the `ReadWriteOnce` access mode, for simplicity. For\nproduction use, the Kubernetes project recommends using the `ReadWriteOncePod`\naccess mode instead.\n{{< /note >}}  \nIn the above example:  \n* A Headless Service, named `nginx`, is used to control the network domain.\n* The StatefulSet, named `web`, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.\n* The `volumeClaimTemplates` will provide stable storage using\n[PersistentVolumes](/docs/concepts/storage/persistent-volumes/) provisioned by a\nPersistentVolume Provisioner.  \nThe name of a StatefulSet object must be a valid\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\n", "relevant_passages": ["What is the purpose of the Headless Service in a StatefulSet?"]}
{"query": "A user asked the following question:\nQuestion: How can I learn more about Pods in Kubernetes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn more about [Pods](/docs/concepts/workloads/pods).\n* [Run a stateless application using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/).\n* Read the {{< api-reference page=\"workload-resources/deployment-v1\" >}} to understand the Deployment API.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how\nyou can use it to manage application availability during disruptions.\n* Use kubectl to [create a Deployment](/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/).\n", "relevant_passages": ["How can I learn more about Pods in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: How does the TTL mechanism work for finished Jobs in Kubernetes?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- janetkuo\ntitle: Automatic Cleanup for Finished Jobs\ncontent_type: concept\nweight: 70\ndescription: >-\nA time-to-live mechanism to clean up old Jobs that have finished execution.\n---  \n<!-- overview -->  \n{{< feature-state for_k8s_version=\"v1.23\" state=\"stable\" >}}  \nWhen your Job has finished, it's useful to keep that Job in the API (and not immediately delete the Job)\nso that you can tell whether the Job succeeded or failed.  \nKubernetes' TTL-after-finished {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} provides a\nTTL (time to live) mechanism to limit the lifetime of Job objects that\nhave finished execution.  \n<!-- body -->\n", "relevant_passages": ["How does the TTL mechanism work for finished Jobs in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I set the revision history limit to zero?\nThis is about the following runbook:\nRunbook Title: Revision History Limit\nRunbook Content: Writing a Deployment SpecRevision History LimitA Deployment's revision history is stored in the ReplicaSets it controls.  \n`.spec.revisionHistoryLimit` is an optional field that specifies the number of old ReplicaSets to retain\nto allow rollback. These old ReplicaSets consume resources in `etcd` and crowd the output of `kubectl get rs`. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.  \nMore specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.\nIn this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.\n", "relevant_passages": ["What happens if I set the revision history limit to zero?"]}
{"query": "A user asked the following question:\nQuestion: Is there a way to automatically scale the replicas?\nThis is about the following runbook:\nRunbook Title: Scaling\nRunbook Content: Common usage patternsScalingThe ReplicationController enables scaling the number of replicas up or down, either manually or by an auto-scaling control agent, by updating the `replicas` field.\n", "relevant_passages": ["Is there a way to automatically scale the replicas?"]}
{"query": "A user asked the following question:\nQuestion: Do I need to create a service for StatefulSets?\nThis is about the following runbook:\nRunbook Title: Limitations\nRunbook Content: Limitations* The storage for a given Pod must either be provisioned by a\n[PersistentVolume Provisioner](/docs/concepts/storage/dynamic-provisioning/) ([examples here](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md))\nbased on the requested _storage class_, or pre-provisioned by an admin.\n* Deleting and/or scaling a StatefulSet down will *not* delete the volumes associated with the\nStatefulSet. This is done to ensure data safety, which is generally more valuable than an\nautomatic purge of all related StatefulSet resources.\n* StatefulSets currently require a [Headless Service](/docs/concepts/services-networking/service/#headless-services)\nto be responsible for the network identity of the Pods. You are responsible for creating this\nService.\n* StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is\ndeleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is\npossible to scale the StatefulSet down to 0 prior to deletion.\n* When using [Rolling Updates](#rolling-updates) with the default\n[Pod Management Policy](#pod-management-policies) (`OrderedReady`),\nit's possible to get into a broken state that requires\n[manual intervention to repair](#forced-rollback).\n", "relevant_passages": ["Do I need to create a service for StatefulSets?"]}
{"query": "A user asked the following question:\nQuestion: When should I consider using a Deployment or ReplicaSet instead of a StatefulSet?\nThis is about the following runbook:\nRunbook Title: Using StatefulSets\nRunbook Content: Using StatefulSetsStatefulSets are valuable for applications that require one or more of the\nfollowing.  \n* Stable, unique network identifiers.\n* Stable, persistent storage.\n* Ordered, graceful deployment and scaling.\n* Ordered, automated rolling updates.  \nIn the above, stable is synonymous with persistence across Pod (re)scheduling.\nIf an application doesn't require any stable identifiers or ordered deployment,\ndeletion, or scaling, you should deploy your application using a workload object\nthat provides a set of stateless replicas.\n[Deployment](/docs/concepts/workloads/controllers/deployment/) or\n[ReplicaSet](/docs/concepts/workloads/controllers/replicaset/) may be better suited to your stateless needs.\n", "relevant_passages": ["When should I consider using a Deployment or ReplicaSet instead of a StatefulSet?"]}
{"query": "A user asked the following question:\nQuestion: What are the reasons for a Pod termination indicated by the Disruption Target condition?\nThis is about the following runbook:\nRunbook Title: Pod disruption conditions {#pod-disruption-conditions}\nRunbook Content: Pod disruption conditions {#pod-disruption-conditions}{{< feature-state feature_gate_name=\"PodDisruptionConditions\" >}}  \nA dedicated Pod `DisruptionTarget` [condition](/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions)\nis added to indicate\nthat the Pod is about to be deleted due to a {{<glossary_tooltip term_id=\"disruption\" text=\"disruption\">}}.\nThe `reason` field of the condition additionally\nindicates one of the following reasons for the Pod termination:  \n`PreemptionByScheduler`\n: Pod is due to be {{<glossary_tooltip term_id=\"preemption\" text=\"preempted\">}} by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see [Pod priority preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).  \n`DeletionByTaintManager`\n: Pod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within `kube-controller-manager`) due to a `NoExecute` taint that the Pod does not tolerate; see {{<glossary_tooltip term_id=\"taint\" text=\"taint\">}}-based evictions.  \n`EvictionByEvictionAPI`\n: Pod has been marked for {{<glossary_tooltip term_id=\"api-eviction\" text=\"eviction using the Kubernetes API\">}} .  \n`DeletionByPodGC`\n: Pod, that is bound to a no longer existing Node, is due to be deleted by [Pod garbage collection](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection).  \n`TerminationByKubelet`\n: Pod has been terminated by the kubelet, because of either {{<glossary_tooltip term_id=\"node-pressure-eviction\" text=\"node pressure eviction\">}},\nthe [graceful node shutdown](/docs/concepts/architecture/nodes/#graceful-node-shutdown),\nor preemption for [system critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).  \nIn all other disruption scenarios, like eviction due to exceeding\n[Pod container limits](/docs/concepts/configuration/manage-resources-containers/),\nPods don't receive the `DisruptionTarget` condition because the disruptions were\nprobably caused by the Pod and would reoccur on retry.  \n{{< note >}}\nA Pod disruption might be interrupted. The control plane might re-attempt to\ncontinue the disruption of the same Pod, but it is not guaranteed. As a result,\nthe `DisruptionTarget` condition might be added to a Pod, but that Pod might then not actually be\ndeleted. In such a situation, after some time, the\nPod disruption condition will be cleared.\n{{< /note >}}  \nAlong with cleaning up the pods, the Pod garbage collector (PodGC) will also mark them as failed if they are in a non-terminal\nphase (see also [Pod garbage collection](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)).  \nWhen using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job's\n[Pod failure policy](/docs/concepts/workloads/controllers/job#pod-failure-policy).\n", "relevant_passages": ["What are the reasons for a Pod termination indicated by the Disruption Target condition?"]}
{"query": "A user asked the following question:\nQuestion: How can I check the rollout status of my Deployment after scaling?\nThis is about the following runbook:\nRunbook Title: Proportional scaling\nRunbook Content: Scaling a DeploymentProportional scalingRollingUpdate Deployments support running multiple versions of an application at the same time. When you\nor an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress\nor paused), the Deployment controller balances the additional replicas in the existing active\nReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called *proportional scaling*.  \nFor example, you are running a Deployment with 10 replicas, [maxSurge](#max-surge)=3, and [maxUnavailable](#max-unavailable)=2.  \n* Ensure that the 10 replicas in your Deployment are running.\n```shell\nkubectl get deploy\n```\nThe output is similar to this:  \n```\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment     10        10        10           10          50s\n```  \n* You update to a new image which happens to be unresolvable from inside the cluster.\n```shell\nkubectl set image deployment/nginx-deployment nginx=nginx:sometag\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment image updated\n```  \n* The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the\n`maxUnavailable` requirement that you mentioned above. Check out the rollout status:\n```shell\nkubectl get rs\n```\nThe output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY     AGE\nnginx-deployment-1989198191   5         5         0         9s\nnginx-deployment-618515232    8         8         8         1m\n```  \n* Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas\nto 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using\nproportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you\nspread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the\nmost replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the\nReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.  \nIn our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the\nnew ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming\nthe new replicas become healthy. To confirm this, run:  \n```shell\nkubectl get deploy\n```  \nThe output is similar to this:\n```\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment     15        18        7            8           7m\n```\nThe rollout status confirms how the replicas were added to each ReplicaSet.\n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY     AGE\nnginx-deployment-1989198191   7         7         0         7m\nnginx-deployment-618515232    11        11        11        7m\n```\n", "relevant_passages": ["How can I check the rollout status of my Deployment after scaling?"]}
{"query": "A user asked the following question:\nQuestion: What happens to Bare Pods if they get deleted or terminated?\nThis is about the following runbook:\nRunbook Title: Bare Pods\nRunbook Content: Alternatives to ReplicationControllerBare PodsUnlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node.  A ReplicationController delegates local container restarts to some agent on the node, such as the kubelet.\n", "relevant_passages": ["What happens to Bare Pods if they get deleted or terminated?"]}
{"query": "A user asked the following question:\nQuestion: What does the `Running` status mean for a container?\nThis is about the following runbook:\nRunbook Title: `Running` {#container-state-running}\nRunbook Content: Container states`Running` {#container-state-running}The `Running` status indicates that a container is executing without issues. If there\nwas a `postStart` hook configured, it has already executed and finished. When you use\n`kubectl` to query a Pod with a container that is `Running`, you also see information\nabout when the container entered the `Running` state.\n", "relevant_passages": ["What does the `Running` status mean for a container?"]}
{"query": "A user asked the following question:\nQuestion: How do I suspend a CronJob from executing Jobs?\nThis is about the following runbook:\nRunbook Title: Schedule suspension\nRunbook Content: Writing a CronJob specSchedule suspensionYou can suspend execution of Jobs for a CronJob, by setting the optional `.spec.suspend` field\nto true. The field defaults to false.  \nThis setting does _not_ affect Jobs that the CronJob has already started.  \nIf you do set that field to true, all subsequent executions are suspended (they remain\nscheduled, but the CronJob controller does not start the Jobs to run the tasks) until\nyou unsuspend the CronJob.  \n{{< caution >}}\nExecutions that are suspended during their scheduled time count as missed Jobs.\nWhen `.spec.suspend` changes from `true` to `false` on an existing CronJob without a\n[starting deadline](#starting-deadline), the missed Jobs are scheduled immediately.\n{{< /caution >}}\n", "relevant_passages": ["How do I suspend a CronJob from executing Jobs?"]}
{"query": "A user asked the following question:\nQuestion: Can I customize how pod failures are handled in a Job?\nThis is about the following runbook:\nRunbook Title: Handling Pod and container failures\nRunbook Content: Handling Pod and container failuresA container in a Pod may fail for a number of reasons, such as because the process in it exited with\na non-zero exit code, or the container was killed for exceeding a memory limit, etc. If this\nhappens, and the `.spec.template.spec.restartPolicy = \"OnFailure\"`, then the Pod stays\non the node, but the container is re-run. Therefore, your program needs to handle the case when it is\nrestarted locally, or else specify `.spec.template.spec.restartPolicy = \"Never\"`.\nSee [pod lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/#example-states) for more information on `restartPolicy`.  \nAn entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node\n(node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the\n`.spec.template.spec.restartPolicy = \"Never\"`. When a Pod fails, then the Job controller\nstarts a new Pod. This means that your application needs to handle the case when it is restarted in a new\npod. In particular, it needs to handle temporary files, locks, incomplete output and the like\ncaused by previous runs.  \nBy default, each pod failure is counted towards the `.spec.backoffLimit` limit,\nsee [pod backoff failure policy](#pod-backoff-failure-policy). However, you can\ncustomize handling of pod failures by setting the Job's [pod failure policy](#pod-failure-policy).  \nAdditionally, you can choose to count the pod failures independently for each\nindex of an [Indexed](#completion-mode) Job by setting the `.spec.backoffLimitPerIndex` field\n(for more information, see [backoff limit per index](#backoff-limit-per-index)).  \nNote that even if you specify `.spec.parallelism = 1` and `.spec.completions = 1` and\n`.spec.template.spec.restartPolicy = \"Never\"`, the same program may\nsometimes be started twice.  \nIf you do specify `.spec.parallelism` and `.spec.completions` both greater than 1, then there may be\nmultiple pods running at once. Therefore, your pods must also be tolerant of concurrency.  \nIf you specify the `.spec.podFailurePolicy` field, the Job controller does not consider a terminating\nPod (a pod that has a `.metadata.deletionTimestamp` field set) as a failure until that Pod is\nterminal (its `.status.phase` is `Failed` or `Succeeded`). However, the Job controller\ncreates a replacement Pod as soon as the termination becomes apparent. Once the\npod terminates, the Job controller evaluates `.backoffLimit` and `.podFailurePolicy`\nfor the relevant Job, taking this now-terminated Pod into consideration.  \nIf either of these requirements is not satisfied, the Job controller counts\na terminating Pod as an immediate failure, even if that Pod later terminates\nwith `phase: \"Succeeded\"`.\n", "relevant_passages": ["Can I customize how pod failures are handled in a Job?"]}
{"query": "A user asked the following question:\nQuestion: What happens to Pods with an ordinal less than the specified partition during an update?\nThis is about the following runbook:\nRunbook Title: Partitioned rolling updates {#partitions}\nRunbook Content: Rolling UpdatesPartitioned rolling updates {#partitions}The `RollingUpdate` update strategy can be partitioned, by specifying a\n`.spec.updateStrategy.rollingUpdate.partition`. If a partition is specified, all Pods with an\nordinal that is greater than or equal to the partition will be updated when the StatefulSet's\n`.spec.template` is updated. All Pods with an ordinal that is less than the partition will not\nbe updated, and, even if they are deleted, they will be recreated at the previous version. If a\nStatefulSet's `.spec.updateStrategy.rollingUpdate.partition` is greater than its `.spec.replicas`,\nupdates to its `.spec.template` will not be propagated to its Pods.\nIn most cases you will not need to use a partition, but they are useful if you want to stage an\nupdate, roll out a canary, or perform a phased roll out.\n", "relevant_passages": ["What happens to Pods with an ordinal less than the specified partition during an update?"]}
{"query": "A user asked the following question:\nQuestion: What happens to a container when it enters the Terminated state?\nThis is about the following runbook:\nRunbook Title: `Terminated` {#container-state-terminated}\nRunbook Content: Container states`Terminated` {#container-state-terminated}A container in the `Terminated` state began execution and then either ran to\ncompletion or failed for some reason. When you use `kubectl` to query a Pod with\na container that is `Terminated`, you see a reason, an exit code, and the start and\nfinish time for that container's period of execution.  \nIf a container has a `preStop` hook configured, this hook runs before the container enters\nthe `Terminated` state.\n", "relevant_passages": ["What happens to a container when it enters the Terminated state?"]}
{"query": "A user asked the following question:\nQuestion: What happens when I update the Pod template in a Deployment?\nThis is about the following runbook:\nRunbook Title: Rolling Back a Deployment\nRunbook Content: Rolling Back a DeploymentSometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.\nBy default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want\n(you can change that by modifying revision history limit).  \n{{< note >}}\nA Deployment's revision is created when a Deployment's rollout is triggered. This means that the\nnew revision is created if and only if the Deployment's Pod template (`.spec.template`) is changed,\nfor example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,\ndo not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.\nThis means that when you roll back to an earlier revision, only the Deployment's Pod template part is\nrolled back.\n{{< /note >}}  \n* Suppose that you made a typo while updating the Deployment, by putting the image name as `nginx:1.161` instead of `nginx:1.16.1`:  \n```shell\nkubectl set image deployment/nginx-deployment nginx=nginx:1.161\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment image updated\n```  \n* The rollout gets stuck. You can verify it by checking the rollout status:  \n```shell\nkubectl rollout status deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\n```  \n* Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,\n[read more here](#deployment-status).  \n* You see that the number of old replicas (adding the replica count from\n`nginx-deployment-1564180365` and `nginx-deployment-2035384211`) is 3, and the number of\nnew replicas (from `nginx-deployment-3066724191`) is 1.  \n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY   AGE\nnginx-deployment-1564180365   3         3         3       25s\nnginx-deployment-2035384211   0         0         0       36s\nnginx-deployment-3066724191   1         1         0       6s\n```  \n* Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.  \n```shell\nkubectl get pods\n```  \nThe output is similar to this:\n```\nNAME                                READY     STATUS             RESTARTS   AGE\nnginx-deployment-1564180365-70iae   1/1       Running            0          25s\nnginx-deployment-1564180365-jbqqo   1/1       Running            0          25s\nnginx-deployment-1564180365-hysrc   1/1       Running            0          25s\nnginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s\n```  \n{{< note >}}\nThe Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (`maxUnavailable` specifically) that you have specified. Kubernetes by default sets the value to 25%.\n{{< /note >}}  \n* Get the description of the Deployment:\n```shell\nkubectl describe deployment\n```  \nThe output is similar to this:\n```\nName:           nginx-deployment\nNamespace:      default\nCreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700\nLabels:         app=nginx\nSelector:       app=nginx\nReplicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable\nStrategyType:       RollingUpdate\nMinReadySeconds:    0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx\nContainers:\nnginx:\nImage:        nginx:1.161\nPort:         80/TCP\nHost Port:    0/TCP\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    ReplicaSetUpdated\nOldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)\nNewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)\nEvents:\nFirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message\n--------- --------    -----   ----                    -------------   --------    ------              -------\n1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3\n22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1\n22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2\n22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2\n21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1\n21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3\n13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0\n13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1\n```  \nTo fix this, you need to rollback to a previous revision of Deployment that is stable.\n", "relevant_passages": ["What happens when I update the Pod template in a Deployment?"]}
{"query": "A user asked the following question:\nQuestion: How does the Pod garbage collector know when to clean up terminated Pods?\nThis is about the following runbook:\nRunbook Title: Garbage collection of Pods {#pod-garbage-collection}\nRunbook Content: Termination of Pods {#pod-termination}Garbage collection of Pods {#pod-garbage-collection}For failed Pods, the API objects remain in the cluster's API until a human or\n{{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}} process\nexplicitly removes them.  \nThe Pod garbage collector (PodGC), which is a controller in the control plane, cleans up\nterminated Pods (with a phase of `Succeeded` or `Failed`), when the number of Pods exceeds the\nconfigured threshold (determined by `terminated-pod-gc-threshold` in the kube-controller-manager).\nThis avoids a resource leak as Pods are created and terminated over time.  \nAdditionally, PodGC cleans up any Pods which satisfy any of the following conditions:  \n1. are orphan Pods - bound to a node which no longer exists,\n1. are unscheduled terminating Pods,\n1. are terminating Pods, bound to a non-ready node tainted with\n[`node.kubernetes.io/out-of-service`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-out-of-service),\nwhen the `NodeOutOfServiceVolumeDetach` feature gate is enabled.  \nAlong with cleaning up the Pods, PodGC will also mark them as failed if they are in a non-terminal\nphase. Also, PodGC adds a Pod disruption condition when cleaning up an orphan Pod.\nSee [Pod disruption conditions](/docs/concepts/workloads/pods/disruptions#pod-disruption-conditions)\nfor more details.\n", "relevant_passages": ["How does the Pod garbage collector know when to clean up terminated Pods?"]}
{"query": "A user asked the following question:\nQuestion: If all containers in a Pod are terminated, will it restart?\nThis is about the following runbook:\nRunbook Title: Pod restart reasons\nRunbook Content: Detailed behaviorPod restart reasonsA Pod can restart, causing re-execution of init containers, for the following\nreasons:  \n* The Pod infrastructure container is restarted. This is uncommon and would\nhave to be done by someone with root access to nodes.\n* All containers in a Pod are terminated while `restartPolicy` is set to Always,\nforcing a restart, and the init container completion record has been lost due\nto {{< glossary_tooltip text=\"garbage collection\" term_id=\"garbage-collection\" >}}.  \nThe Pod will not be restarted when the init container image is changed, or the\ninit container completion record has been lost due to garbage collection. This\napplies for Kubernetes v1.20 and later. If you are using an earlier version of\nKubernetes, consult the documentation for the version you are using.\n", "relevant_passages": ["If all containers in a Pod are terminated, will it restart?"]}
{"query": "A user asked the following question:\nQuestion: How does a Pod get its network configured after being scheduled?\nThis is about the following runbook:\nRunbook Title: Pod network readiness {#pod-has-network}\nRunbook Content: Pod conditionsPod network readiness {#pod-has-network}{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \n{{< note >}}\nDuring its early development, this condition was named `PodHasNetwork`.\n{{< /note >}}  \nAfter a Pod gets scheduled on a node, it needs to be admitted by the kubelet and\nto have any required storage volumes mounted. Once these phases are complete,\nthe kubelet works with\na container runtime (using {{< glossary_tooltip term_id=\"cri\" >}}) to set up a\nruntime sandbox and configure networking for the Pod. If the\n`PodReadyToStartContainersCondition`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled\n(it is enabled by default for Kubernetes {{< skew currentVersion >}}), the\n`PodReadyToStartContainers` condition will be added to the `status.conditions` field of a Pod.  \nThe `PodReadyToStartContainers` condition is set to `False` by the Kubelet when it detects a\nPod does not have a runtime sandbox with networking configured. This occurs in\nthe following scenarios:  \n- Early in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox for\nthe Pod using the container runtime.\n- Later in the lifecycle of the Pod, when the Pod sandbox has been destroyed due to either:\n- the node rebooting, without the Pod getting evicted\n- for container runtimes that use virtual machines for isolation, the Pod\nsandbox virtual machine rebooting, which then requires creating a new sandbox and\nfresh container network configuration.  \nThe `PodReadyToStartContainers` condition is set to `True` by the kubelet after the\nsuccessful completion of sandbox creation and network configuration for the Pod\nby the runtime plugin. The kubelet can start pulling container images and create\ncontainers after `PodReadyToStartContainers` condition has been set to `True`.  \nFor a Pod with init containers, the kubelet sets the `Initialized` condition to\n`True` after the init containers have successfully completed (which happens\nafter successful sandbox creation and network configuration by the runtime\nplugin). For a Pod without init containers, the kubelet sets the `Initialized`\ncondition to `True` before sandbox creation and network configuration starts.\n", "relevant_passages": ["How does a Pod get its network configured after being scheduled?"]}
{"query": "A user asked the following question:\nQuestion: How does a Deployment manage its Pods?\nThis is about the following runbook:\nRunbook Title: Deployment (Recommended)\nRunbook Content: Alternatives to ReplicationControllerDeployment (Recommended)[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is a higher-level API object that updates its underlying Replica Sets and their Pods. Deployments are recommended if you want the rolling update functionality,  because they are declarative, server-side, and have additional features.\n", "relevant_passages": ["How does a Deployment manage its Pods?"]}
{"query": "A user asked the following question:\nQuestion: How does a Job in Kubernetes ensure that tasks are completed successfully?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- alculquicondor\n- erictune\n- soltysh\ntitle: Jobs\napi_metadata:\n- apiVersion: \"batch/v1\"\nkind: \"Job\"\ncontent_type: concept\ndescription: >-\nJobs represent one-off tasks that run to completion and then stop.\nfeature:\ntitle: Batch execution\ndescription: >\nIn addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.\nweight: 50\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nA Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate.\nAs pods successfully complete, the Job tracks the successful completions. When a specified number\nof successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up\nthe Pods it created. Suspending a Job will delete its active Pods until the Job\nis resumed again.  \nA simple case is to create one Job object in order to reliably run one Pod to completion.\nThe Job object will start a new Pod if the first Pod fails or is deleted (for example\ndue to a node hardware failure or a node reboot).  \nYou can also use a Job to run multiple Pods in parallel.  \nIf you want to run a Job (either a single task, or several in parallel) on a schedule,\nsee [CronJob](/docs/concepts/workloads/controllers/cron-jobs/).  \n<!-- body -->\n", "relevant_passages": ["How does a Job in Kubernetes ensure that tasks are completed successfully?"]}
{"query": "A user asked the following question:\nQuestion: How can I check when a container entered the `Running` state using kubectl?\nThis is about the following runbook:\nRunbook Title: `Running` {#container-state-running}\nRunbook Content: Container states`Running` {#container-state-running}The `Running` status indicates that a container is executing without issues. If there\nwas a `postStart` hook configured, it has already executed and finished. When you use\n`kubectl` to query a Pod with a container that is `Running`, you also see information\nabout when the container entered the `Running` state.\n", "relevant_passages": ["How can I check when a container entered the `Running` state using kubectl?"]}
{"query": "A user asked the following question:\nQuestion: What does the Deployment controller do when a new Deployment is observed?\nThis is about the following runbook:\nRunbook Title: Rollover (aka multiple updates in-flight)\nRunbook Content: Updating a DeploymentRollover (aka multiple updates in-flight)Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up\nthe desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels\nmatch `.spec.selector` but whose template does not match `.spec.template` are scaled down. Eventually, the new\nReplicaSet is scaled to `.spec.replicas` and all old ReplicaSets is scaled to 0.  \nIf you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet\nas per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously\n-- it will add it to its list of old ReplicaSets and start scaling it down.  \nFor example, suppose you create a Deployment to create 5 replicas of `nginx:1.14.2`,\nbut then update the Deployment to create 5 replicas of `nginx:1.16.1`, when only 3\nreplicas of `nginx:1.14.2` had been created. In that case, the Deployment immediately starts\nkilling the 3 `nginx:1.14.2` Pods that it had created, and starts creating\n`nginx:1.16.1` Pods. It does not wait for the 5 replicas of `nginx:1.14.2` to be created\nbefore changing course.\n", "relevant_passages": ["What does the Deployment controller do when a new Deployment is observed?"]}
{"query": "A user asked the following question:\nQuestion: Is it mandatory to specify startingDeadlineSeconds for a CronJob?\nThis is about the following runbook:\nRunbook Title: Deadline for delayed Job start {#starting-deadline}\nRunbook Content: Writing a CronJob specDeadline for delayed Job start {#starting-deadline}The `.spec.startingDeadlineSeconds` field is optional.\nThis field defines a deadline (in whole seconds) for starting the Job, if that Job misses its scheduled time\nfor any reason.  \nAfter missing the deadline, the CronJob skips that instance of the Job (future occurrences are still scheduled).\nFor example, if you have a backup Job that runs twice a day, you might allow it to start up to 8 hours late,\nbut no later, because a backup taken any later wouldn't be useful: you would instead prefer to wait for\nthe next scheduled run.  \nFor Jobs that miss their configured deadline, Kubernetes treats them as failed Jobs.\nIf you don't specify `startingDeadlineSeconds` for a CronJob, the Job occurrences have no deadline.  \nIf the `.spec.startingDeadlineSeconds` field is set (not null), the CronJob\ncontroller measures the time between when a Job is expected to be created and\nnow. If the difference is higher than that limit, it will skip this execution.  \nFor example, if it is set to `200`, it allows a Job to be created for up to 200\nseconds after the actual schedule.\n", "relevant_passages": ["Is it mandatory to specify startingDeadlineSeconds for a CronJob?"]}
{"query": "A user asked the following question:\nQuestion: Is there a specific proposal for TTL after job finish in Kubernetes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Read [Clean up Jobs automatically](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically)  \n* Refer to the [Kubernetes Enhancement Proposal](https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md)\n(KEP) for adding this mechanism.\n", "relevant_passages": ["Is there a specific proposal for TTL after job finish in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What labels should I use to manage different release tracks?\nThis is about the following runbook:\nRunbook Title: Multiple release tracks\nRunbook Content: Common usage patternsMultiple release tracksIn addition to running multiple releases of an application while a rolling update is in progress, it's common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.  \nFor instance, a service might target all pods with `tier in (frontend), environment in (prod)`.  Now say you have 10 replicated pods that make up this tier.  But you want to be able to 'canary' a new version of this component.  You could set up a ReplicationController with `replicas` set to 9 for the bulk of the replicas, with labels `tier=frontend, environment=prod, track=stable`, and another ReplicationController with `replicas` set to 1 for the canary, with labels `tier=frontend, environment=prod, track=canary`.  Now the service is covering both the canary and non-canary pods.  But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.\n", "relevant_passages": ["What labels should I use to manage different release tracks?"]}
{"query": "A user asked the following question:\nQuestion: How can I check the state of containers in a Pod?\nThis is about the following runbook:\nRunbook Title: Container states\nRunbook Content: Container statesAs well as the [phase](#pod-phase) of the Pod overall, Kubernetes tracks the state of\neach container inside a Pod. You can use\n[container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/) to\ntrigger events to run at certain points in a container's lifecycle.  \nOnce the {{< glossary_tooltip text=\"scheduler\" term_id=\"kube-scheduler\" >}}\nassigns a Pod to a Node, the kubelet starts creating containers for that Pod\nusing a {{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}.\nThere are three possible container states: `Waiting`, `Running`, and `Terminated`.  \nTo check the state of a Pod's containers, you can use\n`kubectl describe pod <name-of-pod>`. The output shows the state for each container\nwithin that Pod.  \nEach state has a specific meaning:\n", "relevant_passages": ["How can I check the state of containers in a Pod?"]}
{"query": "A user asked the following question:\nQuestion: What happens when a Job reaches its backoff limit?\nThis is about the following runbook:\nRunbook Title: Pod backoff failure policy\nRunbook Content: Handling Pod and container failuresPod backoff failure policyThere are situations where you want to fail a Job after some amount of retries\ndue to a logical error in configuration etc.\nTo do so, set `.spec.backoffLimit` to specify the number of retries before\nconsidering a Job as failed. The back-off limit is set by default to 6. Failed\nPods associated with the Job are recreated by the Job controller with an\nexponential back-off delay (10s, 20s, 40s ...) capped at six minutes.  \nThe number of retries is calculated in two ways:  \n- The number of Pods with `.status.phase = \"Failed\"`.\n- When using `restartPolicy = \"OnFailure\"`, the number of retries in all the\ncontainers of Pods with `.status.phase` equal to `Pending` or `Running`.  \nIf either of the calculations reaches the `.spec.backoffLimit`, the Job is\nconsidered failed.  \n{{< note >}}\nIf your job has `restartPolicy = \"OnFailure\"`, keep in mind that your Pod running the Job\nwill be terminated once the job backoff limit has been reached. This can make debugging\nthe Job's executable more difficult. We suggest setting\n`restartPolicy = \"Never\"` when debugging the Job or using a logging system to ensure output\nfrom failed Jobs is not lost inadvertently.\n{{< /note >}}\n", "relevant_passages": ["What happens when a Job reaches its backoff limit?"]}
{"query": "A user asked the following question:\nQuestion: What does the phase of a Pod indicate in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Pod phase\nRunbook Content: Pod phaseA Pod's `status` field is a\n[PodStatus](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podstatus-v1-core)\nobject, which has a `phase` field.  \nThe phase of a Pod is a simple, high-level summary of where the Pod is in its\nlifecycle. The phase is not intended to be a comprehensive rollup of observations\nof container or Pod state, nor is it intended to be a comprehensive state machine.  \nThe number and meanings of Pod phase values are tightly guarded.\nOther than what is documented here, nothing should be assumed about Pods that\nhave a given `phase` value.  \nHere are the possible values for `phase`:  \nValue       | Description\n:-----------|:-----------\n`Pending`   | The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.\n`Running`   | The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.\n`Succeeded` | All containers in the Pod have terminated in success, and will not be restarted.\n`Failed`    | All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system, and is not set for automatic restarting.\n`Unknown`   | For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.  \n{{< note >}}  \nWhen a pod is failing to start repeatedly, `CrashLoopBackOff` may appear in the `Status` field of some kubectl commands. Similarly, when a pod is being deleted, `Terminating` may appear in the `Status` field of some kubectl commands.  \nMake sure not to confuse _Status_, a kubectl display field for user intuition, with the pod's `phase`.\nPod phase is an explicit part of the Kubernetes data model and of the\n[Pod API](/docs/reference/kubernetes-api/workload-resources/pod-v1/).  \n```\nNAMESPACE               NAME               READY   STATUS             RESTARTS   AGE\nalessandras-namespace   alessandras-pod    0/1     CrashLoopBackOff   200        2d9h\n```  \n---  \nA Pod is granted a term to terminate gracefully, which defaults to 30 seconds.\nYou can use the flag `--force` to [terminate a Pod by force](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced).\n{{< /note >}}  \nSince Kubernetes 1.27, the kubelet transitions deleted Pods, except for\n[static Pods](/docs/tasks/configure-pod-container/static-pod/) and\n[force-deleted Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced)\nwithout a finalizer, to a terminal phase (`Failed` or `Succeeded` depending on\nthe exit statuses of the pod containers) before their deletion from the API server.  \nIf a node dies or is disconnected from the rest of the cluster, Kubernetes\napplies a policy for setting the `phase` of all Pods on the lost node to Failed.\n", "relevant_passages": ["What does the phase of a Pod indicate in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: Why should I clean up finished jobs automatically?\nThis is about the following runbook:\nRunbook Title: Clean up finished jobs automatically\nRunbook Content: Clean up finished jobs automaticallyFinished Jobs are usually no longer needed in the system. Keeping them around in\nthe system will put pressure on the API server. If the Jobs are managed directly\nby a higher level controller, such as\n[CronJobs](/docs/concepts/workloads/controllers/cron-jobs/), the Jobs can be\ncleaned up by CronJobs based on the specified capacity-based cleanup policy.\n", "relevant_passages": ["Why should I clean up finished jobs automatically?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I try to set hostNetwork to true with hostUsers set to false?\nThis is about the following runbook:\nRunbook Title: Limitations\nRunbook Content: LimitationsWhen using a user namespace for the pod, it is disallowed to use other host\nnamespaces. In particular, if you set `hostUsers: false` then you are not\nallowed to set any of:  \n* `hostNetwork: true`\n* `hostIPC: true`\n* `hostPID: true`\n", "relevant_passages": ["What happens if I try to set hostNetwork to true with hostUsers set to false?"]}
{"query": "A user asked the following question:\nQuestion: How do I suspend a Job in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Suspending a Job\nRunbook Content: Advanced usageSuspending a Job{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}  \nWhen a Job is created, the Job controller will immediately begin creating Pods\nto satisfy the Job's requirements and will continue to do so until the Job is\ncomplete. However, you may want to temporarily suspend a Job's execution and\nresume it later, or start Jobs in suspended state and have a custom controller\ndecide later when to start them.  \nTo suspend a Job, you can update the `.spec.suspend` field of\nthe Job to true; later, when you want to resume it again, update it to false.\nCreating a Job with `.spec.suspend` set to true will create it in the suspended\nstate.  \nWhen a Job is resumed from suspension, its `.status.startTime` field will be\nreset to the current time. This means that the `.spec.activeDeadlineSeconds`\ntimer will be stopped and reset when a Job is suspended and resumed.  \nWhen you suspend a Job, any running Pods that don't have a status of `Completed`\nwill be [terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)\nwith a SIGTERM signal. The Pod's graceful termination period will be honored and\nyour Pod must handle this signal in this period. This may involve saving\nprogress for later or undoing changes. Pods terminated this way will not count\ntowards the Job's `completions` count.  \nAn example Job definition in the suspended state can be like so:  \n```shell\nkubectl get job myjob -o yaml\n```  \n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: myjob\nspec:\nsuspend: true\nparallelism: 1\ncompletions: 5\ntemplate:\nspec:\n...\n```  \nYou can also toggle Job suspension by patching the Job using the command line.  \nSuspend an active Job:  \n```shell\nkubectl patch job/myjob --type=strategic --patch '{\"spec\":{\"suspend\":true}}'\n```  \nResume a suspended Job:  \n```shell\nkubectl patch job/myjob --type=strategic --patch '{\"spec\":{\"suspend\":false}}'\n```  \nThe Job's status can be used to determine if a Job is suspended or has been\nsuspended in the past:  \n```shell\nkubectl get jobs/myjob -o yaml\n```  \n```yaml\napiVersion: batch/v1\nkind: Job\n# .metadata and .spec omitted\nstatus:\nconditions:\n- lastProbeTime: \"2021-02-05T13:14:33Z\"\nlastTransitionTime: \"2021-02-05T13:14:33Z\"\nstatus: \"True\"\ntype: Suspended\nstartTime: \"2021-02-05T13:13:48Z\"\n```  \nThe Job condition of type \"Suspended\" with status \"True\" means the Job is\nsuspended; the `lastTransitionTime` field can be used to determine how long the\nJob has been suspended for. If the status of that condition is \"False\", then the\nJob was previously suspended and is now running. If such a condition does not\nexist in the Job's status, the Job has never been stopped.  \nEvents are also created when the Job is suspended and resumed:  \n```shell\nkubectl describe jobs/myjob\n```  \n```\nName:           myjob\n...\nEvents:\nType    Reason            Age   From            Message\n----    ------            ----  ----            -------\nNormal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl\nNormal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl\nNormal  Suspended         11m   job-controller  Job suspended\nNormal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44\nNormal  Resumed           3s    job-controller  Job resumed\n```  \nThe last four events, particularly the \"Suspended\" and \"Resumed\" events, are\ndirectly a result of toggling the `.spec.suspend` field. In the time between\nthese two events, we see that no Pods were created, but Pod creation restarted\nas soon as the Job was resumed.\n", "relevant_passages": ["How do I suspend a Job in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What happens after the Job controller adds the `FailureTarget` or `SuccessCriteriaMet` condition?\nThis is about the following runbook:\nRunbook Title: Termination of Job pods\nRunbook Content: Job termination and cleanupTermination of Job podsThe Job controller adds the `FailureTarget` condition or the `SuccessCriteriaMet`\ncondition to the Job to trigger Pod termination after a Job meets either the\nsuccess or failure criteria.  \nFactors like `terminationGracePeriodSeconds` might increase the amount of time\nfrom the moment that the Job controller adds the `FailureTarget` condition or the\n`SuccessCriteriaMet` condition to the moment that all of the Job Pods terminate\nand the Job controller adds a [terminal condition](#terminal-job-conditions)\n(`Failed` or `Complete`).  \nYou can use the `FailureTarget` or the `SuccessCriteriaMet` condition to evaluate\nwhether the Job has failed or succeeded without having to wait for the controller\nto add a terminal condition.  \nFor example, you might want to decide when to create a replacement Job\nthat replaces a failed Job. If you replace the failed Job when the `FailureTarget`\ncondition appears, your replacement Job runs sooner, but could result in Pods\nfrom the failed and the replacement Job running at the same time, using\nextra compute resources.  \nAlternatively, if your cluster has limited resource capacity, you could choose to\nwait until the `Failed` condition appears on the Job, which would delay your\nreplacement Job but would ensure that you conserve resources by waiting\nuntil all of the failed Pods are removed.\n", "relevant_passages": ["What happens after the Job controller adds the `FailureTarget` or `SuccessCriteriaMet` condition?"]}
{"query": "A user asked the following question:\nQuestion: How does Kubernetes determine the QoS class for a Pod?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: Pod Quality of Service Classes\ncontent_type: concept\nweight: 85\n---  \n<!-- overview -->  \nThis page introduces _Quality of Service (QoS) classes_ in Kubernetes, and explains\nhow Kubernetes assigns a QoS class to each Pod as a consequence of the resource\nconstraints that you specify for the containers in that Pod. Kubernetes relies on this\nclassification to make decisions about which Pods to evict when there are not enough\navailable resources on a Node.  \n<!-- body -->\n", "relevant_passages": ["How does Kubernetes determine the QoS class for a Pod?"]}
{"query": "A user asked the following question:\nQuestion: How can I set a success policy for my Indexed Job?\nThis is about the following runbook:\nRunbook Title: Success policy {#success-policy}\nRunbook Content: Success policy {#success-policy}{{< feature-state feature_gate_name=\"JobSuccessPolicy\" >}}  \n{{< note >}}\nYou can only configure a success policy for an Indexed Job if you have the\n`JobSuccessPolicy` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nenabled in your cluster.\n{{< /note >}}  \nWhen creating an Indexed Job, you can define when a Job can be declared as succeeded using a `.spec.successPolicy`,\nbased on the pods that succeeded.  \nBy default, a Job succeeds when the number of succeeded Pods equals `.spec.completions`.\nThese are some situations where you might want additional control for declaring a Job succeeded:  \n* When running simulations with different parameters,\nyou might not need all the simulations to succeed for the overall Job to be successful.\n* When following a leader-worker pattern, only the success of the leader determines the success or\nfailure of a Job. Examples of this are frameworks like MPI and PyTorch etc.  \nYou can configure a success policy, in the `.spec.successPolicy` field,\nto meet the above use cases. This policy can handle Job success based on the\nsucceeded pods. After the Job meets the success policy, the job controller terminates the lingering Pods.\nA success policy is defined by rules. Each rule can take one of the following forms:  \n* When you specify the `succeededIndexes` only,\nonce all indexes specified in the `succeededIndexes` succeed, the job controller marks the Job as succeeded.\nThe `succeededIndexes` must be a list of intervals between 0 and `.spec.completions-1`.\n* When you specify the `succeededCount` only,\nonce the number of succeeded indexes reaches the `succeededCount`, the job controller marks the Job as succeeded.\n* When you specify both `succeededIndexes` and `succeededCount`,\nonce the number of succeeded indexes from the subset of indexes specified in the `succeededIndexes` reaches the `succeededCount`,\nthe job controller marks the Job as succeeded.  \nNote that when you specify multiple rules in the `.spec.successPolicy.rules`,\nthe job controller evaluates the rules in order. Once the Job meets a rule, the job controller ignores remaining rules.  \nHere is a manifest for a Job with `successPolicy`:  \n{{% code_sample file=\"/controllers/job-success-policy.yaml\" %}}  \nIn the example above, both `succeededIndexes` and `succeededCount` have been specified.\nTherefore, the job controller will mark the Job as succeeded and terminate the lingering Pods\nwhen either of the specified indexes, 0, 2, or 3, succeed.\nThe Job that meets the success policy gets the `SuccessCriteriaMet` condition with a `SuccessPolicy` reason.\nAfter the removal of the lingering Pods is issued, the Job gets the `Complete` condition.  \nNote that the `succeededIndexes` is represented as intervals separated by a hyphen.\nThe number are listed in represented by the first and last element of the series, separated by a hyphen.  \n{{< note >}}\nWhen you specify both a success policy and some terminating policies such as `.spec.backoffLimit` and `.spec.podFailurePolicy`,\nonce the Job meets either policy, the job controller respects the terminating policy and ignores the success policy.\n{{< /note >}}\n", "relevant_passages": ["How can I set a success policy for my Indexed Job?"]}
{"query": "A user asked the following question:\nQuestion: What does the kubelet do when the grace period expires and containers are still running?\nThis is about the following runbook:\nRunbook Title: Termination of Pods {#pod-termination}\nRunbook Content: Termination of Pods {#pod-termination}Because Pods represent processes running on nodes in the cluster, it is important to\nallow those processes to gracefully terminate when they are no longer needed (rather\nthan being abruptly stopped with a `KILL` signal and having no chance to clean up).  \nThe design aim is for you to be able to request deletion and know when processes\nterminate, but also be able to ensure that deletes eventually complete.\nWhen you request deletion of a Pod, the cluster records and tracks the intended grace period\nbefore the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in\nplace, the {{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} attempts graceful\nshutdown.  \nTypically, with this graceful termination of the pod, kubelet makes requests to the container runtime to attempt to stop the containers in the pod by first sending a TERM (aka. SIGTERM) signal, with a grace period timeout, to the main process in each container. The requests to stop the containers are processed by the container runtime asynchronously. There is no guarantee to the order of processing for these requests. Many container runtimes respect the `STOPSIGNAL` value defined in the container image and, if different, send the container image configured STOPSIGNAL instead of TERM.\nOnce the grace period has expired, the KILL signal is sent to any remaining\nprocesses, and the Pod is then deleted from the\n{{< glossary_tooltip text=\"API Server\" term_id=\"kube-apiserver\" >}}. If the kubelet or the\ncontainer runtime's management service is restarted while waiting for processes to terminate, the\ncluster retries from the start including the full original grace period.  \nPod termination flow, illustrated with an example:  \n1. You use the `kubectl` tool to manually delete a specific Pod, with the default grace period\n(30 seconds).  \n1. The Pod in the API server is updated with the time beyond which the Pod is considered \"dead\"\nalong with the grace period.\nIf you use `kubectl describe` to check the Pod you're deleting, that Pod shows up as \"Terminating\".\nOn the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked\nas terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod\nshutdown process.  \n1. If one of the Pod's containers has defined a `preStop`\n[hook](/docs/concepts/containers/container-lifecycle-hooks) and the `terminationGracePeriodSeconds`\nin the Pod spec is not set to 0, the kubelet runs that hook inside of the container.\nThe default `terminationGracePeriodSeconds` setting is 30 seconds.  \nIf the `preStop` hook is still running after the grace period expires, the kubelet requests\na small, one-off grace period extension of 2 seconds.\n{{% note %}}\nIf the `preStop` hook needs longer to complete than the default grace period allows,\nyou must modify `terminationGracePeriodSeconds` to suit this.\n{{% /note %}}  \n1. The kubelet triggers the container runtime to send a TERM signal to process 1 inside each\ncontainer.  \nThere is [special ordering](#termination-with-sidecars) if the Pod has any\n{{< glossary_tooltip text=\"sidecar containers\" term_id=\"sidecar-container\" >}} defined.\nOtherwise, the containers in the Pod receive the TERM signal at different times and in\nan arbitrary order. If the order of shutdowns matters, consider using a `preStop` hook\nto synchronize (or switch to using sidecar containers).  \n1. At the same time as the kubelet is starting graceful shutdown of the Pod, the control plane\nevaluates whether to remove that shutting-down Pod from EndpointSlice (and Endpoints) objects,\nwhere those objects represent a {{< glossary_tooltip term_id=\"service\" text=\"Service\" >}}\nwith a configured {{< glossary_tooltip text=\"selector\" term_id=\"selector\" >}}.\n{{< glossary_tooltip text=\"ReplicaSets\" term_id=\"replica-set\" >}} and other workload resources\nno longer treat the shutting-down Pod as a valid, in-service replica.  \nPods that shut down slowly should not continue to serve regular traffic and should start\nterminating and finish processing open connections.  Some applications need to go beyond\nfinishing open connections and need more graceful termination, for example, session draining\nand completion.  \nAny endpoints that represent the terminating Pods are not immediately removed from\nEndpointSlices, and a status indicating [terminating state](/docs/concepts/services-networking/endpoint-slices/#conditions)\nis exposed from the EndpointSlice API (and the legacy Endpoints API).\nTerminating endpoints always have their `ready` status as `false` (for backward compatibility\nwith versions before 1.26), so load balancers will not use it for regular traffic.  \nIf traffic draining on terminating Pod is needed, the actual readiness can be checked as a\ncondition `serving`.  You can find more details on how to implement connections draining in the\ntutorial [Pods And Endpoints Termination Flow](/docs/tutorials/services/pods-and-endpoint-termination-flow/)  \n<a id=\"pod-termination-beyond-grace-period\" />  \n1. The kubelet ensures the Pod is shut down and terminated\n1. When the grace period expires, if there is still any container running in the Pod, the\nkubelet triggers forcible shutdown.\nThe container runtime sends `SIGKILL` to any processes still running in any container in the Pod.\nThe kubelet also cleans up a hidden `pause` container if that container runtime uses one.\n1. The kubelet transitions the Pod into a terminal phase (`Failed` or `Succeeded` depending on\nthe end state of its containers).\n1. The kubelet triggers forcible removal of the Pod object from the API server, by setting grace period\nto 0 (immediate deletion).\n1. The API server deletes the Pod's API object, which is then no longer visible from any client.\n", "relevant_passages": ["What does the kubelet do when the grace period expires and containers are still running?"]}
{"query": "A user asked the following question:\nQuestion: Can I use any restart policy in my pod template?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a Job specPod TemplateThe `.spec.template` is the only required field of the `.spec`.  \nThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates).\nIt has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}},\nexcept it is nested and does not have an `apiVersion` or `kind`.  \nIn addition to required fields for a Pod, a pod template in a Job must specify appropriate\nlabels (see [pod selector](#pod-selector)) and an appropriate restart policy.  \nOnly a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)\nequal to `Never` or `OnFailure` is allowed.\n", "relevant_passages": ["Can I use any restart policy in my pod template?"]}
{"query": "A user asked the following question:\nQuestion: What happens when the TTL controller cleans up a Job?\nThis is about the following runbook:\nRunbook Title: TTL mechanism for finished Jobs\nRunbook Content: Clean up finished jobs automaticallyTTL mechanism for finished Jobs{{< feature-state for_k8s_version=\"v1.23\" state=\"stable\" >}}  \nAnother way to clean up finished Jobs (either `Complete` or `Failed`)\nautomatically is to use a TTL mechanism provided by a\n[TTL controller](/docs/concepts/workloads/controllers/ttlafterfinished/) for\nfinished resources, by specifying the `.spec.ttlSecondsAfterFinished` field of\nthe Job.  \nWhen the TTL controller cleans up the Job, it will delete the Job cascadingly,\ni.e. delete its dependent objects, such as Pods, together with the Job. Note\nthat when the Job is deleted, its lifecycle guarantees, such as finalizers, will\nbe honored.  \nFor example:  \n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: pi-with-ttl\nspec:\nttlSecondsAfterFinished: 100\ntemplate:\nspec:\ncontainers:\n- name: pi\nimage: perl:5.34.0\ncommand: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\nrestartPolicy: Never\n```  \nThe Job `pi-with-ttl` will be eligible to be automatically deleted, `100`\nseconds after it finishes.  \nIf the field is set to `0`, the Job will be eligible to be automatically deleted\nimmediately after it finishes. If the field is unset, this Job won't be cleaned\nup by the TTL controller after it finishes.  \n{{< note >}}\nIt is recommended to set `ttlSecondsAfterFinished` field because unmanaged jobs\n(Jobs that you created directly, and not indirectly through other workload APIs\nsuch as CronJob) have a default deletion\npolicy of `orphanDependents` causing Pods created by an unmanaged Job to be left around\nafter that Job is fully deleted.\nEven though the {{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} eventually\n[garbage collects](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)\nthe Pods from a deleted Job after they either fail or complete, sometimes those\nlingering pods may cause cluster performance degradation or in worst case cause the\ncluster to go offline due to this degradation.  \nYou can use [LimitRanges](/docs/concepts/policy/limit-range/) and\n[ResourceQuotas](/docs/concepts/policy/resource-quotas/) to place a\ncap on the amount of resources that a particular namespace can\nconsume.\n{{< /note >}}\n", "relevant_passages": ["What happens when the TTL controller cleans up a Job?"]}
{"query": "A user asked the following question:\nQuestion: How can I set up a Horizontal Pod Autoscaler for my ReplicaSet?\nThis is about the following runbook:\nRunbook Title: ReplicaSet as a Horizontal Pod Autoscaler Target\nRunbook Content: Working with ReplicaSetsReplicaSet as a Horizontal Pod Autoscaler TargetA ReplicaSet can also be a target for\n[Horizontal Pod Autoscalers (HPA)](/docs/tasks/run-application/horizontal-pod-autoscale/). That is,\na ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting\nthe ReplicaSet we created in the previous example.  \n{{% code_sample file=\"controllers/hpa-rs.yaml\" %}}  \nSaving this manifest into `hpa-rs.yaml` and submitting it to a Kubernetes cluster should\ncreate the defined HPA that autoscales the target ReplicaSet depending on the CPU usage\nof the replicated Pods.  \n```shell\nkubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml\n```  \nAlternatively, you can use the `kubectl autoscale` command to accomplish the same\n(and it's easier!)  \n```shell\nkubectl autoscale rs frontend --max=10 --min=3 --cpu-percent=50\n```\n", "relevant_passages": ["How can I set up a Horizontal Pod Autoscaler for my ReplicaSet?"]}
{"query": "A user asked the following question:\nQuestion: How do I manage application availability during disruptions with PodDisruptionBudget?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about the [lifecycle of a Pod](/docs/concepts/workloads/pods/pod-lifecycle/).\n* Learn about [RuntimeClass](/docs/concepts/containers/runtime-class/) and how you can use it to\nconfigure different Pods with different container runtime configurations.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how you can use it to manage application availability during disruptions.\n* Pod is a top-level resource in the Kubernetes REST API.\nThe {{< api-reference page=\"workload-resources/pod-v1\" >}}\nobject definition describes the object in detail.\n* [The Distributed System Toolkit: Patterns for Composite Containers](/blog/2015/06/the-distributed-system-toolkit-patterns/) explains common layouts for Pods with more than one container.\n* Read about [Pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/)  \nTo understand the context for why Kubernetes wraps a common Pod API in other resources (such as {{< glossary_tooltip text=\"StatefulSets\" term_id=\"statefulset\" >}} or {{< glossary_tooltip text=\"Deployments\" term_id=\"deployment\" >}}), you can read about the prior art, including:  \n* [Aurora](https://aurora.apache.org/documentation/latest/reference/configuration/#job-schema)\n* [Borg](https://research.google.com/pubs/pub43438.html)\n* [Marathon](https://github.com/d2iq-archive/marathon)\n* [Omega](https://research.google/pubs/pub41684/)\n* [Tupperware](https://engineering.fb.com/data-center-engineering/tupperware/).\n", "relevant_passages": ["How do I manage application availability during disruptions with PodDisruptionBudget?"]}
{"query": "A user asked the following question:\nQuestion: What happens to Pod Security Standards when user namespaces are enabled?\nThis is about the following runbook:\nRunbook Title: Integration with Pod security admission checks\nRunbook Content: Integration with Pod security admission checks{{< feature-state state=\"alpha\" for_k8s_version=\"v1.29\" >}}  \nFor Linux Pods that enable user namespaces, Kubernetes relaxes the application of\n[Pod Security Standards](/docs/concepts/security/pod-security-standards) in a controlled way.\nThis behavior can be controlled by the [feature\ngate](/docs/reference/command-line-tools-reference/feature-gates/)\n`UserNamespacesPodSecurityStandards`, which allows an early opt-in for end\nusers. Admins have to ensure that user namespaces are enabled by all nodes\nwithin the cluster if using the feature gate.  \nIf you enable the associated feature gate and create a Pod that uses user\nnamespaces, the following fields won't be constrained even in contexts that enforce the\n_Baseline_ or _Restricted_ pod security standard. This behavior does not\npresent a security concern because `root` inside a Pod with user namespaces\nactually refers to the user inside the container, that is never mapped to a\nprivileged user on the host. Here's the list of fields that are **not** checks for Pods in those\ncircumstances:  \n- `spec.securityContext.runAsNonRoot`\n- `spec.containers[*].securityContext.runAsNonRoot`\n- `spec.initContainers[*].securityContext.runAsNonRoot`\n- `spec.ephemeralContainers[*].securityContext.runAsNonRoot`\n- `spec.securityContext.runAsUser`\n- `spec.containers[*].securityContext.runAsUser`\n- `spec.initContainers[*].securityContext.runAsUser`\n- `spec.ephemeralContainers[*].securityContext.runAsUser`\n", "relevant_passages": ["What happens to Pod Security Standards when user namespaces are enabled?"]}
{"query": "A user asked the following question:\nQuestion: Can I manage ReplicaSets directly if they are owned by a Deployment?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- janetkuo\ntitle: Deployments\napi_metadata:\n- apiVersion: \"apps/v1\"\nkind: \"Deployment\"\nfeature:\ntitle: Automated rollouts and rollbacks\ndescription: >\nKubernetes progressively rolls out changes to your application or its configuration, while monitoring application health to ensure it doesn't kill all your instances at the same time. If something goes wrong, Kubernetes will rollback the change for you. Take advantage of a growing ecosystem of deployment solutions.\ndescription: >-\nA Deployment manages a set of Pods to run an application workload, usually one that doesn't maintain state.\ncontent_type: concept\nweight: 10\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nA _Deployment_ provides declarative updates for {{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} and\n{{< glossary_tooltip term_id=\"replica-set\" text=\"ReplicaSets\" >}}.  \nYou describe a _desired state_ in a Deployment, and the Deployment {{< glossary_tooltip term_id=\"controller\" >}} changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.  \n{{< note >}}\nDo not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.\n{{< /note >}}  \n<!-- body -->\n", "relevant_passages": ["Can I manage ReplicaSets directly if they are owned by a Deployment?"]}
{"query": "A user asked the following question:\nQuestion: Is there a special consideration for StatefulSets when force deleting Pods?\nThis is about the following runbook:\nRunbook Title: Forced Pod termination {#pod-termination-forced}\nRunbook Content: Termination of Pods {#pod-termination}Forced Pod termination {#pod-termination-forced}{{< caution >}}\nForced deletions can be potentially disruptive for some workloads and their Pods.\n{{< /caution >}}  \nBy default, all deletes are graceful within 30 seconds. The `kubectl delete` command supports\nthe `--grace-period=<seconds>` option which allows you to override the default and specify your\nown value.  \nSetting the grace period to `0` forcibly and immediately deletes the Pod from the API\nserver. If the Pod was still running on a node, that forcible deletion triggers the kubelet to\nbegin immediate cleanup.  \nUsing kubectl, You must specify an additional flag `--force` along with `--grace-period=0`\nin order to perform force deletions.  \nWhen a force deletion is performed, the API server does not wait for confirmation\nfrom the kubelet that the Pod has been terminated on the node it was running on. It\nremoves the Pod in the API immediately so a new Pod can be created with the same\nname. On the node, Pods that are set to terminate immediately will still be given\na small grace period before being force killed.  \n{{< caution >}}\nImmediate deletion does not wait for confirmation that the running resource has been terminated.\nThe resource may continue to run on the cluster indefinitely.\n{{< /caution >}}  \nIf you need to force-delete Pods that are part of a StatefulSet, refer to the task\ndocumentation for\n[deleting Pods from a StatefulSet](/docs/tasks/run-application/force-delete-stateful-set-pod/).\n", "relevant_passages": ["Is there a special consideration for StatefulSets when force deleting Pods?"]}
{"query": "A user asked the following question:\nQuestion: What does a 'Success' outcome mean for a container probe?\nThis is about the following runbook:\nRunbook Title: Probe outcome\nRunbook Content: Container probesProbe outcomeEach probe has one of three results:  \n`Success`\n: The container passed the diagnostic.  \n`Failure`\n: The container failed the diagnostic.  \n`Unknown`\n: The diagnostic failed (no action should be taken, and the kubelet\nwill make further checks).\n", "relevant_passages": ["What does a 'Success' outcome mean for a container probe?"]}
{"query": "A user asked the following question:\nQuestion: How do I rollback my nginx deployment to the previous version?\nThis is about the following runbook:\nRunbook Title: Rolling Back to a Previous Revision\nRunbook Content: Rolling Back a DeploymentRolling Back to a Previous RevisionFollow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.  \n1. Now you've decided to undo the current rollout and rollback to the previous revision:\n```shell\nkubectl rollout undo deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment rolled back\n```\nAlternatively, you can rollback to a specific revision by specifying it with `--to-revision`:  \n```shell\nkubectl rollout undo deployment/nginx-deployment --to-revision=2\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment rolled back\n```  \nFor more details about rollout related commands, read [`kubectl rollout`](/docs/reference/generated/kubectl/kubectl-commands#rollout).  \nThe Deployment is now rolled back to a previous stable revision. As you can see, a `DeploymentRollback` event\nfor rolling back to revision 2 is generated from Deployment controller.  \n2. Check if the rollback was successful and the Deployment is running as expected, run:\n```shell\nkubectl get deployment nginx-deployment\n```  \nThe output is similar to this:\n```\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           30m\n```\n3. Get the description of the Deployment:\n```shell\nkubectl describe deployment nginx-deployment\n```\nThe output is similar to this:\n```\nName:                   nginx-deployment\nNamespace:              default\nCreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500\nLabels:                 app=nginx\nAnnotations:            deployment.kubernetes.io/revision=4\nkubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\nSelector:               app=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx\nContainers:\nnginx:\nImage:        nginx:1.16.1\nPort:         80/TCP\nHost Port:    0/TCP\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    NewReplicaSetAvailable\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)\nEvents:\nType    Reason              Age   From                   Message\n----    ------              ----  ----                   -------\nNormal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1\nNormal  DeploymentRollback  15s   deployment-controller  Rolled back deployment \"nginx-deployment\" to revision 2\nNormal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0\n```\n", "relevant_passages": ["How do I rollback my nginx deployment to the previous version?"]}
{"query": "A user asked the following question:\nQuestion: Can I pause a failed deployment to make changes?\nThis is about the following runbook:\nRunbook Title: Operating on a failed deployment\nRunbook Content: Deployment statusOperating on a failed deploymentAll actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back\nto a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.\n", "relevant_passages": ["Can I pause a failed deployment to make changes?"]}
{"query": "A user asked the following question:\nQuestion: How can I ensure my Pods are automatically replaced if they get deleted?\nThis is about the following runbook:\nRunbook Title: Bare Pods\nRunbook Content: Alternatives to DaemonSetBare PodsIt is possible to create Pods directly which specify a particular node to run on.  However,\na DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of\nnode failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should\nuse a DaemonSet rather than creating individual Pods.\n", "relevant_passages": ["How can I ensure my Pods are automatically replaced if they get deleted?"]}
{"query": "A user asked the following question:\nQuestion: What happens if a pod gets terminated unexpectedly?\nThis is about the following runbook:\nRunbook Title: Rescheduling\nRunbook Content: Common usage patternsReschedulingAs mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).\n", "relevant_passages": ["What happens if a pod gets terminated unexpectedly?"]}
{"query": "A user asked the following question:\nQuestion: Where can I find the API definition for ReplicationController?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Learn about [Deployment](/docs/concepts/workloads/controllers/deployment/), the replacement\nfor ReplicationController.\n* `ReplicationController` is part of the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/replication-controller-v1\" >}}\nobject definition to understand the API for replication controllers.\n", "relevant_passages": ["Where can I find the API definition for ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: What should I learn about that relates to CronJobs?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods/) and\n[Jobs](/docs/concepts/workloads/controllers/job/), two concepts\nthat CronJobs rely upon.\n* Read about the detailed [format](https://pkg.go.dev/github.com/robfig/cron/v3#hdr-CRON_Expression_Format)\nof CronJob `.spec.schedule` fields.\n* For instructions on creating and working with CronJobs, and for an example\nof a CronJob manifest,\nsee [Running automated tasks with CronJobs](/docs/tasks/job/automated-tasks-with-cron-jobs/).\n* `CronJob` is part of the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/cron-job-v1\" >}}\nAPI reference for more details.\n", "relevant_passages": ["What should I learn about that relates to CronJobs?"]}
{"query": "A user asked the following question:\nQuestion: What does a DaemonSet do in a Kubernetes cluster?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- enisoc\n- erictune\n- foxish\n- janetkuo\n- kow3ns\ntitle: DaemonSet\napi_metadata:\n- apiVersion: \"apps/v1\"\nkind: \"DaemonSet\"\ndescription: >-\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.\ncontent_type: concept\nweight: 40\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nA _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the\ncluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage\ncollected.  Deleting a DaemonSet will clean up the Pods it created.  \nSome typical uses of a DaemonSet are:  \n- running a cluster storage daemon on every node\n- running a logs collection daemon on every node\n- running a node monitoring daemon on every node  \nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.\nA more complex setup might use multiple DaemonSets for a single type of daemon, but with\ndifferent flags and/or different memory and cpu requests for different hardware types.  \n<!-- body -->\n", "relevant_passages": ["What does a DaemonSet do in a Kubernetes cluster?"]}
{"query": "A user asked the following question:\nQuestion: What happens to sidecar containers during pod termination?\nThis is about the following runbook:\nRunbook Title: Differences from application containers\nRunbook Content: Differences from application containersSidecar containers run alongside _app containers_ in the same pod. However, they do not\nexecute the primary application logic; instead, they provide supporting functionality to\nthe main application.  \nSidecar containers have their own independent lifecycles. They can be started, stopped,\nand restarted independently of app containers. This means you can update, scale, or\nmaintain sidecar containers without affecting the primary application.  \nSidecar containers share the same network and storage namespaces with the primary\ncontainer. This co-location allows them to interact closely and share resources.  \nFrom Kubernetes perspective, sidecars graceful termination is less important.\nWhen other containers took all alloted graceful termination time, sidecar containers\nwill receive the `SIGTERM` following with `SIGKILL` faster than may be expected.\nSo exit codes different from `0` (`0` indicates successful exit), for sidecar containers are normal\non Pod termination and should be generally ignored by the external tooling.\n", "relevant_passages": ["What happens to sidecar containers during pod termination?"]}
{"query": "A user asked the following question:\nQuestion: Is 'OrderedReady' the default pod management policy for StatefulSets?\nThis is about the following runbook:\nRunbook Title: Pod Management Policies\nRunbook Content: Deployment and Scaling GuaranteesPod Management PoliciesStatefulSet allows you to relax its ordering guarantees while\npreserving its uniqueness and identity guarantees via its `.spec.podManagementPolicy` field.  \n#### OrderedReady Pod Management  \n`OrderedReady` pod management is the default for StatefulSets. It implements the behavior\ndescribed [above](#deployment-and-scaling-guarantees).  \n#### Parallel Pod Management  \n`Parallel` pod management tells the StatefulSet controller to launch or\nterminate all Pods in parallel, and to not wait for Pods to become Running\nand Ready or completely terminated prior to launching or terminating another\nPod. This option only affects the behavior for scaling operations. Updates are not\naffected.\n", "relevant_passages": ["Is 'OrderedReady' the default pod management policy for StatefulSets?"]}
{"query": "A user asked the following question:\nQuestion: What's the best way to customize pod configurations without creating issues?\nThis is about the following runbook:\nRunbook Title: Writing programs for Replication\nRunbook Content: Writing programs for ReplicationPods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the [RabbitMQ work queues](https://www.rabbitmq.com/tutorials/tutorial-two-python.html), as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.\n", "relevant_passages": ["What's the best way to customize pod configurations without creating issues?"]}
{"query": "A user asked the following question:\nQuestion: How do I create a Pod with the nginx image?\nThis is about the following runbook:\nRunbook Title: Using Pods\nRunbook Content: Using PodsThe following is an example of a Pod which consists of a container running the image `nginx:1.14.2`.  \n{{% code_sample file=\"pods/simple-pod.yaml\" %}}  \nTo create the Pod shown above, run the following command:\n```shell\nkubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml\n```  \nPods are generally not created directly and are created using workload resources.\nSee [Working with Pods](#working-with-pods) for more information on how Pods are used\nwith workload resources.\n", "relevant_passages": ["How do I create a Pod with the nginx image?"]}
{"query": "A user asked the following question:\nQuestion: What do I need to include in the pod template for a ReplicationController?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a ReplicationController ManifestPod TemplateThe `.spec.template` is the only required field of the `.spec`.  \nThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}, except it is nested and does not have an `apiVersion` or `kind`.  \nIn addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate\nlabels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [pod selector](#pod-selector).  \nOnly a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is allowed, which is the default if not specified.  \nFor local container restarts, ReplicationControllers delegate to an agent on the node,\nfor example the [Kubelet](/docs/reference/command-line-tools-reference/kubelet/).\n", "relevant_passages": ["What do I need to include in the pod template for a ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: Do I need to use Pod Disruption Budgets if my organization doesn't separate these roles?\nThis is about the following runbook:\nRunbook Title: Separating Cluster Owner and Application Owner Roles\nRunbook Content: Separating Cluster Owner and Application Owner RolesOften, it is useful to think of the Cluster Manager\nand Application Owner as separate roles with limited knowledge\nof each other.   This separation of responsibilities\nmay make sense in these scenarios:  \n- when there are many application teams sharing a Kubernetes cluster, and\nthere is natural specialization of roles\n- when third-party tools or services are used to automate cluster management  \nPod Disruption Budgets support this separation of roles by providing an\ninterface between the roles.  \nIf you do not have such a separation of responsibilities in your organization,\nyou may not need to use Pod Disruption Budgets.\n", "relevant_passages": ["Do I need to use Pod Disruption Budgets if my organization doesn't separate these roles?"]}
{"query": "A user asked the following question:\nQuestion: How do I set the number of desired Pods in a PersistentVolumeClaim?\nThis is about the following runbook:\nRunbook Title: Replicas\nRunbook Content: PersistentVolumeClaim retentionReplicas`.spec.replicas` is an optional field that specifies the number of desired Pods. It defaults to 1.  \nShould you manually scale a deployment, example via `kubectl scale\nstatefulset statefulset --replicas=X`, and then you update that StatefulSet\nbased on a manifest (for example: by running `kubectl apply -f\nstatefulset.yaml`), then applying that manifest overwrites the manual scaling\nthat you previously did.  \nIf a [HorizontalPodAutoscaler](/docs/tasks/run-application/horizontal-pod-autoscale/)\n(or any similar API for horizontal scaling) is managing scaling for a\nStatefulset, don't set `.spec.replicas`. Instead, allow the Kubernetes\n{{<glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} to manage\nthe `.spec.replicas` field automatically.\n", "relevant_passages": ["How do I set the number of desired Pods in a PersistentVolumeClaim?"]}
{"query": "A user asked the following question:\nQuestion: What does a Job do when a Bare Pod is terminated?\nThis is about the following runbook:\nRunbook Title: Bare Pods\nRunbook Content: AlternativesBare PodsWhen the node that a Pod is running on reboots or fails, the pod is terminated\nand will not be restarted. However, a Job will create new Pods to replace terminated ones.\nFor this reason, we recommend that you use a Job rather than a bare Pod, even if your application\nrequires only a single Pod.\n", "relevant_passages": ["What does a Job do when a Bare Pod is terminated?"]}
{"query": "A user asked the following question:\nQuestion: How does the name of a ReplicationController affect the Pods it creates?\nThis is about the following runbook:\nRunbook Title: Writing a ReplicationController Manifest\nRunbook Content: Writing a ReplicationController ManifestAs with all other Kubernetes config, a ReplicationController needs `apiVersion`, `kind`, and `metadata` fields.  \nWhen the control plane creates new Pods for a ReplicationController, the `.metadata.name` of the\nReplicationController is part of the basis for naming those Pods.  The name of a ReplicationController must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).  \nFor general information about working with configuration files, see [object management](/docs/concepts/overview/working-with-objects/object-management/).  \nA ReplicationController also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\n", "relevant_passages": ["How does the name of a ReplicationController affect the Pods it creates?"]}
{"query": "A user asked the following question:\nQuestion: What can I do to increase the availability of my application during disruptions?\nThis is about the following runbook:\nRunbook Title: Dealing with disruptions\nRunbook Content: Dealing with disruptionsHere are some ways to mitigate involuntary disruptions:  \n- Ensure your pod [requests the resources](/docs/tasks/configure-pod-container/assign-memory-resource) it needs.\n- Replicate your application if you need higher availability.  (Learn about running replicated\n[stateless](/docs/tasks/run-application/run-stateless-application-deployment/)\nand [stateful](/docs/tasks/run-application/run-replicated-stateful-application/) applications.)\n- For even higher availability when running replicated applications,\nspread applications across racks (using\n[anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity))\nor across zones (if using a\n[multi-zone cluster](/docs/setup/multiple-zones).)  \nThe frequency of voluntary disruptions varies.  On a basic Kubernetes cluster, there are\nno automated voluntary disruptions (only user-triggered ones).  However, your cluster administrator or hosting provider\nmay run some additional services which cause voluntary disruptions. For example,\nrolling out node software updates can cause voluntary disruptions. Also, some implementations\nof cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.\nYour cluster administrator or hosting provider should have documented what level of voluntary\ndisruptions, if any, to expect. Certain configuration options, such as\n[using PriorityClasses](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\nin your pod spec can also cause voluntary (and involuntary) disruptions.\n", "relevant_passages": ["What can I do to increase the availability of my application during disruptions?"]}
{"query": "A user asked the following question:\nQuestion: What types of probes can I use in my Pods?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}Learn more about the following:\n* [Creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).\n* [Debug init containers](/docs/tasks/debug/debug-application/debug-init-containers/).\n* Overview of [kubelet](/docs/reference/command-line-tools-reference/kubelet/) and [kubectl](/docs/reference/kubectl/).\n* [Types of probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe): liveness, readiness, startup probe.\n* [Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers).\n", "relevant_passages": ["What types of probes can I use in my Pods?"]}
{"query": "A user asked the following question:\nQuestion: How does a ReplicationController handle pod failures or terminations?\nThis is about the following runbook:\nRunbook Title: How a ReplicationController works\nRunbook Content: How a ReplicationController worksIf there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the\nReplicationController starts more pods. Unlike manually created pods, the pods maintained by a\nReplicationController are automatically replaced if they fail, are deleted, or are terminated.\nFor example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.\nFor this reason, you should use a ReplicationController even if your application requires\nonly a single pod. A ReplicationController is similar to a process supervisor,\nbut instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods\nacross multiple nodes.  \nReplicationController is often abbreviated to \"rc\" in discussion, and as a shortcut in\nkubectl commands.  \nA simple case is to create one ReplicationController object to reliably run one instance of\na Pod indefinitely.  A more complex use case is to run several identical replicas of a replicated\nservice, such as web servers.\n", "relevant_passages": ["How does a ReplicationController handle pod failures or terminations?"]}
{"query": "A user asked the following question:\nQuestion: Can I use multiple DaemonSets for the same type of daemon?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- enisoc\n- erictune\n- foxish\n- janetkuo\n- kow3ns\ntitle: DaemonSet\napi_metadata:\n- apiVersion: \"apps/v1\"\nkind: \"DaemonSet\"\ndescription: >-\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.\ncontent_type: concept\nweight: 40\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nA _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the\ncluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage\ncollected.  Deleting a DaemonSet will clean up the Pods it created.  \nSome typical uses of a DaemonSet are:  \n- running a cluster storage daemon on every node\n- running a logs collection daemon on every node\n- running a node monitoring daemon on every node  \nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.\nA more complex setup might use multiple DaemonSets for a single type of daemon, but with\ndifferent flags and/or different memory and cpu requests for different hardware types.  \n<!-- body -->\n", "relevant_passages": ["Can I use multiple DaemonSets for the same type of daemon?"]}
{"query": "A user asked the following question:\nQuestion: What happens when I use Indexed completion mode for a Job?\nThis is about the following runbook:\nRunbook Title: Completion mode\nRunbook Content: Writing a Job specCompletion mode{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}  \nJobs with _fixed completion count_ - that is, jobs that have non null\n`.spec.completions` - can have a completion mode that is specified in `.spec.completionMode`:  \n- `NonIndexed` (default): the Job is considered complete when there have been\n`.spec.completions` successfully completed Pods. In other words, each Pod\ncompletion is homologous to each other. Note that Jobs that have null\n`.spec.completions` are implicitly `NonIndexed`.\n- `Indexed`: the Pods of a Job get an associated completion index from 0 to\n`.spec.completions-1`. The index is available through four mechanisms:\n- The Pod annotation `batch.kubernetes.io/job-completion-index`.\n- The Pod label `batch.kubernetes.io/job-completion-index` (for v1.28 and later). Note\nthe feature gate `PodIndexLabel` must be enabled to use this label, and it is enabled\nby default.\n- As part of the Pod hostname, following the pattern `$(job-name)-$(index)`.\nWhen you use an Indexed Job in combination with a\n{{< glossary_tooltip term_id=\"Service\" >}}, Pods within the Job can use\nthe deterministic hostnames to address each other via DNS. For more information about\nhow to configure this, see [Job with Pod-to-Pod Communication](/docs/tasks/job/job-with-pod-to-pod-communication/).\n- From the containerized task, in the environment variable `JOB_COMPLETION_INDEX`.  \nThe Job is considered complete when there is one successfully completed Pod\nfor each index. For more information about how to use this mode, see\n[Indexed Job for Parallel Processing with Static Work Assignment](/docs/tasks/job/indexed-parallel-processing-static/).  \n{{< note >}}\nAlthough rare, more than one Pod could be started for the same index (due to various reasons such as node failures,\nkubelet restarts, or Pod evictions). In this case, only the first Pod that completes successfully will\ncount towards the completion count and update the status of the Job. The other Pods that are running\nor completed for the same index will be deleted by the Job controller once they are detected.\n{{< /note >}}\n", "relevant_passages": ["What happens when I use Indexed completion mode for a Job?"]}
{"query": "A user asked the following question:\nQuestion: How do I set the limit for successful jobs history in a CronJob?\nThis is about the following runbook:\nRunbook Title: Jobs history limits\nRunbook Content: Writing a CronJob specJobs history limitsThe `.spec.successfulJobsHistoryLimit` and `.spec.failedJobsHistoryLimit` fields specify\nhow many completed and failed Jobs should be kept. Both fields are optional.  \n* `.spec.successfulJobsHistoryLimit`: This field specifies the number of successful finished\njobs to keep. The default value is `3`. Setting this field to `0` will not keep any successful jobs.  \n* `.spec.failedJobsHistoryLimit`: This field specifies the number of failed finished jobs to keep.\nThe default value is `1`. Setting this field to `0` will not keep any failed jobs.  \nFor another way to clean up Jobs automatically, see\n[Clean up finished Jobs automatically](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically).\n", "relevant_passages": ["How do I set the limit for successful jobs history in a CronJob?"]}
{"query": "A user asked the following question:\nQuestion: What does a question mark mean in the CronJob schedule syntax?\nThis is about the following runbook:\nRunbook Title: Schedule syntax\nRunbook Content: Writing a CronJob specSchedule syntaxThe `.spec.schedule` field is required. The value of that field follows the [Cron](https://en.wikipedia.org/wiki/Cron) syntax:  \n```\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n# \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n# \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31)\n# \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12)\n# \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6) (Sunday to Saturday)\n# \u2502 \u2502 \u2502 \u2502 \u2502                                   OR sun, mon, tue, wed, thu, fri, sat\n# \u2502 \u2502 \u2502 \u2502 \u2502\n# \u2502 \u2502 \u2502 \u2502 \u2502\n# * * * * *\n```  \nFor example, `0 3 * * 1` means this task is scheduled to run weekly on a Monday at 3 AM.  \nThe format also includes extended \"Vixie cron\" step values. As explained in the\n[FreeBSD manual](https://www.freebsd.org/cgi/man.cgi?crontab%285%29):  \n> Step values can be used in conjunction with ranges. Following a range\n> with `/<number>` specifies skips of the number's value through the\n> range. For example, `0-23/2` can be used in the hours field to specify\n> command execution every other hour (the alternative in the V7 standard is\n> `0,2,4,6,8,10,12,14,16,18,20,22`). Steps are also permitted after an\n> asterisk, so if you want to say \"every two hours\", just use `*/2`.  \n{{< note >}}\nA question mark (`?`) in the schedule has the same meaning as an asterisk `*`, that is,\nit stands for any of available value for a given field.\n{{< /note >}}  \nOther than the standard syntax, some macros like `@monthly` can also be used:  \n| Entry | Description| Equivalent to |\n| ------------- | ------------- |-------------  |\n| @yearly (or @annually)| Run once a year at midnight of 1 January| 0 0 1 1 * |\n| @monthly | Run once a month at midnight of the first day of the month| 0 0 1 * * |\n| @weekly | Run once a week at midnight on Sunday morning| 0 0 * * 0 |\n| @daily (or @midnight)| Run once a day at midnight| 0 0 * * * |\n| @hourly | Run once an hour at the beginning of the hour| 0 * * * * |  \nTo generate CronJob schedule expressions, you can also use web tools like [crontab.guru](https://crontab.guru/).\n", "relevant_passages": ["What does a question mark mean in the CronJob schedule syntax?"]}
{"query": "A user asked the following question:\nQuestion: How do I create an ephemeral container in Kubernetes?\nThis is about the following runbook:\nRunbook Title: What is an ephemeral container?\nRunbook Content: Understanding ephemeral containersWhat is an ephemeral container?Ephemeral containers differ from other containers in that they lack guarantees\nfor resources or execution, and they will never be automatically restarted, so\nthey are not appropriate for building applications.  Ephemeral containers are\ndescribed using the same `ContainerSpec` as regular containers, but many fields\nare incompatible and disallowed for ephemeral containers.  \n- Ephemeral containers may not have ports, so fields such as `ports`,\n`livenessProbe`, `readinessProbe` are disallowed.\n- Pod resource allocations are immutable, so setting `resources` is disallowed.\n- For a complete list of allowed fields, see the [EphemeralContainer reference\ndocumentation](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#ephemeralcontainer-v1-core).  \nEphemeral containers are created using a special `ephemeralcontainers` handler\nin the API rather than by adding them directly to `pod.spec`, so it's not\npossible to add an ephemeral container using `kubectl edit`.  \nLike regular containers, you may not change or remove an ephemeral container\nafter you have added it to a Pod.  \n{{< note >}}\nEphemeral containers are not supported by [static pods](/docs/tasks/configure-pod-container/static-pod/).\n{{< /note >}}\n", "relevant_passages": ["How do I create an ephemeral container in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What does the `.spec.minReadySeconds` field do in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Minimum ready seconds\nRunbook Content: ComponentsMinimum ready seconds{{< feature-state for_k8s_version=\"v1.25\" state=\"stable\" >}}  \n`.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly\ncreated Pod should be running and ready without any of its containers crashing, for it to be considered available.\nThis is used to check progression of a rollout when using a [Rolling Update](#rolling-updates) strategy.\nThis field defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when\na Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).\n", "relevant_passages": ["What does the `.spec.minReadySeconds` field do in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: How do I handle pods that need to terminate after completing their task?\nThis is about the following runbook:\nRunbook Title: Job\nRunbook Content: Alternatives to ReplicationControllerJobUse a [`Job`](/docs/concepts/workloads/controllers/job/) instead of a ReplicationController for pods that are expected to terminate on their own\n(that is, batch jobs).\n", "relevant_passages": ["How do I handle pods that need to terminate after completing their task?"]}
{"query": "A user asked the following question:\nQuestion: How should I name my Job to avoid issues with Pod hostnames?\nThis is about the following runbook:\nRunbook Title: Writing a Job spec\nRunbook Content: Writing a Job specAs with all other Kubernetes config, a Job needs `apiVersion`, `kind`, and `metadata` fields.  \nWhen the control plane creates new Pods for a Job, the `.metadata.name` of the\nJob is part of the basis for naming those Pods. The name of a Job must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\nEven when the name is a DNS subdomain, the name must be no longer than 63\ncharacters.  \nA Job also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\n", "relevant_passages": ["How should I name my Job to avoid issues with Pod hostnames?"]}
{"query": "A user asked the following question:\nQuestion: How can I set a backoff limit per index for my indexed Job?\nThis is about the following runbook:\nRunbook Title: Backoff limit per index {#backoff-limit-per-index}\nRunbook Content: Handling Pod and container failuresBackoff limit per index {#backoff-limit-per-index}{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \n{{< note >}}\nYou can only configure the backoff limit per index for an [Indexed](#completion-mode) Job, if you\nhave the `JobBackoffLimitPerIndex` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nenabled in your cluster.\n{{< /note >}}  \nWhen you run an [indexed](#completion-mode) Job, you can choose to handle retries\nfor pod failures independently for each index. To do so, set the\n`.spec.backoffLimitPerIndex` to specify the maximal number of pod failures\nper index.  \nWhen the per-index backoff limit is exceeded for an index, Kubernetes considers the index as failed and adds it to the\n`.status.failedIndexes` field. The succeeded indexes, those with a successfully\nexecuted pods, are recorded in the `.status.completedIndexes` field, regardless of whether you set\nthe `backoffLimitPerIndex` field.  \nNote that a failing index does not interrupt execution of other indexes.\nOnce all indexes finish for a Job where you specified a backoff limit per index,\nif at least one of those indexes did fail, the Job controller marks the overall\nJob as failed, by setting the Failed condition in the status. The Job gets\nmarked as failed even if some, potentially nearly all, of the indexes were\nprocessed successfully.  \nYou can additionally limit the maximal number of indexes marked failed by\nsetting the `.spec.maxFailedIndexes` field.\nWhen the number of failed indexes exceeds the `maxFailedIndexes` field, the\nJob controller triggers termination of all remaining running Pods for that Job.\nOnce all pods are terminated, the entire Job is marked failed by the Job\ncontroller, by setting the Failed condition in the Job status.  \nHere is an example manifest for a Job that defines a `backoffLimitPerIndex`:  \n{{< code_sample file=\"/controllers/job-backoff-limit-per-index-example.yaml\" >}}  \nIn the example above, the Job controller allows for one restart for each\nof the indexes. When the total number of failed indexes exceeds 5, then\nthe entire Job is terminated.  \nOnce the job is finished, the Job status looks as follows:  \n```sh\nkubectl get -o yaml job job-backoff-limit-per-index-example\n```  \n```yaml\nstatus:\ncompletedIndexes: 1,3,5,7,9\nfailedIndexes: 0,2,4,6,8\nsucceeded: 5          # 1 succeeded pod for each of 5 succeeded indexes\nfailed: 10            # 2 failed pods (1 retry) for each of 5 failed indexes\nconditions:\n- message: Job has failed indexes\nreason: FailedIndexes\nstatus: \"True\"\ntype: FailureTarget\n- message: Job has failed indexes\nreason: FailedIndexes\nstatus: \"True\"\ntype: Failed\n```  \nThe Job controller adds the `FailureTarget` Job condition to trigger\n[Job termination and cleanup](#job-termination-and-cleanup). When all of the\nJob Pods are terminated, the Job controller adds the `Failed` condition\nwith the same values for `reason` and `message` as the `FailureTarget` Job\ncondition. For details, see [Termination of Job Pods](#termination-of-job-pods).  \nAdditionally, you may want to use the per-index backoff along with a\n[pod failure policy](#pod-failure-policy). When using\nper-index backoff, there is a new `FailIndex` action available which allows you to\navoid unnecessary retries within an index.\n", "relevant_passages": ["How can I set a backoff limit per index for my indexed Job?"]}
{"query": "A user asked the following question:\nQuestion: How does the identity of StatefulSet Pods work when they are rescheduled?\nThis is about the following runbook:\nRunbook Title: Pod Identity\nRunbook Content: Pod IdentityStatefulSet Pods have a unique identity that consists of an ordinal, a\nstable network identity, and stable storage. The identity sticks to the Pod,\nregardless of which node it's (re)scheduled on.\n", "relevant_passages": ["How does the identity of StatefulSet Pods work when they are rescheduled?"]}
{"query": "A user asked the following question:\nQuestion: How can I see the status of the Deployment rollout?\nThis is about the following runbook:\nRunbook Title: Creating a Deployment\nRunbook Content: Creating a DeploymentThe following is an example of a Deployment. It creates a ReplicaSet to bring up three `nginx` Pods:  \n{{% code_sample file=\"controllers/nginx-deployment.yaml\" %}}  \nIn this example:  \n* A Deployment named `nginx-deployment` is created, indicated by the\n`.metadata.name` field. This name will become the basis for the ReplicaSets\nand Pods which are created later. See [Writing a Deployment Spec](#writing-a-deployment-spec)\nfor more details.\n* The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the `.spec.replicas` field.\n* The `.spec.selector` field defines how the created ReplicaSet finds which Pods to manage.\nIn this case, you select a label that is defined in the Pod template (`app: nginx`).\nHowever, more sophisticated selection rules are possible,\nas long as the Pod template itself satisfies the rule.  \n{{< note >}}\nThe `.spec.selector.matchLabels` field is a map of {key,value} pairs.\nA single {key,value} in the `matchLabels` map is equivalent to an element of `matchExpressions`,\nwhose `key` field is \"key\", the `operator` is \"In\", and the `values` array contains only \"value\".\nAll of the requirements, from both `matchLabels` and `matchExpressions`, must be satisfied in order to match.\n{{< /note >}}  \n* The `template` field contains the following sub-fields:\n* The Pods are labeled `app: nginx`using the `.metadata.labels` field.\n* The Pod template's specification, or `.template.spec` field, indicates that\nthe Pods run one container, `nginx`, which runs the `nginx`\n[Docker Hub](https://hub.docker.com/) image at version 1.14.2.\n* Create one container and name it `nginx` using the `.spec.template.spec.containers[0].name` field.  \nBefore you begin, make sure your Kubernetes cluster is up and running.\nFollow the steps given below to create the above Deployment:  \n1. Create the Deployment by running the following command:  \n```shell\nkubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml\n```  \n2. Run `kubectl get deployments` to check if the Deployment was created.  \nIf the Deployment is still being created, the output is similar to the following:\n```\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   0/3     0            0           1s\n```\nWhen you inspect the Deployments in your cluster, the following fields are displayed:\n* `NAME` lists the names of the Deployments in the namespace.\n* `READY` displays how many replicas of the application are available to your users. It follows the pattern ready/desired.\n* `UP-TO-DATE` displays the number of replicas that have been updated to achieve the desired state.\n* `AVAILABLE` displays how many replicas of the application are available to your users.\n* `AGE` displays the amount of time that the application has been running.  \nNotice how the number of desired replicas is 3 according to `.spec.replicas` field.  \n3. To see the Deployment rollout status, run `kubectl rollout status deployment/nginx-deployment`.  \nThe output is similar to:\n```\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\ndeployment \"nginx-deployment\" successfully rolled out\n```  \n4. Run the `kubectl get deployments` again a few seconds later.\nThe output is similar to this:\n```\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           18s\n```\nNotice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.  \n5. To see the ReplicaSet (`rs`) created by the Deployment, run `kubectl get rs`. The output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY   AGE\nnginx-deployment-75675f5897   3         3         3       18s\n```\nReplicaSet output shows the following fields:  \n* `NAME` lists the names of the ReplicaSets in the namespace.\n* `DESIRED` displays the desired number of _replicas_ of the application, which you define when you create the Deployment. This is the _desired state_.\n* `CURRENT` displays how many replicas are currently running.\n* `READY` displays how many replicas of the application are available to your users.\n* `AGE` displays the amount of time that the application has been running.  \nNotice that the name of the ReplicaSet is always formatted as\n`[DEPLOYMENT-NAME]-[HASH]`. This name will become the basis for the Pods\nwhich are created.  \nThe `HASH` string is the same as the `pod-template-hash` label on the ReplicaSet.  \n6. To see the labels automatically generated for each Pod, run `kubectl get pods --show-labels`.\nThe output is similar to:\n```\nNAME                                READY     STATUS    RESTARTS   AGE       LABELS\nnginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\nnginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\nnginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\n```\nThe created ReplicaSet ensures that there are three `nginx` Pods.  \n{{< note >}}\nYou must specify an appropriate selector and Pod template labels in a Deployment\n(in this case, `app: nginx`).  \nDo not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.\n{{< /note >}}\n", "relevant_passages": ["How can I see the status of the Deployment rollout?"]}
{"query": "A user asked the following question:\nQuestion: What are the different QoS classes that Kubernetes uses?\nThis is about the following runbook:\nRunbook Title: Quality of Service classes\nRunbook Content: Quality of Service classesKubernetes classifies the Pods that you run and allocates each Pod into a specific\n_quality of service (QoS) class_. Kubernetes uses that classification to influence how different\npods are handled. Kubernetes does this classification based on the\n[resource requests](/docs/concepts/configuration/manage-resources-containers/)\nof the {{< glossary_tooltip text=\"Containers\" term_id=\"container\" >}} in that Pod, along with\nhow those requests relate to resource limits.\nThis is known as {{< glossary_tooltip text=\"Quality of Service\" term_id=\"qos-class\" >}}\n(QoS) class. Kubernetes assigns every Pod a QoS class based on the resource requests\nand limits of its component Containers. QoS classes are used by Kubernetes to decide\nwhich Pods to evict from a Node experiencing\n[Node Pressure](/docs/concepts/scheduling-eviction/node-pressure-eviction/). The possible\nQoS classes are `Guaranteed`, `Burstable`, and `BestEffort`. When a Node runs out of resources,\nKubernetes will first evict `BestEffort` Pods running on that Node, followed by `Burstable` and\nfinally `Guaranteed` Pods. When this eviction is due to resource pressure, only Pods exceeding\nresource requests are candidates for eviction.\n", "relevant_passages": ["What are the different QoS classes that Kubernetes uses?"]}
{"query": "A user asked the following question:\nQuestion: How can I automatically clean up finished jobs in Kubernetes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Read [Clean up Jobs automatically](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically)  \n* Refer to the [Kubernetes Enhancement Proposal](https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md)\n(KEP) for adding this mechanism.\n", "relevant_passages": ["How can I automatically clean up finished jobs in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What does the Kubernetes control plane do before updating a Pod in a Rolling Update?\nThis is about the following runbook:\nRunbook Title: Rolling Updates\nRunbook Content: Rolling UpdatesWhen a StatefulSet's `.spec.updateStrategy.type` is set to `RollingUpdate`, the\nStatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed\nin the same order as Pod termination (from the largest ordinal to the smallest), updating\neach Pod one at a time.  \nThe Kubernetes control plane waits until an updated Pod is Running and Ready prior\nto updating its predecessor. If you have set `.spec.minReadySeconds` (see\n[Minimum Ready Seconds](#minimum-ready-seconds)), the control plane additionally waits that\namount of time after the Pod turns ready, before moving on.\n", "relevant_passages": ["What does the Kubernetes control plane do before updating a Pod in a Rolling Update?"]}
{"query": "A user asked the following question:\nQuestion: Can I manage static pods with kubectl?\nThis is about the following runbook:\nRunbook Title: Static Pods\nRunbook Content: Alternatives to DaemonSetStatic PodsIt is possible to create Pods by writing a file to a certain directory watched by Kubelet.  These\nare called [static pods](/docs/tasks/configure-pod-container/static-pod/).\nUnlike DaemonSet, static Pods cannot be managed with kubectl\nor other Kubernetes API clients.  Static Pods do not depend on the apiserver, making them useful\nin cluster bootstrapping cases.  Also, static Pods may be deprecated in the future.\n", "relevant_passages": ["Can I manage static pods with kubectl?"]}
{"query": "A user asked the following question:\nQuestion: When is a Pod considered ready in relation to minReadySeconds?\nThis is about the following runbook:\nRunbook Title: Min Ready Seconds\nRunbook Content: Writing a Deployment SpecMin Ready Seconds`.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly\ncreated Pod should be ready without any of its containers crashing, for it to be considered available.\nThis defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when\na Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).\n", "relevant_passages": ["When is a Pod considered ready in relation to minReadySeconds?"]}
{"query": "A user asked the following question:\nQuestion: How does time skew affect the TTL-after-finished controller in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Time skew\nRunbook Content: CaveatsTime skewBecause the TTL-after-finished controller uses timestamps stored in the Kubernetes jobs to\ndetermine whether the TTL has expired or not, this feature is sensitive to time\nskew in your cluster, which may cause the control plane to clean up Job objects\nat the wrong time.  \nClocks aren't always correct, but the difference should be\nvery small. Please be aware of this risk when setting a non-zero TTL.\n", "relevant_passages": ["How does time skew affect the TTL-after-finished controller in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: How are resource allocations for Pods controlled in Linux?\nThis is about the following runbook:\nRunbook Title: Init containers and Linux cgroups {#cgroups}\nRunbook Content: Detailed behaviorInit containers and Linux cgroups {#cgroups}On Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod\nrequest and limit, the same as the scheduler.  \n{{< comment >}}\nThis section also present under [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/) page.\nIf you're editing this section, change both places.\n{{< /comment >}}\n", "relevant_passages": ["How are resource allocations for Pods controlled in Linux?"]}
{"query": "A user asked the following question:\nQuestion: Where can I find detailed information on CronJob schedule formats?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods/) and\n[Jobs](/docs/concepts/workloads/controllers/job/), two concepts\nthat CronJobs rely upon.\n* Read about the detailed [format](https://pkg.go.dev/github.com/robfig/cron/v3#hdr-CRON_Expression_Format)\nof CronJob `.spec.schedule` fields.\n* For instructions on creating and working with CronJobs, and for an example\nof a CronJob manifest,\nsee [Running automated tasks with CronJobs](/docs/tasks/job/automated-tasks-with-cron-jobs/).\n* `CronJob` is part of the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/cron-job-v1\" >}}\nAPI reference for more details.\n", "relevant_passages": ["Where can I find detailed information on CronJob schedule formats?"]}
{"query": "A user asked the following question:\nQuestion: How does a ReplicaSet work with Deployments?\nThis is about the following runbook:\nRunbook Title: ReplicaSet\nRunbook Content: Alternatives to ReplicationControllerReplicaSet[`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is the next-generation ReplicationController that supports the new [set-based label selector](/docs/concepts/overview/working-with-objects/labels/#set-based-requirement).\nIt's mainly used by [Deployment](/docs/concepts/workloads/controllers/deployment/) as a mechanism to orchestrate pod creation, deletion and updates.\nNote that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don't require updates at all.\n", "relevant_passages": ["How does a ReplicaSet work with Deployments?"]}
{"query": "A user asked the following question:\nQuestion: How do I access the ReplicationController API object documentation?\nThis is about the following runbook:\nRunbook Title: API Object\nRunbook Content: API ObjectReplication controller is a top-level resource in the Kubernetes REST API. More details about the\nAPI object can be found at:\n[ReplicationController API object](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#replicationcontroller-v1-core).\n", "relevant_passages": ["How do I access the ReplicationController API object documentation?"]}
{"query": "A user asked the following question:\nQuestion: How does each Pod in a StatefulSet get its storage?\nThis is about the following runbook:\nRunbook Title: Stable Storage\nRunbook Content: Pod IdentityStable StorageFor each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one\nPersistentVolumeClaim. In the nginx example above, each Pod receives a single PersistentVolume\nwith a StorageClass of `my-storage-class` and 1 GiB of provisioned storage. If no StorageClass\nis specified, then the default StorageClass will be used. When a Pod is (re)scheduled\nonto a node, its `volumeMounts` mount the PersistentVolumes associated with its\nPersistentVolume Claims. Note that, the PersistentVolumes associated with the\nPods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.\nThis must be done manually.\n", "relevant_passages": ["How does each Pod in a StatefulSet get its storage?"]}
{"query": "A user asked the following question:\nQuestion: Can I create a new ReplicaSet after deleting the old one?\nThis is about the following runbook:\nRunbook Title: Deleting just a ReplicaSet\nRunbook Content: Working with ReplicaSetsDeleting just a ReplicaSetYou can delete a ReplicaSet without affecting any of its Pods using\n[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete)\nwith the `--cascade=orphan` option.\nWhen using the REST API or the `client-go` library, you must set `propagationPolicy` to `Orphan`.\nFor example:  \n```shell\nkubectl proxy --port=8080\ncurl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \\\n-d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Orphan\"}' \\\n-H \"Content-Type: application/json\"\n```  \nOnce the original is deleted, you can create a new ReplicaSet to replace it. As long\nas the old and new `.spec.selector` are the same, then the new one will adopt the old Pods.\nHowever, it will not make any effort to make existing Pods match a new, different pod template.\nTo update Pods to a new spec in a controlled way, use a\n[Deployment](/docs/concepts/workloads/controllers/deployment/#creating-a-deployment), as\nReplicaSets do not support a rolling update directly.\n", "relevant_passages": ["Can I create a new ReplicaSet after deleting the old one?"]}
{"query": "A user asked the following question:\nQuestion: What are some examples of workload resources that handle Pods?\nThis is about the following runbook:\nRunbook Title: Pods and controllers\nRunbook Content: Working with PodsPods and controllersYou can use workload resources to create and manage multiple Pods for you. A controller\nfor the resource handles replication and rollout and automatic healing in case of\nPod failure. For example, if a Node fails, a controller notices that Pods on that\nNode have stopped working and creates a replacement Pod. The scheduler places the\nreplacement Pod onto a healthy Node.  \nHere are some examples of workload resources that manage one or more Pods:  \n* {{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}\n* {{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}}\n* {{< glossary_tooltip text=\"DaemonSet\" term_id=\"daemonset\" >}}\n", "relevant_passages": ["What are some examples of workload resources that handle Pods?"]}
{"query": "A user asked the following question:\nQuestion: Why should I use a ReplicationController instead of just Bare Pods?\nThis is about the following runbook:\nRunbook Title: Bare Pods\nRunbook Content: Alternatives to ReplicationControllerBare PodsUnlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node.  A ReplicationController delegates local container restarts to some agent on the node, such as the kubelet.\n", "relevant_passages": ["Why should I use a ReplicationController instead of just Bare Pods?"]}
{"query": "A user asked the following question:\nQuestion: How can I create a custom controller for Pods using a single Job?\nThis is about the following runbook:\nRunbook Title: Single Job starts controller Pod\nRunbook Content: AlternativesSingle Job starts controller PodAnother pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort\nof custom controller for those Pods. This allows the most flexibility, but may be somewhat\ncomplicated to get started with and offers less integration with Kubernetes.  \nOne example of this pattern would be a Job which starts a Pod which runs a script that in turn\nstarts a Spark master controller (see [spark example](https://github.com/kubernetes/examples/tree/master/staging/spark/README.md)),\nruns a spark driver, and then cleans up.  \nAn advantage of this approach is that the overall process gets the completion guarantee of a Job\nobject, but maintains complete control over what Pods are created and how work is assigned to them.\n", "relevant_passages": ["How can I create a custom controller for Pods using a single Job?"]}
{"query": "A user asked the following question:\nQuestion: How do I update the image of my nginx Pods in a Deployment?\nThis is about the following runbook:\nRunbook Title: Updating a Deployment\nRunbook Content: Updating a Deployment{{< note >}}\nA Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, `.spec.template`)\nis changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.\n{{< /note >}}  \nFollow the steps given below to update your Deployment:  \n1. Let's update the nginx Pods to use the `nginx:1.16.1` image instead of the `nginx:1.14.2` image.  \n```shell\nkubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1\n```  \nor use the following command:  \n```shell\nkubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n```\nwhere `deployment/nginx-deployment` indicates the Deployment,\n`nginx` indicates the Container the update will take place and\n`nginx:1.16.1` indicates the new image and its tag.  \nThe output is similar to:  \n```\ndeployment.apps/nginx-deployment image updated\n```  \nAlternatively, you can `edit` the Deployment and change `.spec.template.spec.containers[0].image` from `nginx:1.14.2` to `nginx:1.16.1`:  \n```shell\nkubectl edit deployment/nginx-deployment\n```  \nThe output is similar to:  \n```\ndeployment.apps/nginx-deployment edited\n```  \n2. To see the rollout status, run:  \n```shell\nkubectl rollout status deployment/nginx-deployment\n```  \nThe output is similar to this:  \n```\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\n```  \nor  \n```\ndeployment \"nginx-deployment\" successfully rolled out\n```  \nGet more details on your updated Deployment:  \n* After the rollout succeeds, you can view the Deployment by running `kubectl get deployments`.\nThe output is similar to this:  \n```ini\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           36s\n```  \n* Run `kubectl get rs` to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it\nup to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.  \n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY   AGE\nnginx-deployment-1564180365   3         3         3       6s\nnginx-deployment-2035384211   0         0         0       36s\n```  \n* Running `get pods` should now show only the new Pods:  \n```shell\nkubectl get pods\n```  \nThe output is similar to this:\n```\nNAME                                READY     STATUS    RESTARTS   AGE\nnginx-deployment-1564180365-khku8   1/1       Running   0          14s\nnginx-deployment-1564180365-nacti   1/1       Running   0          14s\nnginx-deployment-1564180365-z9gth   1/1       Running   0          14s\n```  \nNext time you want to update these Pods, you only need to update the Deployment's Pod template again.  \nDeployment ensures that only a certain number of Pods are down while they are being updated. By default,\nit ensures that at least 75% of the desired number of Pods are up (25% max unavailable).  \nDeployment also ensures that only a certain number of Pods are created above the desired number of Pods.\nBy default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).  \nFor example, if you look at the above Deployment closely, you will see that it first creates a new Pod,\nthen deletes an old Pod, and creates another new one. It does not kill old Pods until a sufficient number of\nnew Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.\nIt makes sure that at least 3 Pods are available and that at max 4 Pods in total are available. In case of\na Deployment with 4 replicas, the number of Pods would be between 3 and 5.  \n* Get details of your Deployment:\n```shell\nkubectl describe deployments\n```\nThe output is similar to this:\n```\nName:                   nginx-deployment\nNamespace:              default\nCreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000\nLabels:                 app=nginx\nAnnotations:            deployment.kubernetes.io/revision=2\nSelector:               app=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx\nContainers:\nnginx:\nImage:        nginx:1.16.1\nPort:         80/TCP\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    NewReplicaSetAvailable\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)\nEvents:\nType    Reason             Age   From                   Message\n----    ------             ----  ----                   -------\nNormal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3\nNormal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1\nNormal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2\nNormal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2\nNormal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1\nNormal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3\nNormal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0\n```\nHere you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)\nand scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet\n(nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet\nto 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times.\nIt then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy.\nFinally, you'll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.  \n{{< note >}}\nKubernetes doesn't count terminating Pods when calculating the number of `availableReplicas`, which must be between\n`replicas - maxUnavailable` and `replicas + maxSurge`. As a result, you might notice that there are more Pods than\nexpected during a rollout, and that the total resources consumed by the Deployment is more than `replicas + maxSurge`\nuntil the `terminationGracePeriodSeconds` of the terminating Pods expires.\n{{< /note >}}\n", "relevant_passages": ["How do I update the image of my nginx Pods in a Deployment?"]}
{"query": "A user asked the following question:\nQuestion: Do the labels on the ReplicationController affect its behavior?\nThis is about the following runbook:\nRunbook Title: Labels on the ReplicationController\nRunbook Content: Writing a ReplicationController ManifestLabels on the ReplicationControllerThe ReplicationController can itself have labels (`.metadata.labels`).  Typically, you\nwould set these the same as the `.spec.template.metadata.labels`; if `.metadata.labels` is not specified\nthen it defaults to  `.spec.template.metadata.labels`. However, they are allowed to be\ndifferent, and the `.metadata.labels` do not affect the behavior of the ReplicationController.\n", "relevant_passages": ["Do the labels on the ReplicationController affect its behavior?"]}
{"query": "A user asked the following question:\nQuestion: Can you give me an example of a sidecar container configuration in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Example application {#sidecar-example}\nRunbook Content: Sidecar containers in Kubernetes {#pod-sidecar-containers}Example application {#sidecar-example}Here's an example of a Deployment with two containers, one of which is a sidecar:  \n{{% code_sample language=\"yaml\" file=\"application/deployment-sidecar.yaml\" %}}\n", "relevant_passages": ["Can you give me an example of a sidecar container configuration in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: Where can I learn more about sidecar containers?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Get hands-on experience\n[attaching handlers to container lifecycle events](/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/).  \n* Get hands-on experience\n[configuring Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).  \n* Learn more about [container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/).  \n* Learn more about [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/).  \n* For detailed information about Pod and container status in the API, see\nthe API reference documentation covering\n[`status`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus) for Pod.\n", "relevant_passages": ["Where can I learn more about sidecar containers?"]}
{"query": "A user asked the following question:\nQuestion: What happens to running Pods when I suspend a Job?\nThis is about the following runbook:\nRunbook Title: Suspending a Job\nRunbook Content: Advanced usageSuspending a Job{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}  \nWhen a Job is created, the Job controller will immediately begin creating Pods\nto satisfy the Job's requirements and will continue to do so until the Job is\ncomplete. However, you may want to temporarily suspend a Job's execution and\nresume it later, or start Jobs in suspended state and have a custom controller\ndecide later when to start them.  \nTo suspend a Job, you can update the `.spec.suspend` field of\nthe Job to true; later, when you want to resume it again, update it to false.\nCreating a Job with `.spec.suspend` set to true will create it in the suspended\nstate.  \nWhen a Job is resumed from suspension, its `.status.startTime` field will be\nreset to the current time. This means that the `.spec.activeDeadlineSeconds`\ntimer will be stopped and reset when a Job is suspended and resumed.  \nWhen you suspend a Job, any running Pods that don't have a status of `Completed`\nwill be [terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)\nwith a SIGTERM signal. The Pod's graceful termination period will be honored and\nyour Pod must handle this signal in this period. This may involve saving\nprogress for later or undoing changes. Pods terminated this way will not count\ntowards the Job's `completions` count.  \nAn example Job definition in the suspended state can be like so:  \n```shell\nkubectl get job myjob -o yaml\n```  \n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: myjob\nspec:\nsuspend: true\nparallelism: 1\ncompletions: 5\ntemplate:\nspec:\n...\n```  \nYou can also toggle Job suspension by patching the Job using the command line.  \nSuspend an active Job:  \n```shell\nkubectl patch job/myjob --type=strategic --patch '{\"spec\":{\"suspend\":true}}'\n```  \nResume a suspended Job:  \n```shell\nkubectl patch job/myjob --type=strategic --patch '{\"spec\":{\"suspend\":false}}'\n```  \nThe Job's status can be used to determine if a Job is suspended or has been\nsuspended in the past:  \n```shell\nkubectl get jobs/myjob -o yaml\n```  \n```yaml\napiVersion: batch/v1\nkind: Job\n# .metadata and .spec omitted\nstatus:\nconditions:\n- lastProbeTime: \"2021-02-05T13:14:33Z\"\nlastTransitionTime: \"2021-02-05T13:14:33Z\"\nstatus: \"True\"\ntype: Suspended\nstartTime: \"2021-02-05T13:13:48Z\"\n```  \nThe Job condition of type \"Suspended\" with status \"True\" means the Job is\nsuspended; the `lastTransitionTime` field can be used to determine how long the\nJob has been suspended for. If the status of that condition is \"False\", then the\nJob was previously suspended and is now running. If such a condition does not\nexist in the Job's status, the Job has never been stopped.  \nEvents are also created when the Job is suspended and resumed:  \n```shell\nkubectl describe jobs/myjob\n```  \n```\nName:           myjob\n...\nEvents:\nType    Reason            Age   From            Message\n----    ------            ----  ----            -------\nNormal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl\nNormal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl\nNormal  Suspended         11m   job-controller  Job suspended\nNormal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44\nNormal  Resumed           3s    job-controller  Job resumed\n```  \nThe last four events, particularly the \"Suspended\" and \"Resumed\" events, are\ndirectly a result of toggling the `.spec.suspend` field. In the time between\nthese two events, we see that no Pods were created, but Pod creation restarted\nas soon as the Job was resumed.\n", "relevant_passages": ["What happens to running Pods when I suspend a Job?"]}
{"query": "A user asked the following question:\nQuestion: What should I do to safely drain a node?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Follow steps to protect your application by [configuring a Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).  \n* Learn more about [draining nodes](/docs/tasks/administer-cluster/safely-drain-node/)  \n* Learn about [updating a deployment](/docs/concepts/workloads/controllers/deployment/#updating-a-deployment)\nincluding steps to maintain its availability during the rollout.\n", "relevant_passages": ["What should I do to safely drain a node?"]}
{"query": "A user asked the following question:\nQuestion: When would I use a DaemonSet in my Kubernetes cluster?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: \"Workload Management\"\nweight: 20\nsimple_list: true\n---  \nKubernetes provides several built-in APIs for declarative management of your\n{{< glossary_tooltip text=\"workloads\" term_id=\"workload\" >}}\nand the components of those workloads.  \nUltimately, your applications run as containers inside\n{{< glossary_tooltip term_id=\"Pod\" text=\"Pods\" >}}; however, managing individual\nPods would be a lot of effort. For example, if a Pod fails, you probably want to\nrun a new Pod to replace it. Kubernetes can do that for you.  \nYou use the Kubernetes API to create a workload\n{{< glossary_tooltip text=\"object\" term_id=\"object\" >}} that represents a higher abstraction level\nthan a Pod, and then the Kubernetes\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} automatically manages\nPod objects on your behalf, based on the specification for the workload object you defined.  \nThe built-in APIs for managing workloads are:  \n[Deployment](/docs/concepts/workloads/controllers/deployment/) (and, indirectly, [ReplicaSet](/docs/concepts/workloads/controllers/replicaset/)),\nthe most common way to run an application on your cluster.\nDeployment is a good fit for managing a stateless application workload on your cluster, where\nany Pod in the Deployment is interchangeable and can be replaced if needed.\n(Deployments are a replacement for the legacy\n{{< glossary_tooltip text=\"ReplicationController\" term_id=\"replication-controller\" >}} API).  \nA [StatefulSet](/docs/concepts/workloads/controllers/statefulset/) lets you\nmanage one or more Pods \u2013 all running the same application code \u2013 where the Pods rely\non having a distinct identity. This is different from a Deployment where the Pods are\nexpected to be interchangeable.\nThe most common use for a StatefulSet is to be able to make a link between its Pods and\ntheir persistent storage. For example, you can run a StatefulSet that associates each Pod\nwith a [PersistentVolume](/docs/concepts/storage/persistent-volumes/). If one of the Pods\nin the StatefulSet fails, Kubernetes makes a replacement Pod that is connected to the\nsame PersistentVolume.  \nA [DaemonSet](/docs/concepts/workloads/controllers/daemonset/) defines Pods that provide\nfacilities that are local to a specific {{< glossary_tooltip text=\"node\" term_id=\"node\" >}};\nfor example, a driver that lets containers on that node access a storage system. You use a DaemonSet\nwhen the driver, or other node-level service, has to run on the node where it's useful.\nEach Pod in a DaemonSet performs a role similar to a system daemon on a classic Unix / POSIX\nserver.\nA DaemonSet might be fundamental to the operation of your cluster,\nsuch as a plugin to let that node access\n[cluster networking](/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-network-model),\nit might help you to manage the node,\nor it could provide less essential facilities that enhance the container platform you are running.\nYou can run DaemonSets (and their pods) across every node in your cluster, or across just a subset (for example,\nonly install the GPU accelerator driver on nodes that have a GPU installed).  \nYou can use a [Job](/docs/concepts/workloads/controllers/job/) and / or\na [CronJob](/docs/concepts/workloads/controllers/cron-jobs/) to\ndefine tasks that run to completion and then stop. A Job represents a one-off task,\nwhereas each CronJob repeats according to a schedule.  \nOther topics in this section:\n<!-- relies on simple_list: true in the front matter -->\n", "relevant_passages": ["When would I use a DaemonSet in my Kubernetes cluster?"]}
{"query": "A user asked the following question:\nQuestion: Is it possible to run daemons in containers without using Pods?\nThis is about the following runbook:\nRunbook Title: Init scripts\nRunbook Content: Alternatives to DaemonSetInit scriptsIt is certainly possible to run daemon processes by directly starting them on a node (e.g. using\n`init`, `upstartd`, or `systemd`).  This is perfectly fine.  However, there are several advantages to\nrunning such processes via a DaemonSet:  \n- Ability to monitor and manage logs for daemons in the same way as applications.\n- Same config language and tools (e.g. Pod templates, `kubectl`) for daemons and applications.\n- Running daemons in containers with resource limits increases isolation between daemons from app\ncontainers.  However, this can also be accomplished by running the daemons in a container but not in a Pod.\n", "relevant_passages": ["Is it possible to run daemons in containers without using Pods?"]}
{"query": "A user asked the following question:\nQuestion: Can I change the number of Pods after creating a ReplicaSet?\nThis is about the following runbook:\nRunbook Title: Replicas\nRunbook Content: Writing a ReplicaSet manifestReplicasYou can specify how many Pods should run concurrently by setting `.spec.replicas`. The ReplicaSet will create/delete\nits Pods to match this number.  \nIf you do not specify `.spec.replicas`, then it defaults to 1.\n", "relevant_passages": ["Can I change the number of Pods after creating a ReplicaSet?"]}
{"query": "A user asked the following question:\nQuestion: What happens to my Bare Pods if a node fails?\nThis is about the following runbook:\nRunbook Title: Bare Pods\nRunbook Content: Alternatives to ReplicaSetBare PodsUnlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or\nterminated for any reason, such as in the case of node failure or disruptive node maintenance,\nsuch as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your\napplication requires only a single Pod. Think of it similarly to a process supervisor, only it\nsupervises multiple Pods across multiple nodes instead of individual processes on a single node. A\nReplicaSet delegates local container restarts to some agent on the node such as Kubelet.\n", "relevant_passages": ["What happens to my Bare Pods if a node fails?"]}
{"query": "A user asked the following question:\nQuestion: What should I consider regarding application readiness during a rolling update?\nThis is about the following runbook:\nRunbook Title: Rolling updates\nRunbook Content: Common usage patternsRolling updatesThe ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.  \nAs explained in [#1353](https://issue.k8s.io/1353), the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.  \nIdeally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.  \nThe two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.\n", "relevant_passages": ["What should I consider regarding application readiness during a rolling update?"]}
{"query": "A user asked the following question:\nQuestion: How can I find out why a container is in the Waiting state?\nThis is about the following runbook:\nRunbook Title: `Waiting` {#container-state-waiting}\nRunbook Content: Container states`Waiting` {#container-state-waiting}If a container is not in either the `Running` or `Terminated` state, it is `Waiting`.\nA container in the `Waiting` state is still running the operations it requires in\norder to complete start up: for example, pulling the container image from a container\nimage registry, or applying {{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}\ndata.\nWhen you use `kubectl` to query a Pod with a container that is `Waiting`, you also see\na Reason field to summarize why the container is in that state.\n", "relevant_passages": ["How can I find out why a container is in the Waiting state?"]}
{"query": "A user asked the following question:\nQuestion: How can I set the maximum number of unavailable Pods during an update?\nThis is about the following runbook:\nRunbook Title: Maximum unavailable Pods\nRunbook Content: Rolling UpdatesMaximum unavailable Pods{{< feature-state for_k8s_version=\"v1.24\" state=\"alpha\" >}}  \nYou can control the maximum number of Pods that can be unavailable during an update\nby specifying the `.spec.updateStrategy.rollingUpdate.maxUnavailable` field.\nThe value can be an absolute number (for example, `5`) or a percentage of desired\nPods (for example, `10%`). Absolute number is calculated from the percentage value\nby rounding it up. This field cannot be 0. The default setting is 1.  \nThis field applies to all Pods in the range `0` to `replicas - 1`. If there is any\nunavailable Pod in the range `0` to `replicas - 1`, it will be counted towards\n`maxUnavailable`.  \n{{< note >}}\nThe `maxUnavailable` field is in Alpha stage and it is honored only by API servers\nthat are running with the `MaxUnavailableStatefulSet`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nenabled.\n{{< /note >}}\n", "relevant_passages": ["How can I set the maximum number of unavailable Pods during an update?"]}
{"query": "A user asked the following question:\nQuestion: How can I check the status of a stuck rollout?\nThis is about the following runbook:\nRunbook Title: Rolling Back a Deployment\nRunbook Content: Rolling Back a DeploymentSometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.\nBy default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want\n(you can change that by modifying revision history limit).  \n{{< note >}}\nA Deployment's revision is created when a Deployment's rollout is triggered. This means that the\nnew revision is created if and only if the Deployment's Pod template (`.spec.template`) is changed,\nfor example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,\ndo not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.\nThis means that when you roll back to an earlier revision, only the Deployment's Pod template part is\nrolled back.\n{{< /note >}}  \n* Suppose that you made a typo while updating the Deployment, by putting the image name as `nginx:1.161` instead of `nginx:1.16.1`:  \n```shell\nkubectl set image deployment/nginx-deployment nginx=nginx:1.161\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment image updated\n```  \n* The rollout gets stuck. You can verify it by checking the rollout status:  \n```shell\nkubectl rollout status deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\n```  \n* Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,\n[read more here](#deployment-status).  \n* You see that the number of old replicas (adding the replica count from\n`nginx-deployment-1564180365` and `nginx-deployment-2035384211`) is 3, and the number of\nnew replicas (from `nginx-deployment-3066724191`) is 1.  \n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY   AGE\nnginx-deployment-1564180365   3         3         3       25s\nnginx-deployment-2035384211   0         0         0       36s\nnginx-deployment-3066724191   1         1         0       6s\n```  \n* Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.  \n```shell\nkubectl get pods\n```  \nThe output is similar to this:\n```\nNAME                                READY     STATUS             RESTARTS   AGE\nnginx-deployment-1564180365-70iae   1/1       Running            0          25s\nnginx-deployment-1564180365-jbqqo   1/1       Running            0          25s\nnginx-deployment-1564180365-hysrc   1/1       Running            0          25s\nnginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s\n```  \n{{< note >}}\nThe Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (`maxUnavailable` specifically) that you have specified. Kubernetes by default sets the value to 25%.\n{{< /note >}}  \n* Get the description of the Deployment:\n```shell\nkubectl describe deployment\n```  \nThe output is similar to this:\n```\nName:           nginx-deployment\nNamespace:      default\nCreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700\nLabels:         app=nginx\nSelector:       app=nginx\nReplicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable\nStrategyType:       RollingUpdate\nMinReadySeconds:    0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx\nContainers:\nnginx:\nImage:        nginx:1.161\nPort:         80/TCP\nHost Port:    0/TCP\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    ReplicaSetUpdated\nOldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)\nNewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)\nEvents:\nFirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message\n--------- --------    -----   ----                    -------------   --------    ------              -------\n1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3\n22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1\n22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2\n22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2\n21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1\n21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3\n13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0\n13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1\n```  \nTo fix this, you need to rollback to a previous revision of Deployment that is stable.\n", "relevant_passages": ["How can I check the status of a stuck rollout?"]}
{"query": "A user asked the following question:\nQuestion: Can you explain how Linux cgroups relate to resource sharing in containers?\nThis is about the following runbook:\nRunbook Title: Sidecar containers and Linux cgroups {#cgroups}\nRunbook Content: Resource sharing within containersSidecar containers and Linux cgroups {#cgroups}On Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod\nrequest and limit, the same as the scheduler.\n", "relevant_passages": ["Can you explain how Linux cgroups relate to resource sharing in containers?"]}
{"query": "A user asked the following question:\nQuestion: Can I update sidecar containers without affecting the app containers?\nThis is about the following runbook:\nRunbook Title: Differences from application containers\nRunbook Content: Differences from application containersSidecar containers run alongside _app containers_ in the same pod. However, they do not\nexecute the primary application logic; instead, they provide supporting functionality to\nthe main application.  \nSidecar containers have their own independent lifecycles. They can be started, stopped,\nand restarted independently of app containers. This means you can update, scale, or\nmaintain sidecar containers without affecting the primary application.  \nSidecar containers share the same network and storage namespaces with the primary\ncontainer. This co-location allows them to interact closely and share resources.  \nFrom Kubernetes perspective, sidecars graceful termination is less important.\nWhen other containers took all alloted graceful termination time, sidecar containers\nwill receive the `SIGTERM` following with `SIGKILL` faster than may be expected.\nSo exit codes different from `0` (`0` indicates successful exit), for sidecar containers are normal\non Pod termination and should be generally ignored by the external tooling.\n", "relevant_passages": ["Can I update sidecar containers without affecting the app containers?"]}
{"query": "A user asked the following question:\nQuestion: What should I read to understand the replacement for ReplicationController?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Learn about [Deployment](/docs/concepts/workloads/controllers/deployment/), the replacement\nfor ReplicationController.\n* `ReplicationController` is part of the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/replication-controller-v1\" >}}\nobject definition to understand the API for replication controllers.\n", "relevant_passages": ["What should I read to understand the replacement for ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: What happens to a Pod if the node it runs on dies?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: Pod Lifecycle\ncontent_type: concept\nweight: 30\n---  \n<!-- overview -->  \nThis page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting\nin the `Pending` [phase](#pod-phase), moving through `Running` if at least one\nof its primary containers starts OK, and then through either the `Succeeded` or\n`Failed` phases depending on whether any container in the Pod terminated in failure.  \nLike individual application containers, Pods are considered to be relatively\nephemeral (rather than durable) entities. Pods are created, assigned a unique\nID ([UID](/docs/concepts/overview/working-with-objects/names/#uids)), and scheduled\nto run on nodes where they remain until termination (according to restart policy) or\ndeletion.\nIf a {{< glossary_tooltip term_id=\"node\" >}} dies, the Pods running on (or scheduled\nto run on) that node are [marked for deletion](#pod-garbage-collection). The control\nplane marks the Pods for removal after a timeout period.  \n<!-- body -->\n", "relevant_passages": ["What happens to a Pod if the node it runs on dies?"]}
{"query": "A user asked the following question:\nQuestion: What happens when a Pod is assigned to a Node in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Container states\nRunbook Content: Container statesAs well as the [phase](#pod-phase) of the Pod overall, Kubernetes tracks the state of\neach container inside a Pod. You can use\n[container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/) to\ntrigger events to run at certain points in a container's lifecycle.  \nOnce the {{< glossary_tooltip text=\"scheduler\" term_id=\"kube-scheduler\" >}}\nassigns a Pod to a Node, the kubelet starts creating containers for that Pod\nusing a {{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}.\nThere are three possible container states: `Waiting`, `Running`, and `Terminated`.  \nTo check the state of a Pod's containers, you can use\n`kubectl describe pod <name-of-pod>`. The output shows the state for each container\nwithin that Pod.  \nEach state has a specific meaning:\n", "relevant_passages": ["What happens when a Pod is assigned to a Node in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: Where can I find more information about specifying my own pod selector?\nThis is about the following runbook:\nRunbook Title: Pod selector\nRunbook Content: Writing a Job specPod selectorThe `.spec.selector` field is optional. In almost all cases you should not specify it.\nSee section [specifying your own pod selector](#specifying-your-own-pod-selector).\n", "relevant_passages": ["Where can I find more information about specifying my own pod selector?"]}
{"query": "A user asked the following question:\nQuestion: Where can I find more information on managing canary deployments?\nThis is about the following runbook:\nRunbook Title: Canary Deployment\nRunbook Content: Canary DeploymentIf you want to roll out releases to a subset of users or servers using the Deployment, you\ncan create multiple Deployments, one for each release, following the canary pattern described in\n[managing resources](/docs/concepts/workloads/management/#canary-deployments).\n", "relevant_passages": ["Where can I find more information on managing canary deployments?"]}
{"query": "A user asked the following question:\nQuestion: Why should I care about the revision history limit in my Deployment?\nThis is about the following runbook:\nRunbook Title: Revision History Limit\nRunbook Content: Writing a Deployment SpecRevision History LimitA Deployment's revision history is stored in the ReplicaSets it controls.  \n`.spec.revisionHistoryLimit` is an optional field that specifies the number of old ReplicaSets to retain\nto allow rollback. These old ReplicaSets consume resources in `etcd` and crowd the output of `kubectl get rs`. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.  \nMore specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.\nIn this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.\n", "relevant_passages": ["Why should I care about the revision history limit in my Deployment?"]}
{"query": "A user asked the following question:\nQuestion: What do I need to include in the Pod Template for a DaemonSet?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a DaemonSet SpecPod TemplateThe `.spec.template` is one of the required fields in `.spec`.  \nThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates).\nIt has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}},\nexcept it is nested and does not have an `apiVersion` or `kind`.  \nIn addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate\nlabels (see [pod selector](#pod-selector)).  \nA Pod Template in a DaemonSet must have a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)\nequal to `Always`, or be unspecified, which defaults to `Always`.\n", "relevant_passages": ["What do I need to include in the Pod Template for a DaemonSet?"]}
{"query": "A user asked the following question:\nQuestion: How do I delete a ReplicaSet and all its Pods using kubectl?\nThis is about the following runbook:\nRunbook Title: Deleting a ReplicaSet and its Pods\nRunbook Content: Working with ReplicaSetsDeleting a ReplicaSet and its PodsTo delete a ReplicaSet and all of its Pods, use\n[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete). The\n[Garbage collector](/docs/concepts/architecture/garbage-collection/) automatically deletes all of\nthe dependent Pods by default.  \nWhen using the REST API or the `client-go` library, you must set `propagationPolicy` to\n`Background` or `Foreground` in the `-d` option. For example:  \n```shell\nkubectl proxy --port=8080\ncurl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \\\n-d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}' \\\n-H \"Content-Type: application/json\"\n```\n", "relevant_passages": ["How do I delete a ReplicaSet and all its Pods using kubectl?"]}
{"query": "A user asked the following question:\nQuestion: How can I create and work with CronJobs?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods/) and\n[Jobs](/docs/concepts/workloads/controllers/job/), two concepts\nthat CronJobs rely upon.\n* Read about the detailed [format](https://pkg.go.dev/github.com/robfig/cron/v3#hdr-CRON_Expression_Format)\nof CronJob `.spec.schedule` fields.\n* For instructions on creating and working with CronJobs, and for an example\nof a CronJob manifest,\nsee [Running automated tasks with CronJobs](/docs/tasks/job/automated-tasks-with-cron-jobs/).\n* `CronJob` is part of the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/cron-job-v1\" >}}\nAPI reference for more details.\n", "relevant_passages": ["How can I create and work with CronJobs?"]}
{"query": "A user asked the following question:\nQuestion: Will the sidecar container affect the completion of my Job?\nThis is about the following runbook:\nRunbook Title: Jobs with sidecar containers\nRunbook Content: Sidecar containers and Pod lifecycleJobs with sidecar containersIf you define a Job that uses sidecar using Kubernetes-style init containers,\nthe sidecar container in each Pod does not prevent the Job from completing after the\nmain container has finished.  \nHere's an example of a Job with two containers, one of which is a sidecar:  \n{{% code_sample language=\"yaml\" file=\"application/job/job-sidecar.yaml\" %}}\n", "relevant_passages": ["Will the sidecar container affect the completion of my Job?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I try to set a schedule with TZ in Kubernetes 1.29?\nThis is about the following runbook:\nRunbook Title: Unsupported TimeZone specification\nRunbook Content: CronJob limitations {#cron-job-limitations}Unsupported TimeZone specificationSpecifying a timezone using `CRON_TZ` or `TZ` variables inside `.spec.schedule`\nis **not officially supported** (and never has been).  \nStarting with Kubernetes 1.29 if you try to set a schedule that includes `TZ` or `CRON_TZ`\ntimezone specification, Kubernetes will fail to create the resource with a validation\nerror.\nUpdates to CronJobs already using `TZ` or `CRON_TZ` will continue to report a\n[warning](/blog/2020/09/03/warnings/) to the client.\n", "relevant_passages": ["What happens if I try to set a schedule with TZ in Kubernetes 1.29?"]}
{"query": "A user asked the following question:\nQuestion: How can I make sure my pod has the resources it needs to avoid disruptions?\nThis is about the following runbook:\nRunbook Title: Dealing with disruptions\nRunbook Content: Dealing with disruptionsHere are some ways to mitigate involuntary disruptions:  \n- Ensure your pod [requests the resources](/docs/tasks/configure-pod-container/assign-memory-resource) it needs.\n- Replicate your application if you need higher availability.  (Learn about running replicated\n[stateless](/docs/tasks/run-application/run-stateless-application-deployment/)\nand [stateful](/docs/tasks/run-application/run-replicated-stateful-application/) applications.)\n- For even higher availability when running replicated applications,\nspread applications across racks (using\n[anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity))\nor across zones (if using a\n[multi-zone cluster](/docs/setup/multiple-zones).)  \nThe frequency of voluntary disruptions varies.  On a basic Kubernetes cluster, there are\nno automated voluntary disruptions (only user-triggered ones).  However, your cluster administrator or hosting provider\nmay run some additional services which cause voluntary disruptions. For example,\nrolling out node software updates can cause voluntary disruptions. Also, some implementations\nof cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.\nYour cluster administrator or hosting provider should have documented what level of voluntary\ndisruptions, if any, to expect. Certain configuration options, such as\n[using PriorityClasses](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\nin your pod spec can also cause voluntary (and involuntary) disruptions.\n", "relevant_passages": ["How can I make sure my pod has the resources it needs to avoid disruptions?"]}
{"query": "A user asked the following question:\nQuestion: What is the default value for `.spec.minReadySeconds` and what does it mean?\nThis is about the following runbook:\nRunbook Title: Minimum ready seconds\nRunbook Content: ComponentsMinimum ready seconds{{< feature-state for_k8s_version=\"v1.25\" state=\"stable\" >}}  \n`.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly\ncreated Pod should be running and ready without any of its containers crashing, for it to be considered available.\nThis is used to check progression of a rollout when using a [Rolling Update](#rolling-updates) strategy.\nThis field defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when\na Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).\n", "relevant_passages": ["What is the default value for `.spec.minReadySeconds` and what does it mean?"]}
{"query": "A user asked the following question:\nQuestion: Where can I find information on debugging pods?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn how to [debug pods using ephemeral containers](/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container).\n", "relevant_passages": ["Where can I find information on debugging pods?"]}
{"query": "A user asked the following question:\nQuestion: What are static Pods mainly used for?\nThis is about the following runbook:\nRunbook Title: Static Pods\nRunbook Content: Static Pods_Static Pods_ are managed directly by the kubelet daemon on a specific node,\nwithout the {{< glossary_tooltip text=\"API server\" term_id=\"kube-apiserver\" >}}\nobserving them.\nWhereas most Pods are managed by the control plane (for example, a\n{{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}), for static\nPods, the kubelet directly supervises each static Pod (and restarts it if it fails).  \nStatic Pods are always bound to one {{< glossary_tooltip term_id=\"kubelet\" >}} on a specific node.\nThe main use for static Pods is to run a self-hosted control plane: in other words,\nusing the kubelet to supervise the individual [control plane components](/docs/concepts/architecture/#control-plane-components).  \nThe kubelet automatically tries to create a {{< glossary_tooltip text=\"mirror Pod\" term_id=\"mirror-pod\" >}}\non the Kubernetes API server for each static Pod.\nThis means that the Pods running on a node are visible on the API server,\nbut cannot be controlled from there. See the guide [Create static Pods](/docs/tasks/configure-pod-container/static-pod) for more information.  \n{{< note >}}\nThe `spec` of a static Pod cannot refer to other API objects\n(e.g., {{< glossary_tooltip text=\"ServiceAccount\" term_id=\"service-account\" >}},\n{{< glossary_tooltip text=\"ConfigMap\" term_id=\"configmap\" >}},\n{{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}, etc).\n{{< /note >}}\n", "relevant_passages": ["What are static Pods mainly used for?"]}
{"query": "A user asked the following question:\nQuestion: How do I set the concurrency policy for a CronJob?\nThis is about the following runbook:\nRunbook Title: Concurrency policy\nRunbook Content: Writing a CronJob specConcurrency policyThe `.spec.concurrencyPolicy` field is also optional.\nIt specifies how to treat concurrent executions of a Job that is created by this CronJob.\nThe spec may specify only one of the following concurrency policies:  \n* `Allow` (default): The CronJob allows concurrently running Jobs\n* `Forbid`: The CronJob does not allow concurrent runs; if it is time for a new Job run and the\nprevious Job run hasn't finished yet, the CronJob skips the new Job run. Also note that when the\nprevious Job run finishes, `.spec.startingDeadlineSeconds` is still taken into account and may\nresult in a new Job run.\n* `Replace`: If it is time for a new Job run and the previous Job run hasn't finished yet, the\nCronJob replaces the currently running Job run with a new Job run  \nNote that concurrency policy only applies to the Jobs created by the same CronJob.\nIf there are multiple CronJobs, their respective Jobs are always allowed to run concurrently.\n", "relevant_passages": ["How do I set the concurrency policy for a CronJob?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I set the revisionHistoryLimit to 0?\nThis is about the following runbook:\nRunbook Title: Clean up Policy\nRunbook Content: Clean up PolicyYou can set `.spec.revisionHistoryLimit` field in a Deployment to specify how many old ReplicaSets for\nthis Deployment you want to retain. The rest will be garbage-collected in the background. By default,\nit is 10.  \n{{< note >}}\nExplicitly setting this field to 0, will result in cleaning up all the history of your Deployment\nthus that Deployment will not be able to roll back.\n{{< /note >}}\n", "relevant_passages": ["What happens if I set the revisionHistoryLimit to 0?"]}
{"query": "A user asked the following question:\nQuestion: What should I do to ignore Pod failures caused by disruptions in my Job?\nThis is about the following runbook:\nRunbook Title: Pod failure policy {#pod-failure-policy}\nRunbook Content: Handling Pod and container failuresPod failure policy {#pod-failure-policy}{{< feature-state feature_gate_name=\"JobPodFailurePolicy\" >}}  \nA Pod failure policy, defined with the `.spec.podFailurePolicy` field, enables\nyour cluster to handle Pod failures based on the container exit codes and the\nPod conditions.  \nIn some situations, you  may want to have a better control when handling Pod\nfailures than the control provided by the [Pod backoff failure policy](#pod-backoff-failure-policy),\nwhich is based on the Job's `.spec.backoffLimit`. These are some examples of use cases:  \n* To optimize costs of running workloads by avoiding unnecessary Pod restarts,\nyou can terminate a Job as soon as one of its Pods fails with an exit code\nindicating a software bug.\n* To guarantee that your Job finishes even if there are disruptions, you can\nignore Pod failures caused by disruptions (such as {{< glossary_tooltip text=\"preemption\" term_id=\"preemption\" >}},\n{{< glossary_tooltip text=\"API-initiated eviction\" term_id=\"api-eviction\" >}}\nor {{< glossary_tooltip text=\"taint\" term_id=\"taint\" >}}-based eviction) so\nthat they don't count towards the `.spec.backoffLimit` limit of retries.  \nYou can configure a Pod failure policy, in the `.spec.podFailurePolicy` field,\nto meet the above use cases. This policy can handle Pod failures based on the\ncontainer exit codes and the Pod conditions.  \nHere is a manifest for a Job that defines a `podFailurePolicy`:  \n{{% code_sample file=\"/controllers/job-pod-failure-policy-example.yaml\" %}}  \nIn the example above, the first rule of the Pod failure policy specifies that\nthe Job should be marked failed if the `main` container fails with the 42 exit\ncode. The following are the rules for the `main` container specifically:  \n- an exit code of 0 means that the container succeeded\n- an exit code of 42 means that the **entire Job** failed\n- any other exit code represents that the container failed, and hence the entire\nPod. The Pod will be re-created if the total number of restarts is\nbelow `backoffLimit`. If the `backoffLimit` is reached the **entire Job** failed.  \n{{< note >}}\nBecause the Pod template specifies a `restartPolicy: Never`,\nthe kubelet does not restart the `main` container in that particular Pod.\n{{< /note >}}  \nThe second rule of the Pod failure policy, specifying the `Ignore` action for\nfailed Pods with condition `DisruptionTarget` excludes Pod disruptions from\nbeing counted towards the `.spec.backoffLimit` limit of retries.  \n{{< note >}}\nIf the Job failed, either by the Pod failure policy or Pod backoff\nfailure policy, and the Job is running multiple Pods, Kubernetes terminates all\nthe Pods in that Job that are still Pending or Running.\n{{< /note >}}  \nThese are some requirements and semantics of the API:  \n- if you want to use a `.spec.podFailurePolicy` field for a Job, you must\nalso define that Job's pod template with `.spec.restartPolicy` set to `Never`.\n- the Pod failure policy rules you specify under `spec.podFailurePolicy.rules`\nare evaluated in order. Once a rule matches a Pod failure, the remaining rules\nare ignored. When no rule matches the Pod failure, the default\nhandling applies.\n- you may want to restrict a rule to a specific container by specifying its name\nin`spec.podFailurePolicy.rules[*].onExitCodes.containerName`. When not specified the rule\napplies to all containers. When specified, it should match one the container\nor `initContainer` names in the Pod template.\n- you may specify the action taken when a Pod failure policy is matched by\n`spec.podFailurePolicy.rules[*].action`. Possible values are:\n- `FailJob`: use to indicate that the Pod's job should be marked as Failed and\nall running Pods should be terminated.\n- `Ignore`: use to indicate that the counter towards the `.spec.backoffLimit`\nshould not be incremented and a replacement Pod should be created.\n- `Count`: use to indicate that the Pod should be handled in the default way.\nThe counter towards the `.spec.backoffLimit` should be incremented.\n- `FailIndex`: use this action along with [backoff limit per index](#backoff-limit-per-index)\nto avoid unnecessary retries within the index of a failed pod.  \n{{< note >}}\nWhen you use a `podFailurePolicy`, the job controller only matches Pods in the\n`Failed` phase. Pods with a deletion timestamp that are not in a terminal phase\n(`Failed` or `Succeeded`) are considered still terminating. This implies that\nterminating pods retain a [tracking finalizer](#job-tracking-with-finalizers)\nuntil they reach a terminal phase.\nSince Kubernetes 1.27, Kubelet transitions deleted pods to a terminal phase\n(see: [Pod Phase](/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase)). This\nensures that deleted pods have their finalizers removed by the Job controller.\n{{< /note >}}  \n{{< note >}}\nStarting with Kubernetes v1.28, when Pod failure policy is used, the Job controller recreates\nterminating Pods only once these Pods reach the terminal `Failed` phase. This behavior is similar\nto `podReplacementPolicy: Failed`. For more information, see [Pod replacement policy](#pod-replacement-policy).\n{{< /note >}}  \nWhen you use the `podFailurePolicy`, and the Job fails due to the pod\nmatching the rule with the `FailJob` action, then the Job controller triggers\nthe Job termination process by adding the `FailureTarget` condition.\nFor more details, see [Job termination and cleanup](#job-termination-and-cleanup).\n", "relevant_passages": ["What should I do to ignore Pod failures caused by disruptions in my Job?"]}
{"query": "A user asked the following question:\nQuestion: What happens to a Pod once it is scheduled and bound to a node?\nThis is about the following runbook:\nRunbook Title: Pod lifetime\nRunbook Content: Pod lifetimeWhilst a Pod is running, the kubelet is able to restart containers to handle some\nkind of faults. Within a Pod, Kubernetes tracks different container\n[states](#container-states) and determines what action to take to make the Pod\nhealthy again.  \nIn the Kubernetes API, Pods have both a specification and an actual status. The\nstatus for a Pod object consists of a set of [Pod conditions](#pod-conditions).\nYou can also inject [custom readiness information](#pod-readiness-gate) into the\ncondition data for a Pod, if that is useful to your application.  \nPods are only [scheduled](/docs/concepts/scheduling-eviction/) once in their lifetime;\nassigning a Pod to a specific node is called _binding_, and the process of selecting\nwhich node to use is called _scheduling_.\nOnce a Pod has been scheduled and is bound to a node, Kubernetes tries\nto run that Pod on the node. The Pod runs on that node until it stops, or until the Pod\nis [terminated](#pod-termination); if Kubernetes isn't able to start the Pod on the selected\nnode (for example, if the node crashes before the Pod starts), then that particular Pod\nnever starts.  \nYou can use [Pod Scheduling Readiness](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/)\nto delay scheduling for a Pod until all its _scheduling gates_ are removed. For example,\nyou might want to define a set of Pods but only trigger scheduling once all the Pods\nhave been created.\n", "relevant_passages": ["What happens to a Pod once it is scheduled and bound to a node?"]}
{"query": "A user asked the following question:\nQuestion: How do I set the pod management policy for a StatefulSet?\nThis is about the following runbook:\nRunbook Title: Pod Management Policies\nRunbook Content: Deployment and Scaling GuaranteesPod Management PoliciesStatefulSet allows you to relax its ordering guarantees while\npreserving its uniqueness and identity guarantees via its `.spec.podManagementPolicy` field.  \n#### OrderedReady Pod Management  \n`OrderedReady` pod management is the default for StatefulSets. It implements the behavior\ndescribed [above](#deployment-and-scaling-guarantees).  \n#### Parallel Pod Management  \n`Parallel` pod management tells the StatefulSet controller to launch or\nterminate all Pods in parallel, and to not wait for Pods to become Running\nand Ready or completely terminated prior to launching or terminating another\nPod. This option only affects the behavior for scaling operations. Updates are not\naffected.\n", "relevant_passages": ["How do I set the pod management policy for a StatefulSet?"]}
{"query": "A user asked the following question:\nQuestion: How can I access the pod's name in my container?\nThis is about the following runbook:\nRunbook Title: Information available via `fieldRef` {#downwardapi-fieldRef}\nRunbook Content: Available fieldsInformation available via `fieldRef` {#downwardapi-fieldRef}For some Pod-level fields, you can provide them to a container either as\nan environment variable or using a `downwardAPI` volume. The fields available\nvia either mechanism are:  \n`metadata.name`\n: the pod's name  \n`metadata.namespace`\n: the pod's {{< glossary_tooltip text=\"namespace\" term_id=\"namespace\" >}}  \n`metadata.uid`\n: the pod's unique ID  \n`metadata.annotations['<KEY>']`\n: the value of the pod's {{< glossary_tooltip text=\"annotation\" term_id=\"annotation\" >}} named `<KEY>` (for example, `metadata.annotations['myannotation']`)  \n`metadata.labels['<KEY>']`\n: the text value of the pod's {{< glossary_tooltip text=\"label\" term_id=\"label\" >}} named `<KEY>` (for example, `metadata.labels['mylabel']`)  \nThe following information is available through environment variables\n**but not as a downwardAPI volume fieldRef**:  \n`spec.serviceAccountName`\n: the name of the pod's {{< glossary_tooltip text=\"service account\" term_id=\"service-account\" >}}  \n`spec.nodeName`\n: the name of the {{< glossary_tooltip term_id=\"node\" text=\"node\">}} where the Pod is executing  \n`status.hostIP`\n: the primary IP address of the node to which the Pod is assigned  \n`status.hostIPs`\n: the IP addresses is a dual-stack version of `status.hostIP`, the first is always the same as `status.hostIP`.  \n`status.podIP`\n: the pod's primary IP address (usually, its IPv4 address)  \n`status.podIPs`\n: the IP addresses is a dual-stack version of `status.podIP`, the first is always the same as `status.podIP`  \nThe following information is available through a `downwardAPI` volume\n`fieldRef`, **but not as environment variables**:  \n`metadata.labels`\n: all of the pod's labels, formatted as `label-key=\"escaped-label-value\"` with one label per line  \n`metadata.annotations`\n: all of the pod's annotations, formatted as `annotation-key=\"escaped-annotation-value\"` with one annotation per line\n", "relevant_passages": ["How can I access the pod's name in my container?"]}
{"query": "A user asked the following question:\nQuestion: How can I tell if all containers in a Pod are ready?\nThis is about the following runbook:\nRunbook Title: Pod conditions\nRunbook Content: Pod conditionsA Pod has a PodStatus, which has an array of\n[PodConditions](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podcondition-v1-core)\nthrough which the Pod has or has not passed. Kubelet manages the following\nPodConditions:  \n* `PodScheduled`: the Pod has been scheduled to a node.\n* `PodReadyToStartContainers`: (beta feature; enabled by [default](#pod-has-network)) the\nPod sandbox has been successfully created and networking configured.\n* `ContainersReady`: all containers in the Pod are ready.\n* `Initialized`: all [init containers](/docs/concepts/workloads/pods/init-containers/)\nhave completed successfully.\n* `Ready`: the Pod is able to serve requests and should be added to the load\nbalancing pools of all matching Services.  \nField name           | Description\n:--------------------|:-----------\n`type`               | Name of this Pod condition.\n`status`             | Indicates whether that condition is applicable, with possible values \"`True`\", \"`False`\", or \"`Unknown`\".\n`lastProbeTime`      | Timestamp of when the Pod condition was last probed.\n`lastTransitionTime` | Timestamp for when the Pod last transitioned from one status to another.\n`reason`             | Machine-readable, UpperCamelCase text indicating the reason for the condition's last transition.\n`message`            | Human-readable message indicating details about the last status transition.\n", "relevant_passages": ["How can I tell if all containers in a Pod are ready?"]}
{"query": "A user asked the following question:\nQuestion: What do I need to do to apply changes to my StatefulSet Pods if I'm using OnDelete strategy?\nThis is about the following runbook:\nRunbook Title: Update strategies\nRunbook Content: Update strategiesA StatefulSet's `.spec.updateStrategy` field allows you to configure\nand disable automated rolling updates for containers, labels, resource request/limits, and\nannotations for the Pods in a StatefulSet. There are two possible values:  \n`OnDelete`\n: When a StatefulSet's `.spec.updateStrategy.type` is set to `OnDelete`,\nthe StatefulSet controller will not automatically update the Pods in a\nStatefulSet. Users must manually delete Pods to cause the controller to\ncreate new Pods that reflect modifications made to a StatefulSet's `.spec.template`.  \n`RollingUpdate`\n: The `RollingUpdate` update strategy implements automated, rolling updates for the Pods in a\nStatefulSet. This is the default update strategy.\n", "relevant_passages": ["What do I need to do to apply changes to my StatefulSet Pods if I'm using OnDelete strategy?"]}
{"query": "A user asked the following question:\nQuestion: What\u2019s the canary pattern in deployment?\nThis is about the following runbook:\nRunbook Title: Canary Deployment\nRunbook Content: Canary DeploymentIf you want to roll out releases to a subset of users or servers using the Deployment, you\ncan create multiple Deployments, one for each release, following the canary pattern described in\n[managing resources](/docs/concepts/workloads/management/#canary-deployments).\n", "relevant_passages": ["What\u2019s the canary pattern in deployment?"]}
{"query": "A user asked the following question:\nQuestion: How do Pods facilitate communication between containers?\nThis is about the following runbook:\nRunbook Title: Resource sharing and communication\nRunbook Content: Resource sharing and communicationPods enable data sharing and communication among their constituent\ncontainers.\n", "relevant_passages": ["How do Pods facilitate communication between containers?"]}
{"query": "A user asked the following question:\nQuestion: Can I mix sidecar containers with regular init containers?\nThis is about the following runbook:\nRunbook Title: Sidecar containers and Pod lifecycle\nRunbook Content: Sidecar containers and Pod lifecycleIf an init container is created with its `restartPolicy` set to `Always`, it will\nstart and remain running during the entire life of the Pod. This can be helpful for\nrunning supporting services separated from the main application containers.  \nIf a `readinessProbe` is specified for this init container, its result will be used\nto determine the `ready` state of the Pod.  \nSince these containers are defined as init containers, they benefit from the same\nordering and sequential guarantees as regular init containers, allowing you to mix\nsidecar containers with regular init containers for complex Pod initialization flows.  \nCompared to regular init containers, sidecars defined within `initContainers` continue to\nrun after they have started. This is important when there is more than one entry inside\n`.spec.initContainers` for a Pod. After a sidecar-style init container is running (the kubelet\nhas set the `started` status for that init container to true), the kubelet then starts the\nnext init container from the ordered `.spec.initContainers` list.\nThat status either becomes true because there is a process running in the\ncontainer and no startup probe defined, or as a result of its `startupProbe` succeeding.  \nUpon Pod [termination](/docs/concepts/workloads/pods/pod-lifecycle/#termination-with-sidecars),\nthe kubelet postpones terminating sidecar containers until the main application container has fully stopped.\nThe sidecar containers are then shut down in the opposite order of their appearance in the Pod specification.\nThis approach ensures that the sidecars remain operational, supporting other containers within the Pod,\nuntil their service is no longer required.\n", "relevant_passages": ["Can I mix sidecar containers with regular init containers?"]}
{"query": "A user asked the following question:\nQuestion: Can multiple Pods be started for the same index in an Indexed Job?\nThis is about the following runbook:\nRunbook Title: Completion mode\nRunbook Content: Writing a Job specCompletion mode{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}  \nJobs with _fixed completion count_ - that is, jobs that have non null\n`.spec.completions` - can have a completion mode that is specified in `.spec.completionMode`:  \n- `NonIndexed` (default): the Job is considered complete when there have been\n`.spec.completions` successfully completed Pods. In other words, each Pod\ncompletion is homologous to each other. Note that Jobs that have null\n`.spec.completions` are implicitly `NonIndexed`.\n- `Indexed`: the Pods of a Job get an associated completion index from 0 to\n`.spec.completions-1`. The index is available through four mechanisms:\n- The Pod annotation `batch.kubernetes.io/job-completion-index`.\n- The Pod label `batch.kubernetes.io/job-completion-index` (for v1.28 and later). Note\nthe feature gate `PodIndexLabel` must be enabled to use this label, and it is enabled\nby default.\n- As part of the Pod hostname, following the pattern `$(job-name)-$(index)`.\nWhen you use an Indexed Job in combination with a\n{{< glossary_tooltip term_id=\"Service\" >}}, Pods within the Job can use\nthe deterministic hostnames to address each other via DNS. For more information about\nhow to configure this, see [Job with Pod-to-Pod Communication](/docs/tasks/job/job-with-pod-to-pod-communication/).\n- From the containerized task, in the environment variable `JOB_COMPLETION_INDEX`.  \nThe Job is considered complete when there is one successfully completed Pod\nfor each index. For more information about how to use this mode, see\n[Indexed Job for Parallel Processing with Static Work Assignment](/docs/tasks/job/indexed-parallel-processing-static/).  \n{{< note >}}\nAlthough rare, more than one Pod could be started for the same index (due to various reasons such as node failures,\nkubelet restarts, or Pod evictions). In this case, only the first Pod that completes successfully will\ncount towards the completion count and update the status of the Job. The other Pods that are running\nor completed for the same index will be deleted by the Job controller once they are detected.\n{{< /note >}}\n", "relevant_passages": ["Can multiple Pods be started for the same index in an Indexed Job?"]}
{"query": "A user asked the following question:\nQuestion: What happens if a container escapes to the host while using user namespaces?\nThis is about the following runbook:\nRunbook Title: Understanding user namespaces for pods {#pods-and-userns}\nRunbook Content: Understanding user namespaces for pods {#pods-and-userns}Several container runtimes with their default configuration (like Docker Engine,\ncontainerd, CRI-O) use Linux namespaces for isolation. Other technologies exist\nand can be used with those runtimes too (e.g. Kata Containers uses VMs instead of\nLinux namespaces). This page is applicable for container runtimes using Linux\nnamespaces for isolation.  \nWhen creating a pod, by default, several new namespaces are used for isolation:\na network namespace to isolate the network of the container, a PID namespace to\nisolate the view of processes, etc. If a user namespace is used, this will\nisolate the users in the container from the users in the node.  \nThis means containers can run as root and be mapped to a non-root user on the\nhost. Inside the container the process will think it is running as root (and\ntherefore tools like `apt`, `yum`, etc. work fine), while in reality the process\ndoesn't have privileges on the host. You can verify this, for example, if you\ncheck which user the container process is running by executing `ps aux` from\nthe host. The user `ps` shows is not the same as the user you see if you\nexecute inside the container the command `id`.  \nThis abstraction limits what can happen, for example, if the container manages\nto escape to the host. Given that the container is running as a non-privileged\nuser on the host, it is limited what it can do to the host.  \nFurthermore, as users on each pod will be mapped to different non-overlapping\nusers in the host, it is limited what they can do to other pods too.  \nCapabilities granted to a pod are also limited to the pod user namespace and\nmostly invalid out of it, some are even completely void. Here are two examples:\n- `CAP_SYS_MODULE` does not have any effect if granted to a pod using user\nnamespaces, the pod isn't able to load kernel modules.\n- `CAP_SYS_ADMIN` is limited to the pod's user namespace and invalid outside\nof it.  \nWithout using a user namespace a container running as root, in the case of a\ncontainer breakout, has root privileges on the node. And if some capability were\ngranted to the container, the capabilities are valid on the host too. None of\nthis is true when we use user namespaces.  \nIf you want to know more details about what changes when user namespaces are in\nuse, see `man 7 user_namespaces`.\n", "relevant_passages": ["What happens if a container escapes to the host while using user namespaces?"]}
{"query": "A user asked the following question:\nQuestion: When should I consider using a ReplicaSet instead of a Deployment?\nThis is about the following runbook:\nRunbook Title: When to use a ReplicaSet\nRunbook Content: When to use a ReplicaSetA ReplicaSet ensures that a specified number of pod replicas are running at any given\ntime. However, a Deployment is a higher-level concept that manages ReplicaSets and\nprovides declarative updates to Pods along with a lot of other useful features.\nTherefore, we recommend using Deployments instead of directly using ReplicaSets, unless\nyou require custom update orchestration or don't require updates at all.  \nThis actually means that you may never need to manipulate ReplicaSet objects:\nuse a Deployment instead, and define your application in the spec section.\n", "relevant_passages": ["When should I consider using a ReplicaSet instead of a Deployment?"]}
{"query": "A user asked the following question:\nQuestion: How do I set a progress deadline for my Deployment?\nThis is about the following runbook:\nRunbook Title: Failed Deployment\nRunbook Content: Deployment statusFailed DeploymentYour Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur\ndue to some of the following factors:  \n* Insufficient quota\n* Readiness probe failures\n* Image pull errors\n* Insufficient permissions\n* Limit ranges\n* Application runtime misconfiguration  \nOne way you can detect this condition is to specify a deadline parameter in your Deployment spec:\n([`.spec.progressDeadlineSeconds`](#progress-deadline-seconds)). `.spec.progressDeadlineSeconds` denotes the\nnumber of seconds the Deployment controller waits before indicating (in the Deployment status) that the\nDeployment progress has stalled.  \nThe following `kubectl` command sets the spec with `progressDeadlineSeconds` to make the controller report\nlack of progress of a rollout for a Deployment after 10 minutes:  \n```shell\nkubectl patch deployment/nginx-deployment -p '{\"spec\":{\"progressDeadlineSeconds\":600}}'\n```\nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment patched\n```\nOnce the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following\nattributes to the Deployment's `.status.conditions`:  \n* `type: Progressing`\n* `status: \"False\"`\n* `reason: ProgressDeadlineExceeded`  \nThis condition can also fail early and is then set to status value of `\"False\"` due to reasons as `ReplicaSetCreateError`.\nAlso, the deadline is not taken into account anymore once the Deployment rollout completes.  \nSee the [Kubernetes API conventions](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties) for more information on status conditions.  \n{{< note >}}\nKubernetes takes no action on a stalled Deployment other than to report a status condition with\n`reason: ProgressDeadlineExceeded`. Higher level orchestrators can take advantage of it and act accordingly, for\nexample, rollback the Deployment to its previous version.\n{{< /note >}}  \n{{< note >}}\nIf you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline.\nYou can safely pause a Deployment rollout in the middle of a rollout and resume without triggering\nthe condition for exceeding the deadline.\n{{< /note >}}  \nYou may experience transient errors with your Deployments, either due to a low timeout that you have set or\ndue to any other kind of error that can be treated as transient. For example, let's suppose you have\ninsufficient quota. If you describe the Deployment you will notice the following section:  \n```shell\nkubectl describe deployment nginx-deployment\n```\nThe output is similar to this:\n```\n<...>\nConditions:\nType            Status  Reason\n----            ------  ------\nAvailable       True    MinimumReplicasAvailable\nProgressing     True    ReplicaSetUpdated\nReplicaFailure  True    FailedCreate\n<...>\n```  \nIf you run `kubectl get deployment nginx-deployment -o yaml`, the Deployment status is similar to this:  \n```\nstatus:\navailableReplicas: 2\nconditions:\n- lastTransitionTime: 2016-10-04T12:25:39Z\nlastUpdateTime: 2016-10-04T12:25:39Z\nmessage: Replica set \"nginx-deployment-4262182780\" is progressing.\nreason: ReplicaSetUpdated\nstatus: \"True\"\ntype: Progressing\n- lastTransitionTime: 2016-10-04T12:25:42Z\nlastUpdateTime: 2016-10-04T12:25:42Z\nmessage: Deployment has minimum availability.\nreason: MinimumReplicasAvailable\nstatus: \"True\"\ntype: Available\n- lastTransitionTime: 2016-10-04T12:25:39Z\nlastUpdateTime: 2016-10-04T12:25:39Z\nmessage: 'Error creating: pods \"nginx-deployment-4262182780-\" is forbidden: exceeded quota:\nobject-counts, requested: pods=1, used: pods=3, limited: pods=2'\nreason: FailedCreate\nstatus: \"True\"\ntype: ReplicaFailure\nobservedGeneration: 3\nreplicas: 2\nunavailableReplicas: 2\n```  \nEventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the\nreason for the Progressing condition:  \n```\nConditions:\nType            Status  Reason\n----            ------  ------\nAvailable       True    MinimumReplicasAvailable\nProgressing     False   ProgressDeadlineExceeded\nReplicaFailure  True    FailedCreate\n```  \nYou can address an issue of insufficient quota by scaling down your Deployment, by scaling down other\ncontrollers you may be running, or by increasing quota in your namespace. If you satisfy the quota\nconditions and the Deployment controller then completes the Deployment rollout, you'll see the\nDeployment's status update with a successful condition (`status: \"True\"` and `reason: NewReplicaSetAvailable`).  \n```\nConditions:\nType          Status  Reason\n----          ------  ------\nAvailable     True    MinimumReplicasAvailable\nProgressing   True    NewReplicaSetAvailable\n```  \n`type: Available` with `status: \"True\"` means that your Deployment has minimum availability. Minimum availability is dictated\nby the parameters specified in the deployment strategy. `type: Progressing` with `status: \"True\"` means that your Deployment\nis either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum\nrequired new replicas are available (see the Reason of the condition for the particulars - in our case\n`reason: NewReplicaSetAvailable` means that the Deployment is complete).  \nYou can check if a Deployment has failed to progress by using `kubectl rollout status`. `kubectl rollout status`\nreturns a non-zero exit code if the Deployment has exceeded the progression deadline.  \n```shell\nkubectl rollout status deployment/nginx-deployment\n```\nThe output is similar to this:\n```\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nerror: deployment \"nginx\" exceeded its progress deadline\n```\nand the exit status from `kubectl rollout` is 1 (indicating an error):\n```shell\necho $?\n```\n```\n1\n```\n", "relevant_passages": ["How do I set a progress deadline for my Deployment?"]}
{"query": "A user asked the following question:\nQuestion: What's the recommended restart policy for debugging a Job?\nThis is about the following runbook:\nRunbook Title: Pod backoff failure policy\nRunbook Content: Handling Pod and container failuresPod backoff failure policyThere are situations where you want to fail a Job after some amount of retries\ndue to a logical error in configuration etc.\nTo do so, set `.spec.backoffLimit` to specify the number of retries before\nconsidering a Job as failed. The back-off limit is set by default to 6. Failed\nPods associated with the Job are recreated by the Job controller with an\nexponential back-off delay (10s, 20s, 40s ...) capped at six minutes.  \nThe number of retries is calculated in two ways:  \n- The number of Pods with `.status.phase = \"Failed\"`.\n- When using `restartPolicy = \"OnFailure\"`, the number of retries in all the\ncontainers of Pods with `.status.phase` equal to `Pending` or `Running`.  \nIf either of the calculations reaches the `.spec.backoffLimit`, the Job is\nconsidered failed.  \n{{< note >}}\nIf your job has `restartPolicy = \"OnFailure\"`, keep in mind that your Pod running the Job\nwill be terminated once the job backoff limit has been reached. This can make debugging\nthe Job's executable more difficult. We suggest setting\n`restartPolicy = \"Never\"` when debugging the Job or using a logging system to ensure output\nfrom failed Jobs is not lost inadvertently.\n{{< /note >}}\n", "relevant_passages": ["What's the recommended restart policy for debugging a Job?"]}
{"query": "A user asked the following question:\nQuestion: What\u2019s the role of Pods in resource sharing?\nThis is about the following runbook:\nRunbook Title: Resource sharing and communication\nRunbook Content: Resource sharing and communicationPods enable data sharing and communication among their constituent\ncontainers.\n", "relevant_passages": ["What\u2019s the role of Pods in resource sharing?"]}
{"query": "A user asked the following question:\nQuestion: When should I use a startup probe instead of a liveness probe?\nThis is about the following runbook:\nRunbook Title: Types of probe\nRunbook Content: Container probesTypes of probeThe kubelet can optionally perform and react to three kinds of probes on running\ncontainers:  \n`livenessProbe`\n: Indicates whether the container is running. If\nthe liveness probe fails, the kubelet kills the container, and the container\nis subjected to its [restart policy](#restart-policy). If a container does not\nprovide a liveness probe, the default state is `Success`.  \n`readinessProbe`\n: Indicates whether the container is ready to respond to requests.\nIf the readiness probe fails, the endpoints controller removes the Pod's IP\naddress from the endpoints of all Services that match the Pod. The default\nstate of readiness before the initial delay is `Failure`. If a container does\nnot provide a readiness probe, the default state is `Success`.  \n`startupProbe`\n: Indicates whether the application within the container is started.\nAll other probes are disabled if a startup probe is provided, until it succeeds.\nIf the startup probe fails, the kubelet kills the container, and the container\nis subjected to its [restart policy](#restart-policy). If a container does not\nprovide a startup probe, the default state is `Success`.  \nFor more information about how to set up a liveness, readiness, or startup probe,\nsee [Configure Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).  \n#### When should you use a liveness probe?  \nIf the process in your container is able to crash on its own whenever it\nencounters an issue or becomes unhealthy, you do not necessarily need a liveness\nprobe; the kubelet will automatically perform the correct action in accordance\nwith the Pod's `restartPolicy`.  \nIf you'd like your container to be killed and restarted if a probe fails, then\nspecify a liveness probe, and specify a `restartPolicy` of Always or OnFailure.  \n#### When should you use a readiness probe?  \nIf you'd like to start sending traffic to a Pod only when a probe succeeds,\nspecify a readiness probe. In this case, the readiness probe might be the same\nas the liveness probe, but the existence of the readiness probe in the spec means\nthat the Pod will start without receiving any traffic and only start receiving\ntraffic after the probe starts succeeding.  \nIf you want your container to be able to take itself down for maintenance, you\ncan specify a readiness probe that checks an endpoint specific to readiness that\nis different from the liveness probe.  \nIf your app has a strict dependency on back-end services, you can implement both\na liveness and a readiness probe. The liveness probe passes when the app itself\nis healthy, but the readiness probe additionally checks that each required\nback-end service is available. This helps you avoid directing traffic to Pods\nthat can only respond with error messages.  \nIf your container needs to work on loading large data, configuration files, or\nmigrations during startup, you can use a\n[startup probe](#when-should-you-use-a-startup-probe). However, if you want to\ndetect the difference between an app that has failed and an app that is still\nprocessing its startup data, you might prefer a readiness probe.  \n{{< note >}}\nIf you want to be able to drain requests when the Pod is deleted, you do not\nnecessarily need a readiness probe; on deletion, the Pod automatically puts itself\ninto an unready state regardless of whether the readiness probe exists.\nThe Pod remains in the unready state while it waits for the containers in the Pod\nto stop.\n{{< /note >}}  \n#### When should you use a startup probe?  \nStartup probes are useful for Pods that have containers that take a long time to\ncome into service. Rather than set a long liveness interval, you can configure\na separate configuration for probing the container as it starts up, allowing\na time longer than the liveness interval would allow.  \nIf your container usually starts in more than\n`initialDelaySeconds + failureThreshold \u00d7 periodSeconds`, you should specify a\nstartup probe that checks the same endpoint as the liveness probe. The default for\n`periodSeconds` is 10s. You should then set its `failureThreshold` high enough to\nallow the container to start, without changing the default values of the liveness\nprobe. This helps to protect against deadlocks.\n", "relevant_passages": ["When should I use a startup probe instead of a liveness probe?"]}
{"query": "A user asked the following question:\nQuestion: What should I avoid setting for `pod.Spec.TerminationGracePeriodSeconds` in a StatefulSet?\nThis is about the following runbook:\nRunbook Title: Deployment and Scaling Guarantees\nRunbook Content: Deployment and Scaling Guarantees* For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.\n* When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.\n* Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.\n* Before a Pod is terminated, all of its successors must be completely shutdown.  \nThe StatefulSet should not specify a `pod.Spec.TerminationGracePeriodSeconds` of 0. This practice\nis unsafe and strongly discouraged. For further explanation, please refer to\n[force deleting StatefulSet Pods](/docs/tasks/run-application/force-delete-stateful-set-pod/).  \nWhen the nginx example above is created, three Pods will be deployed in the order\nweb-0, web-1, web-2. web-1 will not be deployed before web-0 is\n[Running and Ready](/docs/concepts/workloads/pods/pod-lifecycle/), and web-2 will not be deployed until\nweb-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before\nweb-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and\nbecomes Running and Ready.  \nIf a user were to scale the deployed example by patching the StatefulSet such that\n`replicas=1`, web-2 would be terminated first. web-1 would not be terminated until web-2\nis fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and\nis completely shutdown, but prior to web-1's termination, web-1 would not be terminated\nuntil web-0 is Running and Ready.\n", "relevant_passages": ["What should I avoid setting for `pod.Spec.TerminationGracePeriodSeconds` in a StatefulSet?"]}
{"query": "A user asked the following question:\nQuestion: What are the possible final phases a Pod can enter after Running?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: Pod Lifecycle\ncontent_type: concept\nweight: 30\n---  \n<!-- overview -->  \nThis page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting\nin the `Pending` [phase](#pod-phase), moving through `Running` if at least one\nof its primary containers starts OK, and then through either the `Succeeded` or\n`Failed` phases depending on whether any container in the Pod terminated in failure.  \nLike individual application containers, Pods are considered to be relatively\nephemeral (rather than durable) entities. Pods are created, assigned a unique\nID ([UID](/docs/concepts/overview/working-with-objects/names/#uids)), and scheduled\nto run on nodes where they remain until termination (according to restart policy) or\ndeletion.\nIf a {{< glossary_tooltip term_id=\"node\" >}} dies, the Pods running on (or scheduled\nto run on) that node are [marked for deletion](#pod-garbage-collection). The control\nplane marks the Pods for removal after a timeout period.  \n<!-- body -->\n", "relevant_passages": ["What are the possible final phases a Pod can enter after Running?"]}
{"query": "A user asked the following question:\nQuestion: Is ReplicationController still the recommended way to manage workloads?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- bprashanth\n- janetkuo\ntitle: ReplicationController\napi_metadata:\n- apiVersion: \"v1\"\nkind: \"ReplicationController\"\ncontent_type: concept\nweight: 90\ndescription: >-\nLegacy API for managing workloads that can scale horizontally.\nSuperseded by the Deployment and ReplicaSet APIs.\n---  \n<!-- overview -->  \n{{< note >}}\nA [`Deployment`](/docs/concepts/workloads/controllers/deployment/) that configures a [`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is now the recommended way to set up replication.\n{{< /note >}}  \nA _ReplicationController_ ensures that a specified number of pod replicas are running at any one\ntime. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is\nalways up and available.  \n<!-- body -->\n", "relevant_passages": ["Is ReplicationController still the recommended way to manage workloads?"]}
{"query": "A user asked the following question:\nQuestion: What are some ways to run Jobs in Kubernetes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Read about different ways of running Jobs:\n* [Coarse Parallel Processing Using a Work Queue](/docs/tasks/job/coarse-parallel-processing-work-queue/)\n* [Fine Parallel Processing Using a Work Queue](/docs/tasks/job/fine-parallel-processing-work-queue/)\n* Use an [indexed Job for parallel processing with static work assignment](/docs/tasks/job/indexed-parallel-processing-static/)\n* Create multiple Jobs based on a template: [Parallel Processing using Expansions](/docs/tasks/job/parallel-processing-expansion/)\n* Follow the links within [Clean up finished jobs automatically](#clean-up-finished-jobs-automatically)\nto learn more about how your cluster can clean up completed and / or failed tasks.\n* `Job` is part of the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/job-v1\" >}}\nobject definition to understand the API for jobs.\n* Read about [`CronJob`](/docs/concepts/workloads/controllers/cron-jobs/), which you\ncan use to define a series of Jobs that will run based on a schedule, similar to\nthe UNIX tool `cron`.\n* Practice how to configure handling of retriable and non-retriable pod failures\nusing `podFailurePolicy`, based on the step-by-step [examples](/docs/tasks/job/pod-failure-policy/).\n", "relevant_passages": ["What are some ways to run Jobs in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: Can I modify an ephemeral container after adding it to a Pod?\nThis is about the following runbook:\nRunbook Title: What is an ephemeral container?\nRunbook Content: Understanding ephemeral containersWhat is an ephemeral container?Ephemeral containers differ from other containers in that they lack guarantees\nfor resources or execution, and they will never be automatically restarted, so\nthey are not appropriate for building applications.  Ephemeral containers are\ndescribed using the same `ContainerSpec` as regular containers, but many fields\nare incompatible and disallowed for ephemeral containers.  \n- Ephemeral containers may not have ports, so fields such as `ports`,\n`livenessProbe`, `readinessProbe` are disallowed.\n- Pod resource allocations are immutable, so setting `resources` is disallowed.\n- For a complete list of allowed fields, see the [EphemeralContainer reference\ndocumentation](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#ephemeralcontainer-v1-core).  \nEphemeral containers are created using a special `ephemeralcontainers` handler\nin the API rather than by adding them directly to `pod.spec`, so it's not\npossible to add an ephemeral container using `kubectl edit`.  \nLike regular containers, you may not change or remove an ephemeral container\nafter you have added it to a Pod.  \n{{< note >}}\nEphemeral containers are not supported by [static pods](/docs/tasks/configure-pod-container/static-pod/).\n{{< /note >}}\n", "relevant_passages": ["Can I modify an ephemeral container after adding it to a Pod?"]}
{"query": "A user asked the following question:\nQuestion: Where can I find information about managing application availability during disruptions?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn more about [Pods](/docs/concepts/workloads/pods).\n* [Run a stateless application using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/).\n* Read the {{< api-reference page=\"workload-resources/deployment-v1\" >}} to understand the Deployment API.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how\nyou can use it to manage application availability during disruptions.\n* Use kubectl to [create a Deployment](/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/).\n", "relevant_passages": ["Where can I find information about managing application availability during disruptions?"]}
{"query": "A user asked the following question:\nQuestion: Why do DaemonSet pods need to run before other pods start?\nThis is about the following runbook:\nRunbook Title: DaemonSet\nRunbook Content: Alternatives to ReplicationControllerDaemonSetUse a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicationController for pods that provide a\nmachine-level function, such as machine monitoring or machine logging.  These pods have a lifetime that is tied\nto a machine lifetime: the pod needs to be running on the machine before other pods start, and are\nsafe to terminate when the machine is otherwise ready to be rebooted/shutdown.\n", "relevant_passages": ["Why do DaemonSet pods need to run before other pods start?"]}
{"query": "A user asked the following question:\nQuestion: What happens if a new DaemonSet Pod can't fit on a node?\nThis is about the following runbook:\nRunbook Title: How Daemon Pods are scheduled\nRunbook Content: How Daemon Pods are scheduledA DaemonSet can be used to ensure that all eligible nodes run a copy of a Pod.\nThe DaemonSet controller creates a Pod for each eligible node and adds the\n`spec.affinity.nodeAffinity` field of the Pod to match the target host. After\nthe Pod is created, the default scheduler typically takes over and then binds\nthe Pod to the target host by setting the `.spec.nodeName` field.  If the new\nPod cannot fit on the node, the default scheduler may preempt (evict) some of\nthe existing Pods based on the\n[priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority)\nof the new Pod.  \n{{< note >}}\nIf it's important that the DaemonSet pod run on each node, it's often desirable\nto set the `.spec.template.spec.priorityClassName` of the DaemonSet to a\n[PriorityClass](/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass)\nwith a higher priority to ensure that this eviction occurs.\n{{< /note >}}  \nThe user can specify a different scheduler for the Pods of the DaemonSet, by\nsetting the `.spec.template.spec.schedulerName` field of the DaemonSet.  \nThe original node affinity specified at the\n`.spec.template.spec.affinity.nodeAffinity` field (if specified) is taken into\nconsideration by the DaemonSet controller when evaluating the eligible nodes,\nbut is replaced on the created Pod with the node affinity that matches the name\nof the eligible node.  \n```yaml\nnodeAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\nnodeSelectorTerms:\n- matchFields:\n- key: metadata.name\noperator: In\nvalues:\n- target-host-name\n```\n", "relevant_passages": ["What happens if a new DaemonSet Pod can't fit on a node?"]}
{"query": "A user asked the following question:\nQuestion: How does a ReplicaSet know which Pods to manage?\nThis is about the following runbook:\nRunbook Title: How a ReplicaSet works\nRunbook Content: How a ReplicaSet worksA ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number\nof replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods\nit should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating\nand deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod\ntemplate.  \nA ReplicaSet is linked to its Pods via the Pods' [metadata.ownerReferences](/docs/concepts/architecture/garbage-collection/#owners-dependents)\nfield, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owning\nReplicaSet's identifying information within their ownerReferences field. It's through this link that the ReplicaSet\nknows of the state of the Pods it is maintaining and plans accordingly.  \nA ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no\nOwnerReference or the OwnerReference is not a {{< glossary_tooltip term_id=\"controller\" >}} and it\nmatches a ReplicaSet's selector, it will be immediately acquired by said ReplicaSet.\n", "relevant_passages": ["How does a ReplicaSet know which Pods to manage?"]}
{"query": "A user asked the following question:\nQuestion: How do I classify a Pod as Burstable?\nThis is about the following runbook:\nRunbook Title: Burstable\nRunbook Content: Quality of Service classesBurstablePods that are `Burstable` have some lower-bound resource guarantees based on the request, but\ndo not require a specific limit. If a limit is not specified, it defaults to a\nlimit equivalent to the capacity of the Node, which allows the Pods to flexibly increase\ntheir resources if resources are available. In the event of Pod eviction due to Node\nresource pressure, these Pods are evicted only after all `BestEffort` Pods are evicted.\nBecause a `Burstable` Pod can include a Container that has no resource limits or requests, a Pod\nthat is `Burstable` can try to use any amount of node resources.  \n#### Criteria  \nA Pod is given a QoS class of `Burstable` if:  \n* The Pod does not meet the criteria for QoS class `Guaranteed`.\n* At least one Container in the Pod has a memory or CPU request or limit.\n", "relevant_passages": ["How do I classify a Pod as Burstable?"]}
{"query": "A user asked the following question:\nQuestion: How do I delete a ReplicationController and all its pods using kubectl?\nThis is about the following runbook:\nRunbook Title: Deleting a ReplicationController and its Pods\nRunbook Content: Working with ReplicationControllersDeleting a ReplicationController and its PodsTo delete a ReplicationController and all its pods, use [`kubectl\ndelete`](/docs/reference/generated/kubectl/kubectl-commands#delete).  Kubectl will scale the ReplicationController to zero and wait\nfor it to delete each pod before deleting the ReplicationController itself.  If this kubectl\ncommand is interrupted, it can be restarted.  \nWhen using the REST API or [client library](/docs/reference/using-api/client-libraries), you need to do the steps explicitly (scale replicas to\n0, wait for pod deletions, then delete the ReplicationController).\n", "relevant_passages": ["How do I delete a ReplicationController and all its pods using kubectl?"]}
{"query": "A user asked the following question:\nQuestion: Is there a way to check my container's health via HTTP?\nThis is about the following runbook:\nRunbook Title: Check mechanisms {#probe-check-methods}\nRunbook Content: Container probesCheck mechanisms {#probe-check-methods}There are four different ways to check a container using a probe.\nEach probe must define exactly one of these four mechanisms:  \n`exec`\n: Executes a specified command inside the container. The diagnostic\nis considered successful if the command exits with a status code of 0.  \n`grpc`\n: Performs a remote procedure call using [gRPC](https://grpc.io/).\nThe target should implement\n[gRPC health checks](https://grpc.io/grpc/core/md_doc_health-checking.html).\nThe diagnostic is considered successful if the `status`\nof the response is `SERVING`.  \n`httpGet`\n: Performs an HTTP `GET` request against the Pod's IP\naddress on a specified port and path. The diagnostic is\nconsidered successful if the response has a status code\ngreater than or equal to 200 and less than 400.  \n`tcpSocket`\n: Performs a TCP check against the Pod's IP address on\na specified port. The diagnostic is considered successful if\nthe port is open. If the remote system (the container) closes\nthe connection immediately after it opens, this counts as healthy.  \n{{< caution >}} Unlike the other mechanisms, `exec` probe's implementation involves the creation/forking of multiple processes each time when executed.\nAs a result, in case of the clusters having higher pod densities, lower intervals of `initialDelaySeconds`, `periodSeconds`, configuring any probe with exec mechanism might introduce an overhead on the cpu usage of the node.\nIn such scenarios, consider using the alternative probe mechanisms to avoid the overhead.{{< /caution >}}\n", "relevant_passages": ["Is there a way to check my container's health via HTTP?"]}
{"query": "A user asked the following question:\nQuestion: How can I check the status of my init containers?\nThis is about the following runbook:\nRunbook Title: Understanding init containers\nRunbook Content: Understanding init containersA {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} can have multiple containers\nrunning apps within it, but it can also have one or more init containers, which are run\nbefore the app containers are started.  \nInit containers are exactly like regular containers, except:  \n* Init containers always run to completion.\n* Each init container must complete successfully before the next one starts.  \nIf a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.\nHowever, if the Pod has a `restartPolicy` of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.  \nTo specify an init container for a Pod, add the `initContainers` field into\nthe [Pod specification](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec),\nas an array of `container` items (similar to the app `containers` field and its contents).\nSee [Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container) in the\nAPI reference for more details.  \nThe status of the init containers is returned in `.status.initContainerStatuses`\nfield as an array of the container statuses (similar to the `.status.containerStatuses`\nfield).\n", "relevant_passages": ["How can I check the status of my init containers?"]}
{"query": "A user asked the following question:\nQuestion: How can I set up my DaemonSet Pods to send updates to another service?\nThis is about the following runbook:\nRunbook Title: Communicating with Daemon Pods\nRunbook Content: Communicating with Daemon PodsSome possible patterns for communicating with Pods in a DaemonSet are:  \n- **Push**: Pods in the DaemonSet are configured to send updates to another service, such\nas a stats database.  They do not have clients.\n- **NodeIP and Known Port**: Pods in the DaemonSet can use a `hostPort`, so that the pods\nare reachable via the node IPs.\nClients know the list of node IPs somehow, and know the port by convention.\n- **DNS**: Create a [headless service](/docs/concepts/services-networking/service/#headless-services)\nwith the same pod selector, and then discover DaemonSets using the `endpoints`\nresource or retrieve multiple A records from DNS.\n- **Service**: Create a service with the same Pod selector, and use the service to reach a\ndaemon on a random node. (No way to reach specific node.)\n", "relevant_passages": ["How can I set up my DaemonSet Pods to send updates to another service?"]}
{"query": "A user asked the following question:\nQuestion: Do sidecar containers support probes like init containers do?\nThis is about the following runbook:\nRunbook Title: Differences from init containers\nRunbook Content: Differences from init containersSidecar containers work alongside the main container, extending its functionality and\nproviding additional services.  \nSidecar containers run concurrently with the main application container. They are active\nthroughout the lifecycle of the pod and can be started and stopped independently of the\nmain container. Unlike [init containers](/docs/concepts/workloads/pods/init-containers/),\nsidecar containers support [probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe) to control their lifecycle.  \nSidecar containers can interact directly with the main application containers, because\nlike init containers they always share the same network, and can optionally also share\nvolumes (filesystems).  \nInit containers stop before the main containers start up, so init containers cannot\nexchange messages with the app container in a Pod. Any data passing is one-way\n(for example, an init container can put information inside an `emptyDir` volume).\n", "relevant_passages": ["Do sidecar containers support probes like init containers do?"]}
{"query": "A user asked the following question:\nQuestion: What happens if there are too many pods running under a ReplicationController?\nThis is about the following runbook:\nRunbook Title: How a ReplicationController works\nRunbook Content: How a ReplicationController worksIf there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the\nReplicationController starts more pods. Unlike manually created pods, the pods maintained by a\nReplicationController are automatically replaced if they fail, are deleted, or are terminated.\nFor example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.\nFor this reason, you should use a ReplicationController even if your application requires\nonly a single pod. A ReplicationController is similar to a process supervisor,\nbut instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods\nacross multiple nodes.  \nReplicationController is often abbreviated to \"rc\" in discussion, and as a shortcut in\nkubectl commands.  \nA simple case is to create one ReplicationController object to reliably run one instance of\na Pod indefinitely.  A more complex use case is to run several identical replicas of a replicated\nservice, such as web servers.\n", "relevant_passages": ["What happens if there are too many pods running under a ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: What happens to the hostname seen by containers within a Pod?\nThis is about the following runbook:\nRunbook Title: Pod networking\nRunbook Content: Resource sharing and communicationPod networkingEach Pod is assigned a unique IP address for each address family. Every\ncontainer in a Pod shares the network namespace, including the IP address and\nnetwork ports. Inside a Pod (and **only** then), the containers that belong to the Pod\ncan communicate with one another using `localhost`. When containers in a Pod communicate\nwith entities *outside the Pod*,\nthey must coordinate how they use the shared network resources (such as ports).\nWithin a Pod, containers share an IP address and port space, and\ncan find each other via `localhost`. The containers in a Pod can also communicate\nwith each other using standard inter-process communications like SystemV semaphores\nor POSIX shared memory.  Containers in different Pods have distinct IP addresses\nand can not communicate by OS-level IPC without special configuration.\nContainers that want to interact with a container running in a different Pod can\nuse IP networking to communicate.  \nContainers within the Pod see the system hostname as being the same as the configured\n`name` for the Pod. There's more about this in the [networking](/docs/concepts/cluster-administration/networking/)\nsection.\n", "relevant_passages": ["What happens to the hostname seen by containers within a Pod?"]}
{"query": "A user asked the following question:\nQuestion: How can I automatically clean up finished Jobs in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Cleanup for finished Jobs\nRunbook Content: Cleanup for finished JobsThe TTL-after-finished controller is only supported for Jobs. You can use this mechanism to clean\nup finished Jobs (either `Complete` or `Failed`) automatically by specifying the\n`.spec.ttlSecondsAfterFinished` field of a Job, as in this\n[example](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically).  \nThe TTL-after-finished controller assumes that a Job is eligible to be cleaned up\nTTL seconds after the Job has finished. The timer starts once the\nstatus condition of the Job changes to show that the Job is either `Complete` or `Failed`; once the TTL has\nexpired, that Job becomes eligible for\n[cascading](/docs/concepts/architecture/garbage-collection/#cascading-deletion) removal. When the\nTTL-after-finished controller cleans up a job, it will delete it cascadingly, that is to say it will delete\nits dependent objects together with it.  \nKubernetes honors object lifecycle guarantees on the Job, such as waiting for\n[finalizers](/docs/concepts/overview/working-with-objects/finalizers/).  \nYou can set the TTL seconds at any time. Here are some examples for setting the\n`.spec.ttlSecondsAfterFinished` field of a Job:  \n* Specify this field in the Job manifest, so that a Job can be cleaned up\nautomatically some time after it finishes.\n* Manually set this field of existing, already finished Jobs, so that they become eligible\nfor cleanup.\n* Use a\n[mutating admission webhook](/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\nto set this field dynamically at Job creation time. Cluster administrators can\nuse this to enforce a TTL policy for finished jobs.\n* Use a\n[mutating admission webhook](/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\nto set this field dynamically after the Job has finished, and choose\ndifferent TTL values based on job status, labels. For this case, the webhook needs\nto detect changes to the `.status` of the Job and only set a TTL when the Job\nis being marked as completed.\n* Write your own controller to manage the cleanup TTL for Jobs that match a particular\n{{< glossary_tooltip term_id=\"selector\" text=\"selector\" >}}.\n", "relevant_passages": ["How can I automatically clean up finished Jobs in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: Can a Pod be rescheduled to a different node after it fails?\nThis is about the following runbook:\nRunbook Title: Pods and fault recovery {#pod-fault-recovery}\nRunbook Content: Pod lifetimePods and fault recovery {#pod-fault-recovery}If one of the containers in the Pod fails, then Kubernetes may try to restart that\nspecific container.\nRead [How Pods handle problems with containers](#container-restarts) to learn more.  \nPods can however fail in a way that the cluster cannot recover from, and in that case\nKubernetes does not attempt to heal the Pod further; instead, Kubernetes deletes the\nPod and relies on other components to provide automatic healing.  \nIf a Pod is scheduled to a {{< glossary_tooltip text=\"node\" term_id=\"node\" >}} and that\nnode then fails, the Pod is treated as unhealthy and Kubernetes eventually deletes the Pod.\nA Pod won't survive an {{< glossary_tooltip text=\"eviction\" term_id=\"eviction\" >}} due to\na lack of resources or Node maintenance.  \nKubernetes uses a higher-level abstraction, called a\n{{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}}, that handles the work of\nmanaging the relatively disposable Pod instances.  \nA given Pod (as defined by a UID) is never \"rescheduled\" to a different node; instead,\nthat Pod can be replaced by a new, near-identical Pod. If you make a replacement Pod, it can\neven have same name (as in `.metadata.name`) that the old Pod had, but the replacement\nwould have a different `.metadata.uid` from the old Pod.  \nKubernetes does not guarantee that a replacement for an existing Pod would be scheduled to\nthe same node as the old Pod that was being replaced.\n", "relevant_passages": ["Can a Pod be rescheduled to a different node after it fails?"]}
{"query": "A user asked the following question:\nQuestion: What do I need to set when using the REST API to delete a ReplicaSet?\nThis is about the following runbook:\nRunbook Title: Deleting a ReplicaSet and its Pods\nRunbook Content: Working with ReplicaSetsDeleting a ReplicaSet and its PodsTo delete a ReplicaSet and all of its Pods, use\n[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete). The\n[Garbage collector](/docs/concepts/architecture/garbage-collection/) automatically deletes all of\nthe dependent Pods by default.  \nWhen using the REST API or the `client-go` library, you must set `propagationPolicy` to\n`Background` or `Foreground` in the `-d` option. For example:  \n```shell\nkubectl proxy --port=8080\ncurl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \\\n-d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}' \\\n-H \"Content-Type: application/json\"\n```\n", "relevant_passages": ["What do I need to set when using the REST API to delete a ReplicaSet?"]}
{"query": "A user asked the following question:\nQuestion: Can I set a time limit for a job in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Job termination and cleanup\nRunbook Content: Job termination and cleanupWhen a Job completes, no more Pods are created, but the Pods are [usually](#pod-backoff-failure-policy) not deleted either.\nKeeping them around allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output.\nThe job object also remains after it is completed so that you can view its status. It is up to the user to delete\nold jobs after noting their status. Delete the job with `kubectl` (e.g. `kubectl delete jobs/pi` or `kubectl delete -f ./job.yaml`).\nWhen you delete the job using `kubectl`, all the pods it created are deleted too.  \nBy default, a Job will run uninterrupted unless a Pod fails (`restartPolicy=Never`)\nor a Container exits in error (`restartPolicy=OnFailure`), at which point the Job defers to the\n`.spec.backoffLimit` described above. Once `.spec.backoffLimit` has been reached the Job will\nbe marked as failed and any running Pods will be terminated.  \nAnother way to terminate a Job is by setting an active deadline.\nDo this by setting the `.spec.activeDeadlineSeconds` field of the Job to a number of seconds.\nThe `activeDeadlineSeconds` applies to the duration of the job, no matter how many Pods are created.\nOnce a Job reaches `activeDeadlineSeconds`, all of its running Pods are terminated and the Job status\nwill become `type: Failed` with `reason: DeadlineExceeded`.  \nNote that a Job's `.spec.activeDeadlineSeconds` takes precedence over its `.spec.backoffLimit`.\nTherefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once\nit reaches the time limit specified by `activeDeadlineSeconds`, even if the `backoffLimit` is not yet reached.  \nExample:  \n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: pi-with-timeout\nspec:\nbackoffLimit: 5\nactiveDeadlineSeconds: 100\ntemplate:\nspec:\ncontainers:\n- name: pi\nimage: perl:5.34.0\ncommand: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\nrestartPolicy: Never\n```  \nNote that both the Job spec and the [Pod template spec](/docs/concepts/workloads/pods/init-containers/#detailed-behavior)\nwithin the Job have an `activeDeadlineSeconds` field. Ensure that you set this field at the proper level.  \nKeep in mind that the `restartPolicy` applies to the Pod, and not to the Job itself:\nthere is no automatic Job restart once the Job status is `type: Failed`.\nThat is, the Job termination mechanisms activated with `.spec.activeDeadlineSeconds`\nand `.spec.backoffLimit` result in a permanent Job failure that requires manual intervention to resolve.\n", "relevant_passages": ["Can I set a time limit for a job in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What does Kubernetes consider a Deployment to be complete?\nThis is about the following runbook:\nRunbook Title: Complete Deployment\nRunbook Content: Deployment statusComplete DeploymentKubernetes marks a Deployment as _complete_ when it has the following characteristics:  \n* All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any\nupdates you've requested have been completed.\n* All of the replicas associated with the Deployment are available.\n* No old replicas for the Deployment are running.  \nWhen the rollout becomes \u201ccomplete\u201d, the Deployment controller sets a condition with the following\nattributes to the Deployment's `.status.conditions`:  \n* `type: Progressing`\n* `status: \"True\"`\n* `reason: NewReplicaSetAvailable`  \nThis `Progressing` condition will retain a status value of `\"True\"` until a new rollout\nis initiated. The condition holds even when availability of replicas changes (which\ndoes instead affect the `Available` condition).  \nYou can check if a Deployment has completed by using `kubectl rollout status`. If the rollout completed\nsuccessfully, `kubectl rollout status` returns a zero exit code.  \n```shell\nkubectl rollout status deployment/nginx-deployment\n```\nThe output is similar to this:\n```\nWaiting for rollout to finish: 2 of 3 updated replicas are available...\ndeployment \"nginx-deployment\" successfully rolled out\n```\nand the exit status from `kubectl rollout` is 0 (success):\n```shell\necho $?\n```\n```\n0\n```\n", "relevant_passages": ["What does Kubernetes consider a Deployment to be complete?"]}
{"query": "A user asked the following question:\nQuestion: When do modifications to a CronJob take effect?\nThis is about the following runbook:\nRunbook Title: Modifying a CronJob\nRunbook Content: CronJob limitations {#cron-job-limitations}Modifying a CronJobBy design, a CronJob contains a template for _new_ Jobs.\nIf you modify an existing CronJob, the changes you make will apply to new Jobs that\nstart to run after your modification is complete. Jobs (and their Pods) that have already\nstarted continue to run without changes.\nThat is, the CronJob does _not_ update existing Jobs, even if those remain running.\n", "relevant_passages": ["When do modifications to a CronJob take effect?"]}
{"query": "A user asked the following question:\nQuestion: Is there a specific section that must be included in a Deployment spec?\nThis is about the following runbook:\nRunbook Title: Writing a Deployment Spec\nRunbook Content: Writing a Deployment SpecAs with all other Kubernetes configs, a Deployment needs `.apiVersion`, `.kind`, and `.metadata` fields.\nFor general information about working with config files, see\n[deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/),\nconfiguring containers, and [using kubectl to manage resources](/docs/concepts/overview/working-with-objects/object-management/) documents.  \nWhen the control plane creates new Pods for a Deployment, the `.metadata.name` of the\nDeployment is part of the basis for naming those Pods. The name of a Deployment must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).  \nA Deployment also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\n", "relevant_passages": ["Is there a specific section that must be included in a Deployment spec?"]}
{"query": "A user asked the following question:\nQuestion: Is there a way to temporarily take a pod out of service without affecting the ReplicationController?\nThis is about the following runbook:\nRunbook Title: Isolating pods from a ReplicationController\nRunbook Content: Working with ReplicationControllersIsolating pods from a ReplicationControllerPods may be removed from a ReplicationController's target set by changing their labels. This technique may be used to remove pods from service for debugging and data recovery. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).\n", "relevant_passages": ["Is there a way to temporarily take a pod out of service without affecting the ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I don't specify labels in the ReplicationController?\nThis is about the following runbook:\nRunbook Title: Labels on the ReplicationController\nRunbook Content: Writing a ReplicationController ManifestLabels on the ReplicationControllerThe ReplicationController can itself have labels (`.metadata.labels`).  Typically, you\nwould set these the same as the `.spec.template.metadata.labels`; if `.metadata.labels` is not specified\nthen it defaults to  `.spec.template.metadata.labels`. However, they are allowed to be\ndifferent, and the `.metadata.labels` do not affect the behavior of the ReplicationController.\n", "relevant_passages": ["What happens if I don't specify labels in the ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: What factors contribute to a Pod's effective resource request/limit?\nThis is about the following runbook:\nRunbook Title: Resource sharing within containers\nRunbook Content: Resource sharing within containers{{< comment >}}\nThis section is also present in the [init containers](/docs/concepts/workloads/pods/init-containers/) page.\nIf you're editing this section, change both places.\n{{< /comment >}}  \nGiven the order of execution for init, sidecar and app containers, the following rules\nfor resource usage apply:  \n* The highest of any particular resource request or limit defined on all init\ncontainers is the *effective init request/limit*. If any resource has no\nresource limit specified this is considered as the highest limit.\n* The Pod's *effective request/limit* for a resource is the sum of\n[pod overhead](/docs/concepts/scheduling-eviction/pod-overhead/) and the higher of:\n* the sum of all non-init containers(app and sidecar containers) request/limit for a\nresource\n* the effective init request/limit for a resource\n* Scheduling is done based on effective requests/limits, which means\ninit containers can reserve resources for initialization that are not used\nduring the life of the Pod.\n* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the\nQoS tier for all init, sidecar and app containers alike.  \nQuota and limits are applied based on the effective Pod request and\nlimit.\n", "relevant_passages": ["What factors contribute to a Pod's effective resource request/limit?"]}
{"query": "A user asked the following question:\nQuestion: What should I be cautious about when setting a non-zero TTL for Kubernetes jobs?\nThis is about the following runbook:\nRunbook Title: Time skew\nRunbook Content: CaveatsTime skewBecause the TTL-after-finished controller uses timestamps stored in the Kubernetes jobs to\ndetermine whether the TTL has expired or not, this feature is sensitive to time\nskew in your cluster, which may cause the control plane to clean up Job objects\nat the wrong time.  \nClocks aren't always correct, but the difference should be\nvery small. Please be aware of this risk when setting a non-zero TTL.\n", "relevant_passages": ["What should I be cautious about when setting a non-zero TTL for Kubernetes jobs?"]}
{"query": "A user asked the following question:\nQuestion: How does a Job fail in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Terminal Job conditions\nRunbook Content: Job termination and cleanupTerminal Job conditionsA Job has two possible terminal states, each of which has a corresponding Job\ncondition:\n* Succeeded:  Job condition `Complete`\n* Failed: Job condition `Failed`  \nJobs fail for the following reasons:\n- The number of Pod failures exceeded the specified `.spec.backoffLimit` in the Job\nspecification. For details, see [Pod backoff failure policy](#pod-backoff-failure-policy).\n- The Job runtime exceeded the specified `.spec.activeDeadlineSeconds`\n- An indexed Job that used `.spec.backoffLimitPerIndex` has failed indexes.\nFor details, see [Backoff limit per index](#backoff-limit-per-index).\n- The number of failed indexes in the Job exceeded the specified\n`spec.maxFailedIndexes`. For details, see [Backoff limit per index](#backoff-limit-per-index)\n- A failed Pod matches a rule in `.spec.podFailurePolicy` that has the `FailJob`\naction. For details about how Pod failure policy rules might affect failure\nevaluation, see [Pod failure policy](#pod-failure-policy).  \nJobs succeed for the following reasons:\n- The number of succeeded Pods reached the specified `.spec.completions`\n- The criteria specified in `.spec.successPolicy` are met. For details, see\n[Success policy](#success-policy).  \nIn Kubernetes v1.31 and later the Job controller delays the addition of the\nterminal conditions,`Failed` or `Complete`, until all of the Job Pods are terminated.  \nIn Kubernetes v1.30 and earlier, the Job controller added the `Complete` or the\n`Failed` Job terminal conditions as soon as the Job termination process was\ntriggered and all Pod finalizers were removed. However, some Pods would still\nbe running or terminating at the moment that the terminal condition was added.  \nIn Kubernetes v1.31 and later, the controller only adds the Job terminal conditions\n_after_ all of the Pods are terminated. You can enable this behavior by using the\n`JobManagedBy` or the `JobPodReplacementPolicy` (enabled by default)\n[feature gates](/docs/reference/command-line-tools-reference/feature-gates/).\n", "relevant_passages": ["How does a Job fail in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: Is there a way to automatically clean up finished jobs in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Jobs history limits\nRunbook Content: Writing a CronJob specJobs history limitsThe `.spec.successfulJobsHistoryLimit` and `.spec.failedJobsHistoryLimit` fields specify\nhow many completed and failed Jobs should be kept. Both fields are optional.  \n* `.spec.successfulJobsHistoryLimit`: This field specifies the number of successful finished\njobs to keep. The default value is `3`. Setting this field to `0` will not keep any successful jobs.  \n* `.spec.failedJobsHistoryLimit`: This field specifies the number of failed finished jobs to keep.\nThe default value is `1`. Setting this field to `0` will not keep any failed jobs.  \nFor another way to clean up Jobs automatically, see\n[Clean up finished Jobs automatically](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically).\n", "relevant_passages": ["Is there a way to automatically clean up finished jobs in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What are the different types of probes I should know about for my pods?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn how to [Adopt Sidecar Containers](/docs/tutorials/configuration/pod-sidecar-containers/)\n* Read a blog post on [native sidecar containers](/blog/2023/08/25/native-sidecar-containers/).\n* Read about [creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).\n* Learn about the [types of probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe): liveness, readiness, startup probe.\n* Learn about [pod overhead](/docs/concepts/scheduling-eviction/pod-overhead/).\n", "relevant_passages": ["What are the different types of probes I should know about for my pods?"]}
{"query": "A user asked the following question:\nQuestion: What happens to ReplicationControllers when I update my service?\nThis is about the following runbook:\nRunbook Title: Using ReplicationControllers with Services\nRunbook Content: Common usage patternsUsing ReplicationControllers with ServicesMultiple ReplicationControllers can sit behind a single service, so that, for example, some traffic\ngoes to the old version, and some goes to the new version.  \nA ReplicationController will never terminate on its own, but it isn't expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.\n", "relevant_passages": ["What happens to ReplicationControllers when I update my service?"]}
{"query": "A user asked the following question:\nQuestion: How can I protect my application using a Pod Disruption Budget?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Follow steps to protect your application by [configuring a Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).  \n* Learn more about [draining nodes](/docs/tasks/administer-cluster/safely-drain-node/)  \n* Learn about [updating a deployment](/docs/concepts/workloads/controllers/deployment/#updating-a-deployment)\nincluding steps to maintain its availability during the rollout.\n", "relevant_passages": ["How can I protect my application using a Pod Disruption Budget?"]}
{"query": "A user asked the following question:\nQuestion: How can I write applications that tolerate disruptions in my cluster?\nThis is about the following runbook:\nRunbook Title: How to perform Disruptive Actions on your Cluster\nRunbook Content: How to perform Disruptive Actions on your ClusterIf you are a Cluster Administrator, and you need to perform a disruptive action on all\nthe nodes in your cluster, such as a node or system software upgrade, here are some options:  \n- Accept downtime during the upgrade.\n- Failover to another complete replica cluster.\n-  No downtime, but may be costly both for the duplicated nodes\nand for human effort to orchestrate the switchover.\n- Write disruption tolerant applications and use PDBs.\n- No downtime.\n- Minimal resource duplication.\n- Allows more automation of cluster administration.\n- Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary\ndisruptions largely overlaps with work to support autoscaling and tolerating\ninvoluntary disruptions.\n", "relevant_passages": ["How can I write applications that tolerate disruptions in my cluster?"]}
{"query": "A user asked the following question:\nQuestion: How do containers in a Pod communicate with each other?\nThis is about the following runbook:\nRunbook Title: Pod networking\nRunbook Content: Resource sharing and communicationPod networkingEach Pod is assigned a unique IP address for each address family. Every\ncontainer in a Pod shares the network namespace, including the IP address and\nnetwork ports. Inside a Pod (and **only** then), the containers that belong to the Pod\ncan communicate with one another using `localhost`. When containers in a Pod communicate\nwith entities *outside the Pod*,\nthey must coordinate how they use the shared network resources (such as ports).\nWithin a Pod, containers share an IP address and port space, and\ncan find each other via `localhost`. The containers in a Pod can also communicate\nwith each other using standard inter-process communications like SystemV semaphores\nor POSIX shared memory.  Containers in different Pods have distinct IP addresses\nand can not communicate by OS-level IPC without special configuration.\nContainers that want to interact with a container running in a different Pod can\nuse IP networking to communicate.  \nContainers within the Pod see the system hostname as being the same as the configured\n`name` for the Pod. There's more about this in the [networking](/docs/concepts/cluster-administration/networking/)\nsection.\n", "relevant_passages": ["How do containers in a Pod communicate with each other?"]}
{"query": "A user asked the following question:\nQuestion: What happens if the startingDeadlineSeconds is set to a value less than 10 seconds?\nThis is about the following runbook:\nRunbook Title: Job creation\nRunbook Content: CronJob limitations {#cron-job-limitations}Job creationA CronJob creates a Job object approximately once per execution time of its schedule.\nThe scheduling is approximate because there\nare certain circumstances where two Jobs might be created, or no Job might be created.\nKubernetes tries to avoid those situations, but does not completely prevent them. Therefore,\nthe Jobs that you define should be _idempotent_.  \nIf `startingDeadlineSeconds` is set to a large value or left unset (the default)\nand if `concurrencyPolicy` is set to `Allow`, the Jobs will always run\nat least once.  \n{{< caution >}}\nIf `startingDeadlineSeconds` is set to a value less than 10 seconds, the CronJob may not be scheduled. This is because the CronJob controller checks things every 10 seconds.\n{{< /caution >}}  \nFor every CronJob, the CronJob {{< glossary_tooltip term_id=\"controller\" >}} checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the Job and logs the error.  \n```\nCannot determine if job needs to be started. Too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.\n```  \nIt is important to note that if the `startingDeadlineSeconds` field is set (not `nil`), the controller counts how many missed Jobs occurred from the value of `startingDeadlineSeconds` until now rather than from the last scheduled time until now. For example, if `startingDeadlineSeconds` is `200`, the controller counts how many missed Jobs occurred in the last 200 seconds.  \nA CronJob is counted as missed if it has failed to be created at its scheduled time. For example, if `concurrencyPolicy` is set to `Forbid` and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed.  \nFor example, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its\n`startingDeadlineSeconds` field is not set. If the CronJob controller happens to\nbe down from `08:29:00` to `10:21:00`, the Job will not start as the number of missed Jobs which missed their schedule is greater than 100.  \nTo illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its\n`startingDeadlineSeconds` is set to 200 seconds. If the CronJob controller happens to\nbe down for the same period as the previous example (`08:29:00` to `10:21:00`,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (i.e., 3 missed schedules), rather than from the last scheduled time until now.  \nThe CronJob is only responsible for creating Jobs that match its schedule, and\nthe Job in turn is responsible for the management of the Pods it represents.\n", "relevant_passages": ["What happens if the startingDeadlineSeconds is set to a value less than 10 seconds?"]}
{"query": "A user asked the following question:\nQuestion: What does the 'Ready' condition signify for a Pod?\nThis is about the following runbook:\nRunbook Title: Pod conditions\nRunbook Content: Pod conditionsA Pod has a PodStatus, which has an array of\n[PodConditions](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podcondition-v1-core)\nthrough which the Pod has or has not passed. Kubelet manages the following\nPodConditions:  \n* `PodScheduled`: the Pod has been scheduled to a node.\n* `PodReadyToStartContainers`: (beta feature; enabled by [default](#pod-has-network)) the\nPod sandbox has been successfully created and networking configured.\n* `ContainersReady`: all containers in the Pod are ready.\n* `Initialized`: all [init containers](/docs/concepts/workloads/pods/init-containers/)\nhave completed successfully.\n* `Ready`: the Pod is able to serve requests and should be added to the load\nbalancing pools of all matching Services.  \nField name           | Description\n:--------------------|:-----------\n`type`               | Name of this Pod condition.\n`status`             | Indicates whether that condition is applicable, with possible values \"`True`\", \"`False`\", or \"`Unknown`\".\n`lastProbeTime`      | Timestamp of when the Pod condition was last probed.\n`lastTransitionTime` | Timestamp for when the Pod last transitioned from one status to another.\n`reason`             | Machine-readable, UpperCamelCase text indicating the reason for the condition's last transition.\n`message`            | Human-readable message indicating details about the last status transition.\n", "relevant_passages": ["What does the 'Ready' condition signify for a Pod?"]}
{"query": "A user asked the following question:\nQuestion: What should I do to drop specific Linux capabilities in my Pod?\nThis is about the following runbook:\nRunbook Title: Pod security settings {#pod-security}\nRunbook Content: Pod security settings {#pod-security}To set security constraints on Pods and containers, you use the\n`securityContext` field in the Pod specification. This field gives you\ngranular control over what a Pod or individual containers can do. For example:  \n* Drop specific Linux capabilities to avoid the impact of a CVE.\n* Force all processes in the Pod to run as a non-root user or as a specific\nuser or group ID.\n* Set a specific seccomp profile.\n* Set Windows security options, such as whether containers run as HostProcess.  \n{{< caution >}}\nYou can also use the Pod securityContext to enable\n[_privileged mode_](/docs/concepts/security/linux-kernel-security-constraints/#privileged-containers)\nin Linux containers. Privileged mode overrides many of the other security\nsettings in the securityContext. Avoid using this setting unless you can't grant\nthe equivalent permissions by using other fields in the securityContext.\nIn Kubernetes 1.26 and later, you can run Windows containers in a similarly\nprivileged mode by setting the `windowsOptions.hostProcess` flag on the\nsecurity context of the Pod spec. For details and instructions, see\n[Create a Windows HostProcess Pod](/docs/tasks/configure-pod-container/create-hostprocess-pod/).\n{{< /caution >}}  \n* To learn about kernel-level security constraints that you can use,\nsee [Linux kernel security constraints for Pods and containers](/docs/concepts/security/linux-kernel-security-constraints).\n* To learn more about the Pod security context, see\n[Configure a Security Context for a Pod or Container](/docs/tasks/configure-pod-container/security-context/).\n", "relevant_passages": ["What should I do to drop specific Linux capabilities in my Pod?"]}
{"query": "A user asked the following question:\nQuestion: What kind of applications are best suited for StatefulSets?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- enisoc\n- erictune\n- foxish\n- janetkuo\n- kow3ns\n- smarterclayton\ntitle: StatefulSets\napi_metadata:\n- apiVersion: \"apps/v1\"\nkind: \"StatefulSet\"\ncontent_type: concept\ndescription: >-\nA StatefulSet runs a group of Pods, and maintains a sticky identity for each of those Pods. This is useful for managing\napplications that need persistent storage or a stable, unique network identity.\nweight: 30\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nStatefulSet is the workload API object used to manage stateful applications.  \n{{< glossary_definition term_id=\"statefulset\" length=\"all\" >}}  \n<!-- body -->\n", "relevant_passages": ["What kind of applications are best suited for StatefulSets?"]}
{"query": "A user asked the following question:\nQuestion: Is there an easier way to autoscale a ReplicaSet instead of using a manifest?\nThis is about the following runbook:\nRunbook Title: ReplicaSet as a Horizontal Pod Autoscaler Target\nRunbook Content: Working with ReplicaSetsReplicaSet as a Horizontal Pod Autoscaler TargetA ReplicaSet can also be a target for\n[Horizontal Pod Autoscalers (HPA)](/docs/tasks/run-application/horizontal-pod-autoscale/). That is,\na ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting\nthe ReplicaSet we created in the previous example.  \n{{% code_sample file=\"controllers/hpa-rs.yaml\" %}}  \nSaving this manifest into `hpa-rs.yaml` and submitting it to a Kubernetes cluster should\ncreate the defined HPA that autoscales the target ReplicaSet depending on the CPU usage\nof the replicated Pods.  \n```shell\nkubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml\n```  \nAlternatively, you can use the `kubectl autoscale` command to accomplish the same\n(and it's easier!)  \n```shell\nkubectl autoscale rs frontend --max=10 --min=3 --cpu-percent=50\n```\n", "relevant_passages": ["Is there an easier way to autoscale a ReplicaSet instead of using a manifest?"]}
{"query": "A user asked the following question:\nQuestion: What happens if a Pod created by a Job fails?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- alculquicondor\n- erictune\n- soltysh\ntitle: Jobs\napi_metadata:\n- apiVersion: \"batch/v1\"\nkind: \"Job\"\ncontent_type: concept\ndescription: >-\nJobs represent one-off tasks that run to completion and then stop.\nfeature:\ntitle: Batch execution\ndescription: >\nIn addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.\nweight: 50\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nA Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate.\nAs pods successfully complete, the Job tracks the successful completions. When a specified number\nof successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up\nthe Pods it created. Suspending a Job will delete its active Pods until the Job\nis resumed again.  \nA simple case is to create one Job object in order to reliably run one Pod to completion.\nThe Job object will start a new Pod if the first Pod fails or is deleted (for example\ndue to a node hardware failure or a node reboot).  \nYou can also use a Job to run multiple Pods in parallel.  \nIf you want to run a Job (either a single task, or several in parallel) on a schedule,\nsee [CronJob](/docs/concepts/workloads/controllers/cron-jobs/).  \n<!-- body -->\n", "relevant_passages": ["What happens if a Pod created by a Job fails?"]}
{"query": "A user asked the following question:\nQuestion: What happens when a job reaches its active deadline in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Job termination and cleanup\nRunbook Content: Job termination and cleanupWhen a Job completes, no more Pods are created, but the Pods are [usually](#pod-backoff-failure-policy) not deleted either.\nKeeping them around allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output.\nThe job object also remains after it is completed so that you can view its status. It is up to the user to delete\nold jobs after noting their status. Delete the job with `kubectl` (e.g. `kubectl delete jobs/pi` or `kubectl delete -f ./job.yaml`).\nWhen you delete the job using `kubectl`, all the pods it created are deleted too.  \nBy default, a Job will run uninterrupted unless a Pod fails (`restartPolicy=Never`)\nor a Container exits in error (`restartPolicy=OnFailure`), at which point the Job defers to the\n`.spec.backoffLimit` described above. Once `.spec.backoffLimit` has been reached the Job will\nbe marked as failed and any running Pods will be terminated.  \nAnother way to terminate a Job is by setting an active deadline.\nDo this by setting the `.spec.activeDeadlineSeconds` field of the Job to a number of seconds.\nThe `activeDeadlineSeconds` applies to the duration of the job, no matter how many Pods are created.\nOnce a Job reaches `activeDeadlineSeconds`, all of its running Pods are terminated and the Job status\nwill become `type: Failed` with `reason: DeadlineExceeded`.  \nNote that a Job's `.spec.activeDeadlineSeconds` takes precedence over its `.spec.backoffLimit`.\nTherefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once\nit reaches the time limit specified by `activeDeadlineSeconds`, even if the `backoffLimit` is not yet reached.  \nExample:  \n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: pi-with-timeout\nspec:\nbackoffLimit: 5\nactiveDeadlineSeconds: 100\ntemplate:\nspec:\ncontainers:\n- name: pi\nimage: perl:5.34.0\ncommand: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\nrestartPolicy: Never\n```  \nNote that both the Job spec and the [Pod template spec](/docs/concepts/workloads/pods/init-containers/#detailed-behavior)\nwithin the Job have an `activeDeadlineSeconds` field. Ensure that you set this field at the proper level.  \nKeep in mind that the `restartPolicy` applies to the Pod, and not to the Job itself:\nthere is no automatic Job restart once the Job status is `type: Failed`.\nThat is, the Job termination mechanisms activated with `.spec.activeDeadlineSeconds`\nand `.spec.backoffLimit` result in a permanent Job failure that requires manual intervention to resolve.\n", "relevant_passages": ["What happens when a job reaches its active deadline in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What factors contribute to the Pod's effective request and limit for resources?\nThis is about the following runbook:\nRunbook Title: Resource sharing within containers\nRunbook Content: Detailed behaviorResource sharing within containersGiven the order of execution for init, sidecar and app containers, the following rules\nfor resource usage apply:  \n* The highest of any particular resource request or limit defined on all init\ncontainers is the *effective init request/limit*. If any resource has no\nresource limit specified this is considered as the highest limit.\n* The Pod's *effective request/limit* for a resource is the higher of:\n* the sum of all app containers request/limit for a resource\n* the effective init request/limit for a resource\n* Scheduling is done based on effective requests/limits, which means\ninit containers can reserve resources for initialization that are not used\nduring the life of the Pod.\n* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the\nQoS tier for init containers and app containers alike.  \nQuota and limits are applied based on the effective Pod request and\nlimit.\n", "relevant_passages": ["What factors contribute to the Pod's effective request and limit for resources?"]}
{"query": "A user asked the following question:\nQuestion: How can I change the TTL for a finished Job in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Updating TTL for finished Jobs\nRunbook Content: CaveatsUpdating TTL for finished JobsYou can modify the TTL period, e.g. `.spec.ttlSecondsAfterFinished` field of Jobs,\nafter the job is created or has finished. If you extend the TTL period after the\nexisting `ttlSecondsAfterFinished` period has expired, Kubernetes doesn't guarantee\nto retain that Job, even if an update to extend the TTL returns a successful API\nresponse.\n", "relevant_passages": ["How can I change the TTL for a finished Job in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What kind of functions do Pods in a DaemonSet typically provide?\nThis is about the following runbook:\nRunbook Title: DaemonSet\nRunbook Content: Alternatives to ReplicaSetDaemonSetUse a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicaSet for Pods that provide a\nmachine-level function, such as machine monitoring or machine logging. These Pods have a lifetime that is tied\nto a machine lifetime: the Pod needs to be running on the machine before other Pods start, and are\nsafe to terminate when the machine is otherwise ready to be rebooted/shutdown.\n", "relevant_passages": ["What kind of functions do Pods in a DaemonSet typically provide?"]}
{"query": "A user asked the following question:\nQuestion: What happens if the clocks in my Kubernetes cluster aren't correct?\nThis is about the following runbook:\nRunbook Title: Time skew\nRunbook Content: CaveatsTime skewBecause the TTL-after-finished controller uses timestamps stored in the Kubernetes jobs to\ndetermine whether the TTL has expired or not, this feature is sensitive to time\nskew in your cluster, which may cause the control plane to clean up Job objects\nat the wrong time.  \nClocks aren't always correct, but the difference should be\nvery small. Please be aware of this risk when setting a non-zero TTL.\n", "relevant_passages": ["What happens if the clocks in my Kubernetes cluster aren't correct?"]}
{"query": "A user asked the following question:\nQuestion: What do I need to do to enable the Pod index label feature?\nThis is about the following runbook:\nRunbook Title: Pod index label\nRunbook Content: Pod IdentityPod index label{{< feature-state for_k8s_version=\"v1.28\" state=\"beta\" >}}  \nWhen the StatefulSet {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} creates a Pod,\nthe new Pod is labelled with `apps.kubernetes.io/pod-index`. The value of this label is the ordinal index of\nthe Pod. This label allows you to route traffic to a particular pod index, filter logs/metrics\nusing the pod index label, and more. Note the feature gate `PodIndexLabel` must be enabled for this\nfeature, and it is enabled by default.\n", "relevant_passages": ["What do I need to do to enable the Pod index label feature?"]}
{"query": "A user asked the following question:\nQuestion: How do I pause a rollout for my Deployment?\nThis is about the following runbook:\nRunbook Title: Pausing and Resuming a rollout of a Deployment {#pausing-and-resuming-a-deployment}\nRunbook Content: Pausing and Resuming a rollout of a Deployment {#pausing-and-resuming-a-deployment}When you update a Deployment, or plan to, you can pause rollouts\nfor that Deployment before you trigger one or more updates. When\nyou're ready to apply those changes, you resume rollouts for the\nDeployment. This approach allows you to\napply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.  \n* For example, with a Deployment that was created:  \nGet the Deployment details:\n```shell\nkubectl get deploy\n```\nThe output is similar to this:\n```\nNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx     3         3         3            3           1m\n```\nGet the rollout status:\n```shell\nkubectl get rs\n```\nThe output is similar to this:\n```\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   3         3         3         1m\n```  \n* Pause by running the following command:\n```shell\nkubectl rollout pause deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment paused\n```  \n* Then update the image of the Deployment:\n```shell\nkubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment image updated\n```  \n* Notice that no new rollout started:\n```shell\nkubectl rollout history deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\ndeployments \"nginx\"\nREVISION  CHANGE-CAUSE\n1   <none>\n```\n* Get the rollout status to verify that the existing ReplicaSet has not changed:\n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   3         3         3         2m\n```  \n* You can make as many updates as you wish, for example, update the resources that will be used:\n```shell\nkubectl set resources deployment/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment resource requirements updated\n```  \nThe initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to\nthe Deployment will not have any effect as long as the Deployment rollout is paused.  \n* Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:\n```shell\nkubectl rollout resume deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment resumed\n```\n* {{< glossary_tooltip text=\"Watch\" term_id=\"watch\" >}} the status of the rollout until it's done.\n```shell\nkubectl get rs --watch\n```  \nThe output is similar to this:\n```\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   2         2         2         2m\nnginx-3926361531   2         2         0         6s\nnginx-3926361531   2         2         1         18s\nnginx-2142116321   1         2         2         2m\nnginx-2142116321   1         2         2         2m\nnginx-3926361531   3         2         1         18s\nnginx-3926361531   3         2         1         18s\nnginx-2142116321   1         1         1         2m\nnginx-3926361531   3         3         1         18s\nnginx-3926361531   3         3         2         19s\nnginx-2142116321   0         1         1         2m\nnginx-2142116321   0         1         1         2m\nnginx-2142116321   0         0         0         2m\nnginx-3926361531   3         3         3         20s\n```\n* Get the status of the latest rollout:\n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   0         0         0         2m\nnginx-3926361531   3         3         3         28s\n```\n{{< note >}}\nYou cannot rollback a paused Deployment until you resume it.\n{{< /note >}}\n", "relevant_passages": ["How do I pause a rollout for my Deployment?"]}
{"query": "A user asked the following question:\nQuestion: How does a ReplicaSet handle failing containers?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- Kashomon\n- bprashanth\n- madhusudancs\ntitle: ReplicaSet\napi_metadata:\n- apiVersion: \"apps/v1\"\nkind: \"ReplicaSet\"\nfeature:\ntitle: Self-healing\nanchor: How a ReplicaSet works\ndescription: >\nRestarts containers that fail, replaces and reschedules containers when nodes die,\nkills containers that don't respond to your user-defined health check,\nand doesn't advertise them to clients until they are ready to serve.\ncontent_type: concept\ndescription: >-\nA ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time.\nUsually, you define a Deployment and let that Deployment manage ReplicaSets automatically.\nweight: 20\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nA ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often\nused to guarantee the availability of a specified number of identical Pods.  \n<!-- body -->\n", "relevant_passages": ["How does a ReplicaSet handle failing containers?"]}
{"query": "A user asked the following question:\nQuestion: What should application owners understand to build highly available applications?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- erictune\n- foxish\n- davidopp\ntitle: Disruptions\ncontent_type: concept\nweight: 70\n---  \n<!-- overview -->\nThis guide is for application owners who want to build\nhighly available applications, and thus need to understand\nwhat types of disruptions can happen to Pods.  \nIt is also for cluster administrators who want to perform automated\ncluster actions, like upgrading and autoscaling clusters.  \n<!-- body -->\n", "relevant_passages": ["What should application owners understand to build highly available applications?"]}
{"query": "A user asked the following question:\nQuestion: What is the role of a Deployment in managing Pods?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- janetkuo\ntitle: Deployments\napi_metadata:\n- apiVersion: \"apps/v1\"\nkind: \"Deployment\"\nfeature:\ntitle: Automated rollouts and rollbacks\ndescription: >\nKubernetes progressively rolls out changes to your application or its configuration, while monitoring application health to ensure it doesn't kill all your instances at the same time. If something goes wrong, Kubernetes will rollback the change for you. Take advantage of a growing ecosystem of deployment solutions.\ndescription: >-\nA Deployment manages a set of Pods to run an application workload, usually one that doesn't maintain state.\ncontent_type: concept\nweight: 10\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nA _Deployment_ provides declarative updates for {{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} and\n{{< glossary_tooltip term_id=\"replica-set\" text=\"ReplicaSets\" >}}.  \nYou describe a _desired state_ in a Deployment, and the Deployment {{< glossary_tooltip term_id=\"controller\" >}} changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.  \n{{< note >}}\nDo not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.\n{{< /note >}}  \n<!-- body -->\n", "relevant_passages": ["What is the role of a Deployment in managing Pods?"]}
{"query": "A user asked the following question:\nQuestion: How do I set up a pod selector for a DaemonSet?\nThis is about the following runbook:\nRunbook Title: Pod Selector\nRunbook Content: Writing a DaemonSet SpecPod SelectorThe `.spec.selector` field is a pod selector.  It works the same as the `.spec.selector` of\na [Job](/docs/concepts/workloads/controllers/job/).  \nYou must specify a pod selector that matches the labels of the\n`.spec.template`.\nAlso, once a DaemonSet is created,\nits `.spec.selector` can not be mutated. Mutating the pod selector can lead to the\nunintentional orphaning of Pods, and it was found to be confusing to users.  \nThe `.spec.selector` is an object consisting of two fields:  \n* `matchLabels` - works the same as the `.spec.selector` of a\n[ReplicationController](/docs/concepts/workloads/controllers/replicationcontroller/).\n* `matchExpressions` - allows to build more sophisticated selectors by specifying key,\nlist of values and an operator that relates the key and values.  \nWhen the two are specified the result is ANDed.  \nThe `.spec.selector` must match the `.spec.template.metadata.labels`.\nConfig with these two not matching will be rejected by the API.\n", "relevant_passages": ["How do I set up a pod selector for a DaemonSet?"]}
{"query": "A user asked the following question:\nQuestion: How can I roll out a new release to only a few users?\nThis is about the following runbook:\nRunbook Title: Canary Deployment\nRunbook Content: Canary DeploymentIf you want to roll out releases to a subset of users or servers using the Deployment, you\ncan create multiple Deployments, one for each release, following the canary pattern described in\n[managing resources](/docs/concepts/workloads/management/#canary-deployments).\n", "relevant_passages": ["How can I roll out a new release to only a few users?"]}
{"query": "A user asked the following question:\nQuestion: How does a Pod transition from Pending to Running?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: Pod Lifecycle\ncontent_type: concept\nweight: 30\n---  \n<!-- overview -->  \nThis page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting\nin the `Pending` [phase](#pod-phase), moving through `Running` if at least one\nof its primary containers starts OK, and then through either the `Succeeded` or\n`Failed` phases depending on whether any container in the Pod terminated in failure.  \nLike individual application containers, Pods are considered to be relatively\nephemeral (rather than durable) entities. Pods are created, assigned a unique\nID ([UID](/docs/concepts/overview/working-with-objects/names/#uids)), and scheduled\nto run on nodes where they remain until termination (according to restart policy) or\ndeletion.\nIf a {{< glossary_tooltip term_id=\"node\" >}} dies, the Pods running on (or scheduled\nto run on) that node are [marked for deletion](#pod-garbage-collection). The control\nplane marks the Pods for removal after a timeout period.  \n<!-- body -->\n", "relevant_passages": ["How does a Pod transition from Pending to Running?"]}
{"query": "A user asked the following question:\nQuestion: What should I use if I want to manage scaling with the ReplicationController?\nThis is about the following runbook:\nRunbook Title: Responsibilities of the ReplicationController\nRunbook Content: Responsibilities of the ReplicationControllerThe ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, [readiness](https://issue.k8s.io/620) and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.  \nThe ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in [#492](https://issue.k8s.io/492)), which would change its `replicas` field. We will not add scheduling policies (for example, [spreading](https://issue.k8s.io/367#issuecomment-48428019)) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation ([#170](https://issue.k8s.io/170)).  \nThe ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The \"macro\" operations currently supported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like [Asgard](https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1) managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.\n", "relevant_passages": ["What should I use if I want to manage scaling with the ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: How can I use the downward API to get information about my Pod?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}You can read about [`downwardAPI` volumes](/docs/concepts/storage/volumes/#downwardapi).  \nYou can try using the downward API to expose container- or Pod-level information:\n* as [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)\n* as [files in `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)\n", "relevant_passages": ["How can I use the downward API to get information about my Pod?"]}
{"query": "A user asked the following question:\nQuestion: How do user namespaces enhance security in Kubernetes?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: User Namespaces\nreviewers:\ncontent_type: concept\nweight: 160\nmin-kubernetes-server-version: v1.25\n---  \n<!-- overview -->\n{{< feature-state for_k8s_version=\"v1.30\" state=\"beta\" >}}  \nThis page explains how user namespaces are used in Kubernetes pods. A user\nnamespace isolates the user running inside the container from the one\nin the host.  \nA process running as root in a container can run as a different (non-root) user\nin the host; in other words, the process has full privileges for operations\ninside the user namespace, but is unprivileged for operations outside the\nnamespace.  \nYou can use this feature to reduce the damage a compromised container can do to\nthe host or other pods in the same node. There are [several security\nvulnerabilities][KEP-vulns] rated either **HIGH** or **CRITICAL** that were not\nexploitable when user namespaces is active. It is expected user namespace will\nmitigate some future vulnerabilities too.  \n[KEP-vulns]: https://github.com/kubernetes/enhancements/tree/217d790720c5aef09b8bd4d6ca96284a0affe6c2/keps/sig-node/127-user-namespaces#motivation  \n<!-- body -->\n", "relevant_passages": ["How do user namespaces enhance security in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: Where can I find more information about writing a Job spec?\nThis is about the following runbook:\nRunbook Title: Job template\nRunbook Content: Writing a CronJob specJob templateThe `.spec.jobTemplate` defines a template for the Jobs that the CronJob creates, and it is required.\nIt has exactly the same schema as a [Job](/docs/concepts/workloads/controllers/job/), except that\nit is nested and does not have an `apiVersion` or `kind`.\nYou can specify common metadata for the templated Jobs, such as\n{{< glossary_tooltip text=\"labels\" term_id=\"label\" >}} or\n{{< glossary_tooltip text=\"annotations\" term_id=\"annotation\" >}}.\nFor information about writing a Job `.spec`, see [Writing a Job Spec](/docs/concepts/workloads/controllers/job/#writing-a-job-spec).\n", "relevant_passages": ["Where can I find more information about writing a Job spec?"]}
{"query": "A user asked the following question:\nQuestion: Where can I find instructions for setting up user namespaces in pods?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Take a look at [Use a User Namespace With a Pod](/docs/tasks/configure-pod-container/user-namespaces/)\n", "relevant_passages": ["Where can I find instructions for setting up user namespaces in pods?"]}
{"query": "A user asked the following question:\nQuestion: Can you have more than one app container in a Pod?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: Sidecar Containers\ncontent_type: concept\nweight: 50\n---  \n<!-- overview -->\n{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \nSidecar containers are the secondary containers that run along with the main\napplication container within the same {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}.\nThese containers are used to enhance or to extend the functionality of the primary _app\ncontainer_ by providing additional services, or functionality such as logging, monitoring,\nsecurity, or data synchronization, without directly altering the primary application code.  \nTypically, you only have one app container in a Pod. For example, if you have a web\napplication that requires a local webserver, the local webserver is a sidecar and the\nweb application itself is the app container.  \n<!-- body -->\n", "relevant_passages": ["Can you have more than one app container in a Pod?"]}
{"query": "A user asked the following question:\nQuestion: What happens if a container in a Pod fails and the restart policy is set to 'OnFailure'?\nThis is about the following runbook:\nRunbook Title: Handling Pod and container failures\nRunbook Content: Handling Pod and container failuresA container in a Pod may fail for a number of reasons, such as because the process in it exited with\na non-zero exit code, or the container was killed for exceeding a memory limit, etc. If this\nhappens, and the `.spec.template.spec.restartPolicy = \"OnFailure\"`, then the Pod stays\non the node, but the container is re-run. Therefore, your program needs to handle the case when it is\nrestarted locally, or else specify `.spec.template.spec.restartPolicy = \"Never\"`.\nSee [pod lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/#example-states) for more information on `restartPolicy`.  \nAn entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node\n(node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the\n`.spec.template.spec.restartPolicy = \"Never\"`. When a Pod fails, then the Job controller\nstarts a new Pod. This means that your application needs to handle the case when it is restarted in a new\npod. In particular, it needs to handle temporary files, locks, incomplete output and the like\ncaused by previous runs.  \nBy default, each pod failure is counted towards the `.spec.backoffLimit` limit,\nsee [pod backoff failure policy](#pod-backoff-failure-policy). However, you can\ncustomize handling of pod failures by setting the Job's [pod failure policy](#pod-failure-policy).  \nAdditionally, you can choose to count the pod failures independently for each\nindex of an [Indexed](#completion-mode) Job by setting the `.spec.backoffLimitPerIndex` field\n(for more information, see [backoff limit per index](#backoff-limit-per-index)).  \nNote that even if you specify `.spec.parallelism = 1` and `.spec.completions = 1` and\n`.spec.template.spec.restartPolicy = \"Never\"`, the same program may\nsometimes be started twice.  \nIf you do specify `.spec.parallelism` and `.spec.completions` both greater than 1, then there may be\nmultiple pods running at once. Therefore, your pods must also be tolerant of concurrency.  \nIf you specify the `.spec.podFailurePolicy` field, the Job controller does not consider a terminating\nPod (a pod that has a `.metadata.deletionTimestamp` field set) as a failure until that Pod is\nterminal (its `.status.phase` is `Failed` or `Succeeded`). However, the Job controller\ncreates a replacement Pod as soon as the termination becomes apparent. Once the\npod terminates, the Job controller evaluates `.backoffLimit` and `.podFailurePolicy`\nfor the relevant Job, taking this now-terminated Pod into consideration.  \nIf either of these requirements is not satisfied, the Job controller counts\na terminating Pod as an immediate failure, even if that Pod later terminates\nwith `phase: \"Succeeded\"`.\n", "relevant_passages": ["What happens if a container in a Pod fails and the restart policy is set to 'OnFailure'?"]}
{"query": "A user asked the following question:\nQuestion: Where can I find information on how Kubernetes assigns Pods to Nodes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Learn about [static Pods](#static-pods), which are useful for running Kubernetes\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} components.\n* Find out how to use DaemonSets\n* [Perform a rolling update on a DaemonSet](/docs/tasks/manage-daemon/update-daemon-set/)\n* [Perform a rollback on a DaemonSet](/docs/tasks/manage-daemon/rollback-daemon-set/)\n(for example, if a roll out didn't work how you expected).\n* Understand [how Kubernetes assigns Pods to Nodes](/docs/concepts/scheduling-eviction/assign-pod-node/).\n* Learn about [device plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) and\n[add ons](/docs/concepts/cluster-administration/addons/), which often run as DaemonSets.\n* `DaemonSet` is a top-level resource in the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/daemon-set-v1\" >}}\nobject definition to understand the API for daemon sets.\n", "relevant_passages": ["Where can I find information on how Kubernetes assigns Pods to Nodes?"]}
{"query": "A user asked the following question:\nQuestion: What\u2019s the best way to access DaemonSet Pods using Node IPs?\nThis is about the following runbook:\nRunbook Title: Communicating with Daemon Pods\nRunbook Content: Communicating with Daemon PodsSome possible patterns for communicating with Pods in a DaemonSet are:  \n- **Push**: Pods in the DaemonSet are configured to send updates to another service, such\nas a stats database.  They do not have clients.\n- **NodeIP and Known Port**: Pods in the DaemonSet can use a `hostPort`, so that the pods\nare reachable via the node IPs.\nClients know the list of node IPs somehow, and know the port by convention.\n- **DNS**: Create a [headless service](/docs/concepts/services-networking/service/#headless-services)\nwith the same pod selector, and then discover DaemonSets using the `endpoints`\nresource or retrieve multiple A records from DNS.\n- **Service**: Create a service with the same Pod selector, and use the service to reach a\ndaemon on a random node. (No way to reach specific node.)\n", "relevant_passages": ["What\u2019s the best way to access DaemonSet Pods using Node IPs?"]}
{"query": "A user asked the following question:\nQuestion: How can I set up a Pod failure policy to optimize costs by terminating a Job on specific exit codes?\nThis is about the following runbook:\nRunbook Title: Pod failure policy {#pod-failure-policy}\nRunbook Content: Handling Pod and container failuresPod failure policy {#pod-failure-policy}{{< feature-state feature_gate_name=\"JobPodFailurePolicy\" >}}  \nA Pod failure policy, defined with the `.spec.podFailurePolicy` field, enables\nyour cluster to handle Pod failures based on the container exit codes and the\nPod conditions.  \nIn some situations, you  may want to have a better control when handling Pod\nfailures than the control provided by the [Pod backoff failure policy](#pod-backoff-failure-policy),\nwhich is based on the Job's `.spec.backoffLimit`. These are some examples of use cases:  \n* To optimize costs of running workloads by avoiding unnecessary Pod restarts,\nyou can terminate a Job as soon as one of its Pods fails with an exit code\nindicating a software bug.\n* To guarantee that your Job finishes even if there are disruptions, you can\nignore Pod failures caused by disruptions (such as {{< glossary_tooltip text=\"preemption\" term_id=\"preemption\" >}},\n{{< glossary_tooltip text=\"API-initiated eviction\" term_id=\"api-eviction\" >}}\nor {{< glossary_tooltip text=\"taint\" term_id=\"taint\" >}}-based eviction) so\nthat they don't count towards the `.spec.backoffLimit` limit of retries.  \nYou can configure a Pod failure policy, in the `.spec.podFailurePolicy` field,\nto meet the above use cases. This policy can handle Pod failures based on the\ncontainer exit codes and the Pod conditions.  \nHere is a manifest for a Job that defines a `podFailurePolicy`:  \n{{% code_sample file=\"/controllers/job-pod-failure-policy-example.yaml\" %}}  \nIn the example above, the first rule of the Pod failure policy specifies that\nthe Job should be marked failed if the `main` container fails with the 42 exit\ncode. The following are the rules for the `main` container specifically:  \n- an exit code of 0 means that the container succeeded\n- an exit code of 42 means that the **entire Job** failed\n- any other exit code represents that the container failed, and hence the entire\nPod. The Pod will be re-created if the total number of restarts is\nbelow `backoffLimit`. If the `backoffLimit` is reached the **entire Job** failed.  \n{{< note >}}\nBecause the Pod template specifies a `restartPolicy: Never`,\nthe kubelet does not restart the `main` container in that particular Pod.\n{{< /note >}}  \nThe second rule of the Pod failure policy, specifying the `Ignore` action for\nfailed Pods with condition `DisruptionTarget` excludes Pod disruptions from\nbeing counted towards the `.spec.backoffLimit` limit of retries.  \n{{< note >}}\nIf the Job failed, either by the Pod failure policy or Pod backoff\nfailure policy, and the Job is running multiple Pods, Kubernetes terminates all\nthe Pods in that Job that are still Pending or Running.\n{{< /note >}}  \nThese are some requirements and semantics of the API:  \n- if you want to use a `.spec.podFailurePolicy` field for a Job, you must\nalso define that Job's pod template with `.spec.restartPolicy` set to `Never`.\n- the Pod failure policy rules you specify under `spec.podFailurePolicy.rules`\nare evaluated in order. Once a rule matches a Pod failure, the remaining rules\nare ignored. When no rule matches the Pod failure, the default\nhandling applies.\n- you may want to restrict a rule to a specific container by specifying its name\nin`spec.podFailurePolicy.rules[*].onExitCodes.containerName`. When not specified the rule\napplies to all containers. When specified, it should match one the container\nor `initContainer` names in the Pod template.\n- you may specify the action taken when a Pod failure policy is matched by\n`spec.podFailurePolicy.rules[*].action`. Possible values are:\n- `FailJob`: use to indicate that the Pod's job should be marked as Failed and\nall running Pods should be terminated.\n- `Ignore`: use to indicate that the counter towards the `.spec.backoffLimit`\nshould not be incremented and a replacement Pod should be created.\n- `Count`: use to indicate that the Pod should be handled in the default way.\nThe counter towards the `.spec.backoffLimit` should be incremented.\n- `FailIndex`: use this action along with [backoff limit per index](#backoff-limit-per-index)\nto avoid unnecessary retries within the index of a failed pod.  \n{{< note >}}\nWhen you use a `podFailurePolicy`, the job controller only matches Pods in the\n`Failed` phase. Pods with a deletion timestamp that are not in a terminal phase\n(`Failed` or `Succeeded`) are considered still terminating. This implies that\nterminating pods retain a [tracking finalizer](#job-tracking-with-finalizers)\nuntil they reach a terminal phase.\nSince Kubernetes 1.27, Kubelet transitions deleted pods to a terminal phase\n(see: [Pod Phase](/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase)). This\nensures that deleted pods have their finalizers removed by the Job controller.\n{{< /note >}}  \n{{< note >}}\nStarting with Kubernetes v1.28, when Pod failure policy is used, the Job controller recreates\nterminating Pods only once these Pods reach the terminal `Failed` phase. This behavior is similar\nto `podReplacementPolicy: Failed`. For more information, see [Pod replacement policy](#pod-replacement-policy).\n{{< /note >}}  \nWhen you use the `podFailurePolicy`, and the Job fails due to the pod\nmatching the rule with the `FailJob` action, then the Job controller triggers\nthe Job termination process by adding the `FailureTarget` condition.\nFor more details, see [Job termination and cleanup](#job-termination-and-cleanup).\n", "relevant_passages": ["How can I set up a Pod failure policy to optimize costs by terminating a Job on specific exit codes?"]}
{"query": "A user asked the following question:\nQuestion: What should I do if I need to rollback to an earlier Deployment revision?\nThis is about the following runbook:\nRunbook Title: Use Case\nRunbook Content: Use CaseThe following are typical use cases for Deployments:  \n* [Create a Deployment to rollout a ReplicaSet](#creating-a-deployment). The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.\n* [Declare the new state of the Pods](#updating-a-deployment) by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.\n* [Rollback to an earlier Deployment revision](#rolling-back-a-deployment) if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.\n* [Scale up the Deployment to facilitate more load](#scaling-a-deployment).\n* [Pause the rollout of a Deployment](#pausing-and-resuming-a-deployment) to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.\n* [Use the status of the Deployment](#deployment-status) as an indicator that a rollout has stuck.\n* [Clean up older ReplicaSets](#clean-up-policy) that you don't need anymore.\n", "relevant_passages": ["What should I do if I need to rollback to an earlier Deployment revision?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I don't set a node selector or affinity for my DaemonSet?\nThis is about the following runbook:\nRunbook Title: Running Pods on select Nodes\nRunbook Content: Writing a DaemonSet SpecRunning Pods on select NodesIf you specify a `.spec.template.spec.nodeSelector`, then the DaemonSet controller will\ncreate Pods on nodes which match that [node selector](/docs/concepts/scheduling-eviction/assign-pod-node/).\nLikewise if you specify a `.spec.template.spec.affinity`,\nthen DaemonSet controller will create Pods on nodes which match that\n[node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/).\nIf you do not specify either, then the DaemonSet controller will create Pods on all nodes.\n", "relevant_passages": ["What happens if I don't set a node selector or affinity for my DaemonSet?"]}
{"query": "A user asked the following question:\nQuestion: How does the Job controller keep track of Pods in a Job?\nThis is about the following runbook:\nRunbook Title: Job tracking with finalizers\nRunbook Content: Advanced usageJob tracking with finalizers{{< feature-state for_k8s_version=\"v1.26\" state=\"stable\" >}}  \nThe control plane keeps track of the Pods that belong to any Job and notices if\nany such Pod is removed from the API server. To do that, the Job controller\ncreates Pods with the finalizer `batch.kubernetes.io/job-tracking`. The\ncontroller removes the finalizer only after the Pod has been accounted for in\nthe Job status, allowing the Pod to be removed by other controllers or users.  \n{{< note >}}\nSee [My pod stays terminating](/docs/tasks/debug/debug-application/debug-pods/) if you\nobserve that pods from a Job are stuck with the tracking finalizer.\n{{< /note >}}\n", "relevant_passages": ["How does the Job controller keep track of Pods in a Job?"]}
{"query": "A user asked the following question:\nQuestion: How does a ReplicaSet handle local container restarts for Bare Pods?\nThis is about the following runbook:\nRunbook Title: Bare Pods\nRunbook Content: Alternatives to ReplicaSetBare PodsUnlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or\nterminated for any reason, such as in the case of node failure or disruptive node maintenance,\nsuch as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your\napplication requires only a single Pod. Think of it similarly to a process supervisor, only it\nsupervises multiple Pods across multiple nodes instead of individual processes on a single node. A\nReplicaSet delegates local container restarts to some agent on the node such as Kubelet.\n", "relevant_passages": ["How does a ReplicaSet handle local container restarts for Bare Pods?"]}
{"query": "A user asked the following question:\nQuestion: How can I view the logs of the completed Job?\nThis is about the following runbook:\nRunbook Title: Running an example Job\nRunbook Content: Running an example JobHere is an example Job config. It computes \u03c0 to 2000 places and prints it out.\nIt takes around 10s to complete.  \n{{% code_sample file=\"controllers/job.yaml\" %}}  \nYou can run the example with this command:  \n```shell\nkubectl apply -f https://kubernetes.io/examples/controllers/job.yaml\n```  \nThe output is similar to this:  \n```\njob.batch/pi created\n```  \nCheck on the status of the Job with `kubectl`:  \n{{< tabs name=\"Check status of Job\" >}}\n{{< tab name=\"kubectl describe job pi\" codelang=\"bash\" >}}\nName:           pi\nNamespace:      default\nSelector:       batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\nLabels:         batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\nbatch.kubernetes.io/job-name=pi\n...\nAnnotations:    batch.kubernetes.io/job-tracking: \"\"\nParallelism:    1\nCompletions:    1\nStart Time:     Mon, 02 Dec 2019 15:20:11 +0200\nCompleted At:   Mon, 02 Dec 2019 15:21:16 +0200\nDuration:       65s\nPods Statuses:  0 Running / 1 Succeeded / 0 Failed\nPod Template:\nLabels:  batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\nbatch.kubernetes.io/job-name=pi\nContainers:\npi:\nImage:      perl:5.34.0\nPort:       <none>\nHost Port:  <none>\nCommand:\nperl\n-Mbignum=bpi\n-wle\nprint bpi(2000)\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nEvents:\nType    Reason            Age   From            Message\n----    ------            ----  ----            -------\nNormal  SuccessfulCreate  21s   job-controller  Created pod: pi-xf9p4\nNormal  Completed         18s   job-controller  Job completed\n{{< /tab >}}\n{{< tab name=\"kubectl get job pi -o yaml\" codelang=\"bash\" >}}\napiVersion: batch/v1\nkind: Job\nmetadata:\nannotations: batch.kubernetes.io/job-tracking: \"\"\n...\ncreationTimestamp: \"2022-11-10T17:53:53Z\"\ngeneration: 1\nlabels:\nbatch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\nbatch.kubernetes.io/job-name: pi\nname: pi\nnamespace: default\nresourceVersion: \"4751\"\nuid: 204fb678-040b-497f-9266-35ffa8716d14\nspec:\nbackoffLimit: 4\ncompletionMode: NonIndexed\ncompletions: 1\nparallelism: 1\nselector:\nmatchLabels:\nbatch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\nsuspend: false\ntemplate:\nmetadata:\ncreationTimestamp: null\nlabels:\nbatch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\nbatch.kubernetes.io/job-name: pi\nspec:\ncontainers:\n- command:\n- perl\n- -Mbignum=bpi\n- -wle\n- print bpi(2000)\nimage: perl:5.34.0\nimagePullPolicy: IfNotPresent\nname: pi\nresources: {}\nterminationMessagePath: /dev/termination-log\nterminationMessagePolicy: File\ndnsPolicy: ClusterFirst\nrestartPolicy: Never\nschedulerName: default-scheduler\nsecurityContext: {}\nterminationGracePeriodSeconds: 30\nstatus:\nactive: 1\nready: 0\nstartTime: \"2022-11-10T17:53:57Z\"\nuncountedTerminatedPods: {}\n{{< /tab >}}\n{{< /tabs >}}  \nTo view completed Pods of a Job, use `kubectl get pods`.  \nTo list all the Pods that belong to a Job in a machine readable form, you can use a command like this:  \n```shell\npods=$(kubectl get pods --selector=batch.kubernetes.io/job-name=pi --output=jsonpath='{.items[*].metadata.name}')\necho $pods\n```  \nThe output is similar to this:  \n```\npi-5rwd7\n```  \nHere, the selector is the same as the selector for the Job. The `--output=jsonpath` option specifies an expression\nwith the name from each Pod in the returned list.  \nView the standard output of one of the pods:  \n```shell\nkubectl logs $pods\n```  \nAnother way to view the logs of a Job:  \n```shell\nkubectl logs jobs/pi\n```  \nThe output is similar to this:  \n```\n3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901\n```\n", "relevant_passages": ["How can I view the logs of the completed Job?"]}
{"query": "A user asked the following question:\nQuestion: How do I create a ReplicaSet and its Pods in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Example\nRunbook Content: Example{{% code_sample file=\"controllers/frontend.yaml\" %}}  \nSaving this manifest into `frontend.yaml` and submitting it to a Kubernetes cluster will\ncreate the defined ReplicaSet and the Pods that it manages.  \n```shell\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\n```  \nYou can then get the current ReplicaSets deployed:  \n```shell\nkubectl get rs\n```  \nAnd see the frontend one you created:  \n```\nNAME       DESIRED   CURRENT   READY   AGE\nfrontend   3         3         3       6s\n```  \nYou can also check on the state of the ReplicaSet:  \n```shell\nkubectl describe rs/frontend\n```  \nAnd you will see output similar to:  \n```\nName:         frontend\nNamespace:    default\nSelector:     tier=frontend\nLabels:       app=guestbook\ntier=frontend\nAnnotations:  <none>\nReplicas:     3 current / 3 desired\nPods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\nLabels:  tier=frontend\nContainers:\nphp-redis:\nImage:        us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5\nPort:         <none>\nHost Port:    <none>\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nEvents:\nType    Reason            Age   From                   Message\n----    ------            ----  ----                   -------\nNormal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-gbgfx\nNormal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-rwz57\nNormal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-wkl7w\n```  \nAnd lastly you can check for the Pods brought up:  \n```shell\nkubectl get pods\n```  \nYou should see Pod information similar to:  \n```\nNAME             READY   STATUS    RESTARTS   AGE\nfrontend-gbgfx   1/1     Running   0          10m\nfrontend-rwz57   1/1     Running   0          10m\nfrontend-wkl7w   1/1     Running   0          10m\n```  \nYou can also verify that the owner reference of these pods is set to the frontend ReplicaSet.\nTo do this, get the yaml of one of the Pods running:  \n```shell\nkubectl get pods frontend-gbgfx -o yaml\n```  \nThe output will look similar to this, with the frontend ReplicaSet's info set in the metadata's ownerReferences field:  \n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: \"2024-02-28T22:30:44Z\"\ngenerateName: frontend-\nlabels:\ntier: frontend\nname: frontend-gbgfx\nnamespace: default\nownerReferences:\n- apiVersion: apps/v1\nblockOwnerDeletion: true\ncontroller: true\nkind: ReplicaSet\nname: frontend\nuid: e129deca-f864-481b-bb16-b27abfd92292\n...\n```\n", "relevant_passages": ["How do I create a ReplicaSet and its Pods in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What should I do if the drain command blocks due to insufficient resources?\nThis is about the following runbook:\nRunbook Title: PodDisruptionBudget example {#pdb-example}\nRunbook Content: PodDisruptionBudget example {#pdb-example}Consider a cluster with 3 nodes, `node-1` through `node-3`.\nThe cluster is running several applications.  One of them has 3 replicas initially called\n`pod-a`, `pod-b`, and `pod-c`.  Another, unrelated pod without a PDB, called `pod-x`, is also shown.\nInitially, the pods are laid out as follows:  \n|       node-1         |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *available*   | pod-b *available*   | pod-c *available*  |\n| pod-x  *available*   |                     |                    |  \nAll 3 pods are part of a deployment, and they collectively have a PDB which requires\nthere be at least 2 of the 3 pods to be available at all times.  \nFor example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.\nThe cluster administrator first tries to drain `node-1` using the `kubectl drain` command.\nThat tool tries to evict `pod-a` and `pod-x`.  This succeeds immediately.\nBoth pods go into the `terminating` state at the same time.\nThis puts the cluster in this state:  \n|   node-1 *draining*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |\n| pod-x  *terminating* |                     |                    |  \nThe deployment notices that one of the pods is terminating, so it creates a replacement\ncalled `pod-d`.  Since `node-1` is cordoned, it lands on another node.  Something has\nalso created `pod-y` as a replacement for `pod-x`.  \n(Note: for a StatefulSet, `pod-a`, which would be called something like `pod-0`, would need\nto terminate completely before its replacement, which is also called `pod-0` but has a\ndifferent UID, could be created.  Otherwise, the example applies to a StatefulSet as well.)  \nNow the cluster is in this state:  \n|   node-1 *draining*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |\n| pod-x  *terminating* | pod-d *starting*    | pod-y              |  \nAt some point, the pods terminate, and the cluster looks like this:  \n|    node-1 *drained*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n|                      | pod-b *available*   | pod-c *available*  |\n|                      | pod-d *starting*    | pod-y              |  \nAt this point, if an impatient cluster administrator tries to drain `node-2` or\n`node-3`, the drain command will block, because there are only 2 available\npods for the deployment, and its PDB requires at least 2.  After some time passes, `pod-d` becomes available.  \nThe cluster state now looks like this:  \n|    node-1 *drained*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n|                      | pod-b *available*   | pod-c *available*  |\n|                      | pod-d *available*   | pod-y              |  \nNow, the cluster administrator tries to drain `node-2`.\nThe drain command will try to evict the two pods in some order, say\n`pod-b` first and then `pod-d`.  It will succeed at evicting `pod-b`.\nBut, when it tries to evict `pod-d`, it will be refused because that would leave only\none pod available for the deployment.  \nThe deployment creates a replacement for `pod-b` called `pod-e`.\nBecause there are not enough resources in the cluster to schedule\n`pod-e` the drain will again block.  The cluster may end up in this\nstate:  \n|    node-1 *drained*  |       node-2        |       node-3       | *no node*          |\n|:--------------------:|:-------------------:|:------------------:|:------------------:|\n|                      | pod-b *terminating* | pod-c *available*  | pod-e *pending*    |\n|                      | pod-d *available*   | pod-y              |                    |  \nAt this point, the cluster administrator needs to\nadd a node back to the cluster to proceed with the upgrade.  \nYou can see how Kubernetes varies the rate at which disruptions\ncan happen, according to:  \n- how many replicas an application needs\n- how long it takes to gracefully shutdown an instance\n- how long it takes a new instance to start up\n- the type of controller\n- the cluster's resource capacity\n", "relevant_passages": ["What should I do if the drain command blocks due to insufficient resources?"]}
{"query": "A user asked the following question:\nQuestion: What\u2019s the deal with using gRPC for container health checks?\nThis is about the following runbook:\nRunbook Title: Check mechanisms {#probe-check-methods}\nRunbook Content: Container probesCheck mechanisms {#probe-check-methods}There are four different ways to check a container using a probe.\nEach probe must define exactly one of these four mechanisms:  \n`exec`\n: Executes a specified command inside the container. The diagnostic\nis considered successful if the command exits with a status code of 0.  \n`grpc`\n: Performs a remote procedure call using [gRPC](https://grpc.io/).\nThe target should implement\n[gRPC health checks](https://grpc.io/grpc/core/md_doc_health-checking.html).\nThe diagnostic is considered successful if the `status`\nof the response is `SERVING`.  \n`httpGet`\n: Performs an HTTP `GET` request against the Pod's IP\naddress on a specified port and path. The diagnostic is\nconsidered successful if the response has a status code\ngreater than or equal to 200 and less than 400.  \n`tcpSocket`\n: Performs a TCP check against the Pod's IP address on\na specified port. The diagnostic is considered successful if\nthe port is open. If the remote system (the container) closes\nthe connection immediately after it opens, this counts as healthy.  \n{{< caution >}} Unlike the other mechanisms, `exec` probe's implementation involves the creation/forking of multiple processes each time when executed.\nAs a result, in case of the clusters having higher pod densities, lower intervals of `initialDelaySeconds`, `periodSeconds`, configuring any probe with exec mechanism might introduce an overhead on the cpu usage of the node.\nIn such scenarios, consider using the alternative probe mechanisms to avoid the overhead.{{< /caution >}}\n", "relevant_passages": ["What\u2019s the deal with using gRPC for container health checks?"]}
{"query": "A user asked the following question:\nQuestion: What's the default number of old ReplicaSets kept for a Deployment?\nThis is about the following runbook:\nRunbook Title: Clean up Policy\nRunbook Content: Clean up PolicyYou can set `.spec.revisionHistoryLimit` field in a Deployment to specify how many old ReplicaSets for\nthis Deployment you want to retain. The rest will be garbage-collected in the background. By default,\nit is 10.  \n{{< note >}}\nExplicitly setting this field to 0, will result in cleaning up all the history of your Deployment\nthus that Deployment will not be able to roll back.\n{{< /note >}}\n", "relevant_passages": ["What's the default number of old ReplicaSets kept for a Deployment?"]}
{"query": "A user asked the following question:\nQuestion: Can I change the number of replicas in a Deployment update?\nThis is about the following runbook:\nRunbook Title: Rollover (aka multiple updates in-flight)\nRunbook Content: Updating a DeploymentRollover (aka multiple updates in-flight)Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up\nthe desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels\nmatch `.spec.selector` but whose template does not match `.spec.template` are scaled down. Eventually, the new\nReplicaSet is scaled to `.spec.replicas` and all old ReplicaSets is scaled to 0.  \nIf you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet\nas per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously\n-- it will add it to its list of old ReplicaSets and start scaling it down.  \nFor example, suppose you create a Deployment to create 5 replicas of `nginx:1.14.2`,\nbut then update the Deployment to create 5 replicas of `nginx:1.16.1`, when only 3\nreplicas of `nginx:1.14.2` had been created. In that case, the Deployment immediately starts\nkilling the 3 `nginx:1.14.2` Pods that it had created, and starts creating\n`nginx:1.16.1` Pods. It does not wait for the 5 replicas of `nginx:1.14.2` to be created\nbefore changing course.\n", "relevant_passages": ["Can I change the number of replicas in a Deployment update?"]}
{"query": "A user asked the following question:\nQuestion: What occurs if I change the `.spec.suspend` field from true to false?\nThis is about the following runbook:\nRunbook Title: Schedule suspension\nRunbook Content: Writing a CronJob specSchedule suspensionYou can suspend execution of Jobs for a CronJob, by setting the optional `.spec.suspend` field\nto true. The field defaults to false.  \nThis setting does _not_ affect Jobs that the CronJob has already started.  \nIf you do set that field to true, all subsequent executions are suspended (they remain\nscheduled, but the CronJob controller does not start the Jobs to run the tasks) until\nyou unsuspend the CronJob.  \n{{< caution >}}\nExecutions that are suspended during their scheduled time count as missed Jobs.\nWhen `.spec.suspend` changes from `true` to `false` on an existing CronJob without a\n[starting deadline](#starting-deadline), the missed Jobs are scheduled immediately.\n{{< /caution >}}\n", "relevant_passages": ["What occurs if I change the `.spec.suspend` field from true to false?"]}
{"query": "A user asked the following question:\nQuestion: How do I create static pods in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Static Pods\nRunbook Content: Alternatives to DaemonSetStatic PodsIt is possible to create Pods by writing a file to a certain directory watched by Kubelet.  These\nare called [static pods](/docs/tasks/configure-pod-container/static-pod/).\nUnlike DaemonSet, static Pods cannot be managed with kubectl\nor other Kubernetes API clients.  Static Pods do not depend on the apiserver, making them useful\nin cluster bootstrapping cases.  Also, static Pods may be deprecated in the future.\n", "relevant_passages": ["How do I create static pods in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: Is the `.spec.selector` field mandatory in a Job spec?\nThis is about the following runbook:\nRunbook Title: Pod selector\nRunbook Content: Writing a Job specPod selectorThe `.spec.selector` field is optional. In almost all cases you should not specify it.\nSee section [specifying your own pod selector](#specifying-your-own-pod-selector).\n", "relevant_passages": ["Is the `.spec.selector` field mandatory in a Job spec?"]}
{"query": "A user asked the following question:\nQuestion: Can I create pods with labels that match the ReplicationController's selector?\nThis is about the following runbook:\nRunbook Title: Pod Selector\nRunbook Content: Writing a ReplicationController ManifestPod SelectorThe `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/#label-selectors). A ReplicationController\nmanages all the pods with labels that match the selector. It does not distinguish\nbetween pods that it created or deleted and pods that another person or process created or\ndeleted. This allows the ReplicationController to be replaced without affecting the running pods.  \nIf specified, the `.spec.template.metadata.labels` must be equal to the `.spec.selector`, or it will\nbe rejected by the API.  If `.spec.selector` is unspecified, it will be defaulted to\n`.spec.template.metadata.labels`.  \nAlso you should not normally create any pods whose labels match this selector, either directly, with\nanother ReplicationController, or with another controller such as Job. If you do so, the\nReplicationController thinks that it created the other pods.  Kubernetes does not stop you\nfrom doing this.  \nIf you do end up with multiple controllers that have overlapping selectors, you\nwill have to manage the deletion yourself (see [below](#working-with-replicationcontrollers)).\n", "relevant_passages": ["Can I create pods with labels that match the ReplicationController's selector?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I try to update an image in a Deployment that is blocked by maxUnavailable?\nThis is about the following runbook:\nRunbook Title: Proportional scaling\nRunbook Content: Scaling a DeploymentProportional scalingRollingUpdate Deployments support running multiple versions of an application at the same time. When you\nor an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress\nor paused), the Deployment controller balances the additional replicas in the existing active\nReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called *proportional scaling*.  \nFor example, you are running a Deployment with 10 replicas, [maxSurge](#max-surge)=3, and [maxUnavailable](#max-unavailable)=2.  \n* Ensure that the 10 replicas in your Deployment are running.\n```shell\nkubectl get deploy\n```\nThe output is similar to this:  \n```\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment     10        10        10           10          50s\n```  \n* You update to a new image which happens to be unresolvable from inside the cluster.\n```shell\nkubectl set image deployment/nginx-deployment nginx=nginx:sometag\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment image updated\n```  \n* The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the\n`maxUnavailable` requirement that you mentioned above. Check out the rollout status:\n```shell\nkubectl get rs\n```\nThe output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY     AGE\nnginx-deployment-1989198191   5         5         0         9s\nnginx-deployment-618515232    8         8         8         1m\n```  \n* Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas\nto 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using\nproportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you\nspread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the\nmost replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the\nReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.  \nIn our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the\nnew ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming\nthe new replicas become healthy. To confirm this, run:  \n```shell\nkubectl get deploy\n```  \nThe output is similar to this:\n```\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment     15        18        7            8           7m\n```\nThe rollout status confirms how the replicas were added to each ReplicaSet.\n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY     AGE\nnginx-deployment-1989198191   7         7         0         7m\nnginx-deployment-618515232    11        11        11        7m\n```\n", "relevant_passages": ["What happens if I try to update an image in a Deployment that is blocked by maxUnavailable?"]}
{"query": "A user asked the following question:\nQuestion: Does the .spec.os.name field affect how the kube-scheduler assigns nodes to Pods?\nThis is about the following runbook:\nRunbook Title: Pod OS\nRunbook Content: Working with PodsPod OS{{< feature-state state=\"stable\" for_k8s_version=\"v1.25\" >}}  \nYou should set the `.spec.os.name` field to either `windows` or `linux` to indicate the OS on\nwhich you want the pod to run. These two are the only operating systems supported for now by\nKubernetes. In the future, this list may be expanded.  \nIn Kubernetes v{{< skew currentVersion >}}, the value of `.spec.os.name` does not affect\nhow the {{< glossary_tooltip text=\"kube-scheduler\" term_id=\"kube-scheduler\" >}}\npicks a node for the Pod to run on. In any cluster where there is more than one operating system for\nrunning nodes, you should set the\n[kubernetes.io/os](/docs/reference/labels-annotations-taints/#kubernetes-io-os)\nlabel correctly on each node, and define pods with a `nodeSelector` based on the operating system\nlabel. The kube-scheduler assigns your pod to a node based on other criteria and may or may not\nsucceed in picking a suitable node placement where the node OS is right for the containers in that Pod.\nThe [Pod security standards](/docs/concepts/security/pod-security-standards/) also use this\nfield to avoid enforcing policies that aren't relevant to the operating system.\n", "relevant_passages": ["Does the .spec.os.name field affect how the kube-scheduler assigns nodes to Pods?"]}
{"query": "A user asked the following question:\nQuestion: Why do I need differentiating labels for pods in a rolling update?\nThis is about the following runbook:\nRunbook Title: Rolling updates\nRunbook Content: Common usage patternsRolling updatesThe ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.  \nAs explained in [#1353](https://issue.k8s.io/1353), the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.  \nIdeally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.  \nThe two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.\n", "relevant_passages": ["Why do I need differentiating labels for pods in a rolling update?"]}
{"query": "A user asked the following question:\nQuestion: How does the kubelet handle static Pods?\nThis is about the following runbook:\nRunbook Title: Static Pods\nRunbook Content: Static Pods_Static Pods_ are managed directly by the kubelet daemon on a specific node,\nwithout the {{< glossary_tooltip text=\"API server\" term_id=\"kube-apiserver\" >}}\nobserving them.\nWhereas most Pods are managed by the control plane (for example, a\n{{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}), for static\nPods, the kubelet directly supervises each static Pod (and restarts it if it fails).  \nStatic Pods are always bound to one {{< glossary_tooltip term_id=\"kubelet\" >}} on a specific node.\nThe main use for static Pods is to run a self-hosted control plane: in other words,\nusing the kubelet to supervise the individual [control plane components](/docs/concepts/architecture/#control-plane-components).  \nThe kubelet automatically tries to create a {{< glossary_tooltip text=\"mirror Pod\" term_id=\"mirror-pod\" >}}\non the Kubernetes API server for each static Pod.\nThis means that the Pods running on a node are visible on the API server,\nbut cannot be controlled from there. See the guide [Create static Pods](/docs/tasks/configure-pod-container/static-pod) for more information.  \n{{< note >}}\nThe `spec` of a static Pod cannot refer to other API objects\n(e.g., {{< glossary_tooltip text=\"ServiceAccount\" term_id=\"service-account\" >}},\n{{< glossary_tooltip text=\"ConfigMap\" term_id=\"configmap\" >}},\n{{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}, etc).\n{{< /note >}}\n", "relevant_passages": ["How does the kubelet handle static Pods?"]}
{"query": "A user asked the following question:\nQuestion: What happens to a Pod's phase when it fails to start repeatedly?\nThis is about the following runbook:\nRunbook Title: Pod phase\nRunbook Content: Pod phaseA Pod's `status` field is a\n[PodStatus](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podstatus-v1-core)\nobject, which has a `phase` field.  \nThe phase of a Pod is a simple, high-level summary of where the Pod is in its\nlifecycle. The phase is not intended to be a comprehensive rollup of observations\nof container or Pod state, nor is it intended to be a comprehensive state machine.  \nThe number and meanings of Pod phase values are tightly guarded.\nOther than what is documented here, nothing should be assumed about Pods that\nhave a given `phase` value.  \nHere are the possible values for `phase`:  \nValue       | Description\n:-----------|:-----------\n`Pending`   | The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.\n`Running`   | The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.\n`Succeeded` | All containers in the Pod have terminated in success, and will not be restarted.\n`Failed`    | All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system, and is not set for automatic restarting.\n`Unknown`   | For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.  \n{{< note >}}  \nWhen a pod is failing to start repeatedly, `CrashLoopBackOff` may appear in the `Status` field of some kubectl commands. Similarly, when a pod is being deleted, `Terminating` may appear in the `Status` field of some kubectl commands.  \nMake sure not to confuse _Status_, a kubectl display field for user intuition, with the pod's `phase`.\nPod phase is an explicit part of the Kubernetes data model and of the\n[Pod API](/docs/reference/kubernetes-api/workload-resources/pod-v1/).  \n```\nNAMESPACE               NAME               READY   STATUS             RESTARTS   AGE\nalessandras-namespace   alessandras-pod    0/1     CrashLoopBackOff   200        2d9h\n```  \n---  \nA Pod is granted a term to terminate gracefully, which defaults to 30 seconds.\nYou can use the flag `--force` to [terminate a Pod by force](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced).\n{{< /note >}}  \nSince Kubernetes 1.27, the kubelet transitions deleted Pods, except for\n[static Pods](/docs/tasks/configure-pod-container/static-pod/) and\n[force-deleted Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced)\nwithout a finalizer, to a terminal phase (`Failed` or `Succeeded` depending on\nthe exit statuses of the pod containers) before their deletion from the API server.  \nIf a node dies or is disconnected from the rest of the cluster, Kubernetes\napplies a policy for setting the `phase` of all Pods on the lost node to Failed.\n", "relevant_passages": ["What happens to a Pod's phase when it fails to start repeatedly?"]}
{"query": "A user asked the following question:\nQuestion: How can I check if a Job is currently suspended?\nThis is about the following runbook:\nRunbook Title: Suspending a Job\nRunbook Content: Advanced usageSuspending a Job{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}  \nWhen a Job is created, the Job controller will immediately begin creating Pods\nto satisfy the Job's requirements and will continue to do so until the Job is\ncomplete. However, you may want to temporarily suspend a Job's execution and\nresume it later, or start Jobs in suspended state and have a custom controller\ndecide later when to start them.  \nTo suspend a Job, you can update the `.spec.suspend` field of\nthe Job to true; later, when you want to resume it again, update it to false.\nCreating a Job with `.spec.suspend` set to true will create it in the suspended\nstate.  \nWhen a Job is resumed from suspension, its `.status.startTime` field will be\nreset to the current time. This means that the `.spec.activeDeadlineSeconds`\ntimer will be stopped and reset when a Job is suspended and resumed.  \nWhen you suspend a Job, any running Pods that don't have a status of `Completed`\nwill be [terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)\nwith a SIGTERM signal. The Pod's graceful termination period will be honored and\nyour Pod must handle this signal in this period. This may involve saving\nprogress for later or undoing changes. Pods terminated this way will not count\ntowards the Job's `completions` count.  \nAn example Job definition in the suspended state can be like so:  \n```shell\nkubectl get job myjob -o yaml\n```  \n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: myjob\nspec:\nsuspend: true\nparallelism: 1\ncompletions: 5\ntemplate:\nspec:\n...\n```  \nYou can also toggle Job suspension by patching the Job using the command line.  \nSuspend an active Job:  \n```shell\nkubectl patch job/myjob --type=strategic --patch '{\"spec\":{\"suspend\":true}}'\n```  \nResume a suspended Job:  \n```shell\nkubectl patch job/myjob --type=strategic --patch '{\"spec\":{\"suspend\":false}}'\n```  \nThe Job's status can be used to determine if a Job is suspended or has been\nsuspended in the past:  \n```shell\nkubectl get jobs/myjob -o yaml\n```  \n```yaml\napiVersion: batch/v1\nkind: Job\n# .metadata and .spec omitted\nstatus:\nconditions:\n- lastProbeTime: \"2021-02-05T13:14:33Z\"\nlastTransitionTime: \"2021-02-05T13:14:33Z\"\nstatus: \"True\"\ntype: Suspended\nstartTime: \"2021-02-05T13:13:48Z\"\n```  \nThe Job condition of type \"Suspended\" with status \"True\" means the Job is\nsuspended; the `lastTransitionTime` field can be used to determine how long the\nJob has been suspended for. If the status of that condition is \"False\", then the\nJob was previously suspended and is now running. If such a condition does not\nexist in the Job's status, the Job has never been stopped.  \nEvents are also created when the Job is suspended and resumed:  \n```shell\nkubectl describe jobs/myjob\n```  \n```\nName:           myjob\n...\nEvents:\nType    Reason            Age   From            Message\n----    ------            ----  ----            -------\nNormal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl\nNormal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl\nNormal  Suspended         11m   job-controller  Job suspended\nNormal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44\nNormal  Resumed           3s    job-controller  Job resumed\n```  \nThe last four events, particularly the \"Suspended\" and \"Resumed\" events, are\ndirectly a result of toggling the `.spec.suspend` field. In the time between\nthese two events, we see that no Pods were created, but Pod creation restarted\nas soon as the Job was resumed.\n", "relevant_passages": ["How can I check if a Job is currently suspended?"]}
{"query": "A user asked the following question:\nQuestion: Is there a way to access Container-level fields in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Available fields\nRunbook Content: Available fieldsOnly some Kubernetes API fields are available through the downward API. This\nsection lists which fields you can make available.  \nYou can pass information from available Pod-level fields using `fieldRef`.\nAt the API level, the `spec` for a Pod always defines at least one\n[Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container).\nYou can pass information from available Container-level fields using\n`resourceFieldRef`.\n", "relevant_passages": ["Is there a way to access Container-level fields in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: Is a Deployment paused by default when created?\nThis is about the following runbook:\nRunbook Title: Paused\nRunbook Content: Writing a Deployment SpecPaused`.spec.paused` is an optional boolean field for pausing and resuming a Deployment. The only difference between\na paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused\nDeployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when\nit is created.\n", "relevant_passages": ["Is a Deployment paused by default when created?"]}
{"query": "A user asked the following question:\nQuestion: How do I monitor the progress of my Deployment?\nThis is about the following runbook:\nRunbook Title: Progressing Deployment\nRunbook Content: Deployment statusProgressing DeploymentKubernetes marks a Deployment as _progressing_ when one of the following tasks is performed:  \n* The Deployment creates a new ReplicaSet.\n* The Deployment is scaling up its newest ReplicaSet.\n* The Deployment is scaling down its older ReplicaSet(s).\n* New Pods become ready or available (ready for at least [MinReadySeconds](#min-ready-seconds)).  \nWhen the rollout becomes \u201cprogressing\u201d, the Deployment controller adds a condition with the following\nattributes to the Deployment's `.status.conditions`:  \n* `type: Progressing`\n* `status: \"True\"`\n* `reason: NewReplicaSetCreated` | `reason: FoundNewReplicaSet` | `reason: ReplicaSetUpdated`  \nYou can monitor the progress for a Deployment by using `kubectl rollout status`.\n", "relevant_passages": ["How do I monitor the progress of my Deployment?"]}
{"query": "A user asked the following question:\nQuestion: What do I need to know about running a stateless application?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Learn about [Deployments](/docs/concepts/workloads/controllers/deployment/).\n* [Run a Stateless Application Using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/),\nwhich relies on ReplicaSets to work.\n* `ReplicaSet` is a top-level resource in the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/replica-set-v1\" >}}\nobject definition to understand the API for replica sets.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how\nyou can use it to manage application availability during disruptions.\n", "relevant_passages": ["What do I need to know about running a stateless application?"]}
{"query": "A user asked the following question:\nQuestion: What are the requirements for using a Pod failure policy in a Job?\nThis is about the following runbook:\nRunbook Title: Pod failure policy {#pod-failure-policy}\nRunbook Content: Handling Pod and container failuresPod failure policy {#pod-failure-policy}{{< feature-state feature_gate_name=\"JobPodFailurePolicy\" >}}  \nA Pod failure policy, defined with the `.spec.podFailurePolicy` field, enables\nyour cluster to handle Pod failures based on the container exit codes and the\nPod conditions.  \nIn some situations, you  may want to have a better control when handling Pod\nfailures than the control provided by the [Pod backoff failure policy](#pod-backoff-failure-policy),\nwhich is based on the Job's `.spec.backoffLimit`. These are some examples of use cases:  \n* To optimize costs of running workloads by avoiding unnecessary Pod restarts,\nyou can terminate a Job as soon as one of its Pods fails with an exit code\nindicating a software bug.\n* To guarantee that your Job finishes even if there are disruptions, you can\nignore Pod failures caused by disruptions (such as {{< glossary_tooltip text=\"preemption\" term_id=\"preemption\" >}},\n{{< glossary_tooltip text=\"API-initiated eviction\" term_id=\"api-eviction\" >}}\nor {{< glossary_tooltip text=\"taint\" term_id=\"taint\" >}}-based eviction) so\nthat they don't count towards the `.spec.backoffLimit` limit of retries.  \nYou can configure a Pod failure policy, in the `.spec.podFailurePolicy` field,\nto meet the above use cases. This policy can handle Pod failures based on the\ncontainer exit codes and the Pod conditions.  \nHere is a manifest for a Job that defines a `podFailurePolicy`:  \n{{% code_sample file=\"/controllers/job-pod-failure-policy-example.yaml\" %}}  \nIn the example above, the first rule of the Pod failure policy specifies that\nthe Job should be marked failed if the `main` container fails with the 42 exit\ncode. The following are the rules for the `main` container specifically:  \n- an exit code of 0 means that the container succeeded\n- an exit code of 42 means that the **entire Job** failed\n- any other exit code represents that the container failed, and hence the entire\nPod. The Pod will be re-created if the total number of restarts is\nbelow `backoffLimit`. If the `backoffLimit` is reached the **entire Job** failed.  \n{{< note >}}\nBecause the Pod template specifies a `restartPolicy: Never`,\nthe kubelet does not restart the `main` container in that particular Pod.\n{{< /note >}}  \nThe second rule of the Pod failure policy, specifying the `Ignore` action for\nfailed Pods with condition `DisruptionTarget` excludes Pod disruptions from\nbeing counted towards the `.spec.backoffLimit` limit of retries.  \n{{< note >}}\nIf the Job failed, either by the Pod failure policy or Pod backoff\nfailure policy, and the Job is running multiple Pods, Kubernetes terminates all\nthe Pods in that Job that are still Pending or Running.\n{{< /note >}}  \nThese are some requirements and semantics of the API:  \n- if you want to use a `.spec.podFailurePolicy` field for a Job, you must\nalso define that Job's pod template with `.spec.restartPolicy` set to `Never`.\n- the Pod failure policy rules you specify under `spec.podFailurePolicy.rules`\nare evaluated in order. Once a rule matches a Pod failure, the remaining rules\nare ignored. When no rule matches the Pod failure, the default\nhandling applies.\n- you may want to restrict a rule to a specific container by specifying its name\nin`spec.podFailurePolicy.rules[*].onExitCodes.containerName`. When not specified the rule\napplies to all containers. When specified, it should match one the container\nor `initContainer` names in the Pod template.\n- you may specify the action taken when a Pod failure policy is matched by\n`spec.podFailurePolicy.rules[*].action`. Possible values are:\n- `FailJob`: use to indicate that the Pod's job should be marked as Failed and\nall running Pods should be terminated.\n- `Ignore`: use to indicate that the counter towards the `.spec.backoffLimit`\nshould not be incremented and a replacement Pod should be created.\n- `Count`: use to indicate that the Pod should be handled in the default way.\nThe counter towards the `.spec.backoffLimit` should be incremented.\n- `FailIndex`: use this action along with [backoff limit per index](#backoff-limit-per-index)\nto avoid unnecessary retries within the index of a failed pod.  \n{{< note >}}\nWhen you use a `podFailurePolicy`, the job controller only matches Pods in the\n`Failed` phase. Pods with a deletion timestamp that are not in a terminal phase\n(`Failed` or `Succeeded`) are considered still terminating. This implies that\nterminating pods retain a [tracking finalizer](#job-tracking-with-finalizers)\nuntil they reach a terminal phase.\nSince Kubernetes 1.27, Kubelet transitions deleted pods to a terminal phase\n(see: [Pod Phase](/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase)). This\nensures that deleted pods have their finalizers removed by the Job controller.\n{{< /note >}}  \n{{< note >}}\nStarting with Kubernetes v1.28, when Pod failure policy is used, the Job controller recreates\nterminating Pods only once these Pods reach the terminal `Failed` phase. This behavior is similar\nto `podReplacementPolicy: Failed`. For more information, see [Pod replacement policy](#pod-replacement-policy).\n{{< /note >}}  \nWhen you use the `podFailurePolicy`, and the Job fails due to the pod\nmatching the rule with the `FailJob` action, then the Job controller triggers\nthe Job termination process by adding the `FailureTarget` condition.\nFor more details, see [Job termination and cleanup](#job-termination-and-cleanup).\n", "relevant_passages": ["What are the requirements for using a Pod failure policy in a Job?"]}
{"query": "A user asked the following question:\nQuestion: How does proportional scaling work when I scale a Deployment?\nThis is about the following runbook:\nRunbook Title: Proportional scaling\nRunbook Content: Scaling a DeploymentProportional scalingRollingUpdate Deployments support running multiple versions of an application at the same time. When you\nor an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress\nor paused), the Deployment controller balances the additional replicas in the existing active\nReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called *proportional scaling*.  \nFor example, you are running a Deployment with 10 replicas, [maxSurge](#max-surge)=3, and [maxUnavailable](#max-unavailable)=2.  \n* Ensure that the 10 replicas in your Deployment are running.\n```shell\nkubectl get deploy\n```\nThe output is similar to this:  \n```\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment     10        10        10           10          50s\n```  \n* You update to a new image which happens to be unresolvable from inside the cluster.\n```shell\nkubectl set image deployment/nginx-deployment nginx=nginx:sometag\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment image updated\n```  \n* The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the\n`maxUnavailable` requirement that you mentioned above. Check out the rollout status:\n```shell\nkubectl get rs\n```\nThe output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY     AGE\nnginx-deployment-1989198191   5         5         0         9s\nnginx-deployment-618515232    8         8         8         1m\n```  \n* Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas\nto 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using\nproportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you\nspread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the\nmost replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the\nReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.  \nIn our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the\nnew ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming\nthe new replicas become healthy. To confirm this, run:  \n```shell\nkubectl get deploy\n```  \nThe output is similar to this:\n```\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment     15        18        7            8           7m\n```\nThe rollout status confirms how the replicas were added to each ReplicaSet.\n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY     AGE\nnginx-deployment-1989198191   7         7         0         7m\nnginx-deployment-618515232    11        11        11        7m\n```\n", "relevant_passages": ["How does proportional scaling work when I scale a Deployment?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I set the concurrency policy to 'Forbid'?\nThis is about the following runbook:\nRunbook Title: Concurrency policy\nRunbook Content: Writing a CronJob specConcurrency policyThe `.spec.concurrencyPolicy` field is also optional.\nIt specifies how to treat concurrent executions of a Job that is created by this CronJob.\nThe spec may specify only one of the following concurrency policies:  \n* `Allow` (default): The CronJob allows concurrently running Jobs\n* `Forbid`: The CronJob does not allow concurrent runs; if it is time for a new Job run and the\nprevious Job run hasn't finished yet, the CronJob skips the new Job run. Also note that when the\nprevious Job run finishes, `.spec.startingDeadlineSeconds` is still taken into account and may\nresult in a new Job run.\n* `Replace`: If it is time for a new Job run and the previous Job run hasn't finished yet, the\nCronJob replaces the currently running Job run with a new Job run  \nNote that concurrency policy only applies to the Jobs created by the same CronJob.\nIf there are multiple CronJobs, their respective Jobs are always allowed to run concurrently.\n", "relevant_passages": ["What happens if I set the concurrency policy to 'Forbid'?"]}
{"query": "A user asked the following question:\nQuestion: How can I manage traffic between different versions of my application using ReplicationControllers and Services?\nThis is about the following runbook:\nRunbook Title: Using ReplicationControllers with Services\nRunbook Content: Common usage patternsUsing ReplicationControllers with ServicesMultiple ReplicationControllers can sit behind a single service, so that, for example, some traffic\ngoes to the old version, and some goes to the new version.  \nA ReplicationController will never terminate on its own, but it isn't expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.\n", "relevant_passages": ["How can I manage traffic between different versions of my application using ReplicationControllers and Services?"]}
{"query": "A user asked the following question:\nQuestion: What operations is a container in the Waiting state performing?\nThis is about the following runbook:\nRunbook Title: `Waiting` {#container-state-waiting}\nRunbook Content: Container states`Waiting` {#container-state-waiting}If a container is not in either the `Running` or `Terminated` state, it is `Waiting`.\nA container in the `Waiting` state is still running the operations it requires in\norder to complete start up: for example, pulling the container image from a container\nimage registry, or applying {{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}\ndata.\nWhen you use `kubectl` to query a Pod with a container that is `Waiting`, you also see\na Reason field to summarize why the container is in that state.\n", "relevant_passages": ["What operations is a container in the Waiting state performing?"]}
{"query": "A user asked the following question:\nQuestion: Why would I use a StatefulSet instead of a regular Deployment?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- enisoc\n- erictune\n- foxish\n- janetkuo\n- kow3ns\n- smarterclayton\ntitle: StatefulSets\napi_metadata:\n- apiVersion: \"apps/v1\"\nkind: \"StatefulSet\"\ncontent_type: concept\ndescription: >-\nA StatefulSet runs a group of Pods, and maintains a sticky identity for each of those Pods. This is useful for managing\napplications that need persistent storage or a stable, unique network identity.\nweight: 30\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nStatefulSet is the workload API object used to manage stateful applications.  \n{{< glossary_definition term_id=\"statefulset\" length=\"all\" >}}  \n<!-- body -->\n", "relevant_passages": ["Why would I use a StatefulSet instead of a regular Deployment?"]}
{"query": "A user asked the following question:\nQuestion: How do I determine the effective resource limits for init containers?\nThis is about the following runbook:\nRunbook Title: Resource sharing within containers\nRunbook Content: Resource sharing within containers{{< comment >}}\nThis section is also present in the [init containers](/docs/concepts/workloads/pods/init-containers/) page.\nIf you're editing this section, change both places.\n{{< /comment >}}  \nGiven the order of execution for init, sidecar and app containers, the following rules\nfor resource usage apply:  \n* The highest of any particular resource request or limit defined on all init\ncontainers is the *effective init request/limit*. If any resource has no\nresource limit specified this is considered as the highest limit.\n* The Pod's *effective request/limit* for a resource is the sum of\n[pod overhead](/docs/concepts/scheduling-eviction/pod-overhead/) and the higher of:\n* the sum of all non-init containers(app and sidecar containers) request/limit for a\nresource\n* the effective init request/limit for a resource\n* Scheduling is done based on effective requests/limits, which means\ninit containers can reserve resources for initialization that are not used\nduring the life of the Pod.\n* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the\nQoS tier for all init, sidecar and app containers alike.  \nQuota and limits are applied based on the effective Pod request and\nlimit.\n", "relevant_passages": ["How do I determine the effective resource limits for init containers?"]}
{"query": "A user asked the following question:\nQuestion: How can I ensure that my pods keep running even if there's a node failure?\nThis is about the following runbook:\nRunbook Title: Rescheduling\nRunbook Content: Common usage patternsReschedulingAs mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).\n", "relevant_passages": ["How can I ensure that my pods keep running even if there's a node failure?"]}
{"query": "A user asked the following question:\nQuestion: What are the steps to debug init containers?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}Learn more about the following:\n* [Creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).\n* [Debug init containers](/docs/tasks/debug/debug-application/debug-init-containers/).\n* Overview of [kubelet](/docs/reference/command-line-tools-reference/kubelet/) and [kubectl](/docs/reference/kubectl/).\n* [Types of probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe): liveness, readiness, startup probe.\n* [Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers).\n", "relevant_passages": ["What are the steps to debug init containers?"]}
{"query": "A user asked the following question:\nQuestion: How do I delete a completed job in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Job termination and cleanup\nRunbook Content: Job termination and cleanupWhen a Job completes, no more Pods are created, but the Pods are [usually](#pod-backoff-failure-policy) not deleted either.\nKeeping them around allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output.\nThe job object also remains after it is completed so that you can view its status. It is up to the user to delete\nold jobs after noting their status. Delete the job with `kubectl` (e.g. `kubectl delete jobs/pi` or `kubectl delete -f ./job.yaml`).\nWhen you delete the job using `kubectl`, all the pods it created are deleted too.  \nBy default, a Job will run uninterrupted unless a Pod fails (`restartPolicy=Never`)\nor a Container exits in error (`restartPolicy=OnFailure`), at which point the Job defers to the\n`.spec.backoffLimit` described above. Once `.spec.backoffLimit` has been reached the Job will\nbe marked as failed and any running Pods will be terminated.  \nAnother way to terminate a Job is by setting an active deadline.\nDo this by setting the `.spec.activeDeadlineSeconds` field of the Job to a number of seconds.\nThe `activeDeadlineSeconds` applies to the duration of the job, no matter how many Pods are created.\nOnce a Job reaches `activeDeadlineSeconds`, all of its running Pods are terminated and the Job status\nwill become `type: Failed` with `reason: DeadlineExceeded`.  \nNote that a Job's `.spec.activeDeadlineSeconds` takes precedence over its `.spec.backoffLimit`.\nTherefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once\nit reaches the time limit specified by `activeDeadlineSeconds`, even if the `backoffLimit` is not yet reached.  \nExample:  \n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: pi-with-timeout\nspec:\nbackoffLimit: 5\nactiveDeadlineSeconds: 100\ntemplate:\nspec:\ncontainers:\n- name: pi\nimage: perl:5.34.0\ncommand: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\nrestartPolicy: Never\n```  \nNote that both the Job spec and the [Pod template spec](/docs/concepts/workloads/pods/init-containers/#detailed-behavior)\nwithin the Job have an `activeDeadlineSeconds` field. Ensure that you set this field at the proper level.  \nKeep in mind that the `restartPolicy` applies to the Pod, and not to the Job itself:\nthere is no automatic Job restart once the Job status is `type: Failed`.\nThat is, the Job termination mechanisms activated with `.spec.activeDeadlineSeconds`\nand `.spec.backoffLimit` result in a permanent Job failure that requires manual intervention to resolve.\n", "relevant_passages": ["How do I delete a completed job in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: How do I update a deployment while maintaining its availability?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Follow steps to protect your application by [configuring a Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).  \n* Learn more about [draining nodes](/docs/tasks/administer-cluster/safely-drain-node/)  \n* Learn about [updating a deployment](/docs/concepts/workloads/controllers/deployment/#updating-a-deployment)\nincluding steps to maintain its availability during the rollout.\n", "relevant_passages": ["How do I update a deployment while maintaining its availability?"]}
{"query": "A user asked the following question:\nQuestion: What should I check if I want to know about voluntary disruptions in my cluster?\nThis is about the following runbook:\nRunbook Title: Voluntary and involuntary disruptions\nRunbook Content: Voluntary and involuntary disruptionsPods do not disappear until someone (a person or a controller) destroys them, or\nthere is an unavoidable hardware or system software error.  \nWe call these unavoidable cases *involuntary disruptions* to\nan application.  Examples are:  \n- a hardware failure of the physical machine backing the node\n- cluster administrator deletes VM (instance) by mistake\n- cloud provider or hypervisor failure makes VM disappear\n- a kernel panic\n- the node disappears from the cluster due to cluster network partition\n- eviction of a pod due to the node being [out-of-resources](/docs/concepts/scheduling-eviction/node-pressure-eviction/).  \nExcept for the out-of-resources condition, all these conditions\nshould be familiar to most users; they are not specific\nto Kubernetes.  \nWe call other cases *voluntary disruptions*.  These include both\nactions initiated by the application owner and those initiated by a Cluster\nAdministrator.  Typical application owner actions include:  \n- deleting the deployment or other controller that manages the pod\n- updating a deployment's pod template causing a restart\n- directly deleting a pod (e.g. by accident)  \nCluster administrator actions include:  \n- [Draining a node](/docs/tasks/administer-cluster/safely-drain-node/) for repair or upgrade.\n- Draining a node from a cluster to scale the cluster down (learn about\n[Cluster Autoscaling](https://github.com/kubernetes/autoscaler/#readme)).\n- Removing a pod from a node to permit something else to fit on that node.  \nThese actions might be taken directly by the cluster administrator, or by automation\nrun by the cluster administrator, or by your cluster hosting provider.  \nAsk your cluster administrator or consult your cloud provider or distribution documentation\nto determine if any sources of voluntary disruptions are enabled for your cluster.\nIf none are enabled, you can skip creating Pod Disruption Budgets.  \n{{< caution >}}\nNot all voluntary disruptions are constrained by Pod Disruption Budgets. For example,\ndeleting deployments or pods bypasses Pod Disruption Budgets.\n{{< /caution >}}\n", "relevant_passages": ["What should I check if I want to know about voluntary disruptions in my cluster?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I create bare Pods that match a ReplicaSet's selector?\nThis is about the following runbook:\nRunbook Title: Non-Template Pod acquisitions\nRunbook Content: Non-Template Pod acquisitionsWhile you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have\nlabels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited\nto owning Pods specified by its template-- it can acquire other Pods in the manner specified in the previous sections.  \nTake the previous frontend ReplicaSet example, and the Pods specified in the following manifest:  \n{{% code_sample file=\"pods/pod-rs.yaml\" %}}  \nAs those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend\nReplicaSet, they will immediately be acquired by it.  \nSuppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to\nfulfill its replica count requirement:  \n```shell\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\n```  \nThe new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over\nits desired count.  \nFetching the Pods:  \n```shell\nkubectl get pods\n```  \nThe output shows that the new Pods are either already terminated, or in the process of being terminated:  \n```\nNAME             READY   STATUS        RESTARTS   AGE\nfrontend-b2zdv   1/1     Running       0          10m\nfrontend-vcmts   1/1     Running       0          10m\nfrontend-wtsmm   1/1     Running       0          10m\npod1             0/1     Terminating   0          1s\npod2             0/1     Terminating   0          1s\n```  \nIf you create the Pods first:  \n```shell\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\n```  \nAnd then create the ReplicaSet however:  \n```shell\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\n```  \nYou shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the\nnumber of its new Pods and the original matches its desired count. As fetching the Pods:  \n```shell\nkubectl get pods\n```  \nWill reveal in its output:\n```\nNAME             READY   STATUS    RESTARTS   AGE\nfrontend-hmmj2   1/1     Running   0          9s\npod1             1/1     Running   0          36s\npod2             1/1     Running   0          36s\n```  \nIn this manner, a ReplicaSet can own a non-homogeneous set of Pods\n", "relevant_passages": ["What happens if I create bare Pods that match a ReplicaSet's selector?"]}
{"query": "A user asked the following question:\nQuestion: How can I control where my pods run in a parallel job?\nThis is about the following runbook:\nRunbook Title: Mutable Scheduling Directives\nRunbook Content: Advanced usageMutable Scheduling Directives{{< feature-state for_k8s_version=\"v1.27\" state=\"stable\" >}}  \nIn most cases, a parallel job will want the pods to run with constraints,\nlike all in the same zone, or all either on GPU model x or y but not a mix of both.  \nThe [suspend](#suspending-a-job) field is the first step towards achieving those semantics. Suspend allows a\ncustom queue controller to decide when a job should start; However, once a job is unsuspended,\na custom queue controller has no influence on where the pods of a job will actually land.  \nThis feature allows updating a Job's scheduling directives before it starts, which gives custom queue\ncontrollers the ability to influence pod placement while at the same time offloading actual\npod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that have never\nbeen unsuspended before.  \nThe fields in a Job's pod template that can be updated are node affinity, node selector,\ntolerations, labels, annotations and [scheduling gates](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/).\n", "relevant_passages": ["How can I control where my pods run in a parallel job?"]}
{"query": "A user asked the following question:\nQuestion: How do I delete a ReplicationController without affecting its pods?\nThis is about the following runbook:\nRunbook Title: Deleting only a ReplicationController\nRunbook Content: Working with ReplicationControllersDeleting only a ReplicationControllerYou can delete a ReplicationController without affecting any of its pods.  \nUsing kubectl, specify the `--cascade=orphan` option to [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete).  \nWhen using the REST API or [client library](/docs/reference/using-api/client-libraries), you can delete the ReplicationController object.  \nOnce the original is deleted, you can create a new ReplicationController to replace it.  As long\nas the old and new `.spec.selector` are the same, then the new one will adopt the old pods.\nHowever, it will not make any effort to make existing pods match a new, different pod template.\nTo update pods to a new spec in a controlled way, use a [rolling update](#rolling-updates).\n", "relevant_passages": ["How do I delete a ReplicationController without affecting its pods?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I have multiple operating systems in my Kubernetes cluster?\nThis is about the following runbook:\nRunbook Title: Pod OS\nRunbook Content: Working with PodsPod OS{{< feature-state state=\"stable\" for_k8s_version=\"v1.25\" >}}  \nYou should set the `.spec.os.name` field to either `windows` or `linux` to indicate the OS on\nwhich you want the pod to run. These two are the only operating systems supported for now by\nKubernetes. In the future, this list may be expanded.  \nIn Kubernetes v{{< skew currentVersion >}}, the value of `.spec.os.name` does not affect\nhow the {{< glossary_tooltip text=\"kube-scheduler\" term_id=\"kube-scheduler\" >}}\npicks a node for the Pod to run on. In any cluster where there is more than one operating system for\nrunning nodes, you should set the\n[kubernetes.io/os](/docs/reference/labels-annotations-taints/#kubernetes-io-os)\nlabel correctly on each node, and define pods with a `nodeSelector` based on the operating system\nlabel. The kube-scheduler assigns your pod to a node based on other criteria and may or may not\nsucceed in picking a suitable node placement where the node OS is right for the containers in that Pod.\nThe [Pod security standards](/docs/concepts/security/pod-security-standards/) also use this\nfield to avoid enforcing policies that aren't relevant to the operating system.\n", "relevant_passages": ["What happens if I have multiple operating systems in my Kubernetes cluster?"]}
{"query": "A user asked the following question:\nQuestion: What are the possible values for the restartPolicy in a Pod's spec?\nThis is about the following runbook:\nRunbook Title: Container restart policy {#restart-policy}\nRunbook Content: How Pods handle problems with containers {#container-restarts}Container restart policy {#restart-policy}The `spec` of a Pod has a `restartPolicy` field with possible values Always, OnFailure,\nand Never. The default value is Always.  \nThe `restartPolicy` for a Pod applies to {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}\nin the Pod and to regular [init containers](/docs/concepts/workloads/pods/init-containers/).\n[Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\nignore the Pod-level `restartPolicy` field: in Kubernetes, a sidecar is defined as an\nentry inside `initContainers` that has its container-level `restartPolicy` set to `Always`.\nFor init containers that exit with an error, the kubelet restarts the init container if\nthe Pod level `restartPolicy` is either `OnFailure` or `Always`:  \n* `Always`: Automatically restarts the container after any termination.\n* `OnFailure`: Only restarts the container if it exits with an error (non-zero exit status).\n* `Never`: Does not automatically restart the terminated container.  \nWhen the kubelet is handling container restarts according to the configured restart\npolicy, that only applies to restarts that make replacement containers inside the\nsame Pod and running on the same node. After containers in a Pod exit, the kubelet\nrestarts them with an exponential backoff delay (10s, 20s, 40s, \u2026), that is capped at\n300 seconds (5 minutes). Once a container has executed for 10 minutes without any\nproblems, the kubelet resets the restart backoff timer for that container.\n[Sidecar containers and Pod lifecycle](/docs/concepts/workloads/pods/sidecar-containers/#sidecar-containers-and-pod-lifecycle)\nexplains the behaviour of `init containers` when specify `restartpolicy` field on it.\n", "relevant_passages": ["What are the possible values for the restartPolicy in a Pod's spec?"]}
{"query": "A user asked the following question:\nQuestion: What do I need to know about Node-pressure eviction?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [resource management for Pods and Containers](/docs/concepts/configuration/manage-resources-containers/).\n* Learn about [Node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\n* Learn about [Pod priority and preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).\n* Learn about [Pod disruptions](/docs/concepts/workloads/pods/disruptions/).\n* Learn how to [assign memory resources to containers and pods](/docs/tasks/configure-pod-container/assign-memory-resource/).\n* Learn how to [assign CPU resources to containers and pods](/docs/tasks/configure-pod-container/assign-cpu-resource/).\n* Learn how to [configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/).\n", "relevant_passages": ["What do I need to know about Node-pressure eviction?"]}
{"query": "A user asked the following question:\nQuestion: Where can I find more information on how Kubernetes handles shared storage for Pods?\nThis is about the following runbook:\nRunbook Title: Storage in Pods {#pod-storage}\nRunbook Content: Resource sharing and communicationStorage in Pods {#pod-storage}A Pod can specify a set of shared storage\n{{< glossary_tooltip text=\"volumes\" term_id=\"volume\" >}}. All containers\nin the Pod can access the shared volumes, allowing those containers to\nshare data. Volumes also allow persistent data in a Pod to survive\nin case one of the containers within needs to be restarted. See\n[Storage](/docs/concepts/storage/) for more information on how\nKubernetes implements shared storage and makes it available to Pods.\n", "relevant_passages": ["Where can I find more information on how Kubernetes handles shared storage for Pods?"]}
{"query": "A user asked the following question:\nQuestion: Will I get any notifications if I update a CronJob using TZ or CRON_TZ?\nThis is about the following runbook:\nRunbook Title: Unsupported TimeZone specification\nRunbook Content: CronJob limitations {#cron-job-limitations}Unsupported TimeZone specificationSpecifying a timezone using `CRON_TZ` or `TZ` variables inside `.spec.schedule`\nis **not officially supported** (and never has been).  \nStarting with Kubernetes 1.29 if you try to set a schedule that includes `TZ` or `CRON_TZ`\ntimezone specification, Kubernetes will fail to create the resource with a validation\nerror.\nUpdates to CronJobs already using `TZ` or `CRON_TZ` will continue to report a\n[warning](/blog/2020/09/03/warnings/) to the client.\n", "relevant_passages": ["Will I get any notifications if I update a CronJob using TZ or CRON_TZ?"]}
{"query": "A user asked the following question:\nQuestion: How do I update a DaemonSet when node labels change?\nThis is about the following runbook:\nRunbook Title: Updating a DaemonSet\nRunbook Content: Updating a DaemonSetIf node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete\nPods from newly not-matching nodes.  \nYou can modify the Pods that a DaemonSet creates.  However, Pods do not allow all\nfields to be updated.  Also, the DaemonSet controller will use the original template the next\ntime a node (even with the same name) is created.  \nYou can delete a DaemonSet.  If you specify `--cascade=orphan` with `kubectl`, then the Pods\nwill be left on the nodes.  If you subsequently create a new DaemonSet with the same selector,\nthe new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces\nthem according to its `updateStrategy`.  \nYou can [perform a rolling update](/docs/tasks/manage-daemon/update-daemon-set/) on a DaemonSet.\n", "relevant_passages": ["How do I update a DaemonSet when node labels change?"]}
{"query": "A user asked the following question:\nQuestion: How do I set labels on a ReplicationController?\nThis is about the following runbook:\nRunbook Title: Labels on the ReplicationController\nRunbook Content: Writing a ReplicationController ManifestLabels on the ReplicationControllerThe ReplicationController can itself have labels (`.metadata.labels`).  Typically, you\nwould set these the same as the `.spec.template.metadata.labels`; if `.metadata.labels` is not specified\nthen it defaults to  `.spec.template.metadata.labels`. However, they are allowed to be\ndifferent, and the `.metadata.labels` do not affect the behavior of the ReplicationController.\n", "relevant_passages": ["How do I set labels on a ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: Can I control how many Pods are unavailable during a Rolling Update?\nThis is about the following runbook:\nRunbook Title: Strategy\nRunbook Content: Writing a Deployment SpecStrategy`.spec.strategy` specifies the strategy used to replace old Pods by new ones.\n`.spec.strategy.type` can be \"Recreate\" or \"RollingUpdate\". \"RollingUpdate\" is\nthe default value.  \n#### Recreate Deployment  \nAll existing Pods are killed before new ones are created when `.spec.strategy.type==Recreate`.  \n{{< note >}}\nThis will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods\nof the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new\nrevision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the\nreplacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an\n\"at most\" guarantee for your Pods, you should consider using a\n[StatefulSet](/docs/concepts/workloads/controllers/statefulset/).\n{{< /note >}}  \n#### Rolling Update Deployment  \nThe Deployment updates Pods in a rolling update\nfashion when `.spec.strategy.type==RollingUpdate`. You can specify `maxUnavailable` and `maxSurge` to control\nthe rolling update process.  \n##### Max Unavailable  \n`.spec.strategy.rollingUpdate.maxUnavailable` is an optional field that specifies the maximum number\nof Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)\nor a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by\nrounding down. The value cannot be 0 if `.spec.strategy.rollingUpdate.maxSurge` is 0. The default value is 25%.  \nFor example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired\nPods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled\ndown further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available\nat all times during the update is at least 70% of the desired Pods.  \n##### Max Surge  \n`.spec.strategy.rollingUpdate.maxSurge` is an optional field that specifies the maximum number of Pods\nthat can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a\npercentage of desired Pods (for example, 10%). The value cannot be 0 if `MaxUnavailable` is 0. The absolute number\nis calculated from the percentage by rounding up. The default value is 25%.  \nFor example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the\nrolling update starts, such that the total number of old and new Pods does not exceed 130% of desired\nPods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the\ntotal number of Pods running at any time during the update is at most 130% of desired Pods.  \nHere are some Rolling Update Deployment examples that use the `maxUnavailable` and `maxSurge`:  \n{{< tabs name=\"tab_with_md\" >}}\n{{% tab name=\"Max Unavailable\" %}}  \n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nlabels:\napp: nginx\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\nstrategy:\ntype: RollingUpdate\nrollingUpdate:\nmaxUnavailable: 1\n```  \n{{% /tab %}}\n{{% tab name=\"Max Surge\" %}}  \n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nlabels:\napp: nginx\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\nstrategy:\ntype: RollingUpdate\nrollingUpdate:\nmaxSurge: 1\n```  \n{{% /tab %}}\n{{% tab name=\"Hybrid\" %}}  \n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nlabels:\napp: nginx\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\nstrategy:\ntype: RollingUpdate\nrollingUpdate:\nmaxSurge: 1\nmaxUnavailable: 1\n```  \n{{% /tab %}}\n{{< /tabs >}}\n", "relevant_passages": ["Can I control how many Pods are unavailable during a Rolling Update?"]}
{"query": "A user asked the following question:\nQuestion: What does the Pod Disruption Target condition indicate?\nThis is about the following runbook:\nRunbook Title: Pod disruption conditions {#pod-disruption-conditions}\nRunbook Content: Pod disruption conditions {#pod-disruption-conditions}{{< feature-state feature_gate_name=\"PodDisruptionConditions\" >}}  \nA dedicated Pod `DisruptionTarget` [condition](/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions)\nis added to indicate\nthat the Pod is about to be deleted due to a {{<glossary_tooltip term_id=\"disruption\" text=\"disruption\">}}.\nThe `reason` field of the condition additionally\nindicates one of the following reasons for the Pod termination:  \n`PreemptionByScheduler`\n: Pod is due to be {{<glossary_tooltip term_id=\"preemption\" text=\"preempted\">}} by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see [Pod priority preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).  \n`DeletionByTaintManager`\n: Pod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within `kube-controller-manager`) due to a `NoExecute` taint that the Pod does not tolerate; see {{<glossary_tooltip term_id=\"taint\" text=\"taint\">}}-based evictions.  \n`EvictionByEvictionAPI`\n: Pod has been marked for {{<glossary_tooltip term_id=\"api-eviction\" text=\"eviction using the Kubernetes API\">}} .  \n`DeletionByPodGC`\n: Pod, that is bound to a no longer existing Node, is due to be deleted by [Pod garbage collection](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection).  \n`TerminationByKubelet`\n: Pod has been terminated by the kubelet, because of either {{<glossary_tooltip term_id=\"node-pressure-eviction\" text=\"node pressure eviction\">}},\nthe [graceful node shutdown](/docs/concepts/architecture/nodes/#graceful-node-shutdown),\nor preemption for [system critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).  \nIn all other disruption scenarios, like eviction due to exceeding\n[Pod container limits](/docs/concepts/configuration/manage-resources-containers/),\nPods don't receive the `DisruptionTarget` condition because the disruptions were\nprobably caused by the Pod and would reoccur on retry.  \n{{< note >}}\nA Pod disruption might be interrupted. The control plane might re-attempt to\ncontinue the disruption of the same Pod, but it is not guaranteed. As a result,\nthe `DisruptionTarget` condition might be added to a Pod, but that Pod might then not actually be\ndeleted. In such a situation, after some time, the\nPod disruption condition will be cleared.\n{{< /note >}}  \nAlong with cleaning up the pods, the Pod garbage collector (PodGC) will also mark them as failed if they are in a non-terminal\nphase (see also [Pod garbage collection](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)).  \nWhen using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job's\n[Pod failure policy](/docs/concepts/workloads/controllers/job#pod-failure-policy).\n", "relevant_passages": ["What does the Pod Disruption Target condition indicate?"]}
{"query": "A user asked the following question:\nQuestion: What information can I get from the pod's annotations?\nThis is about the following runbook:\nRunbook Title: Information available via `fieldRef` {#downwardapi-fieldRef}\nRunbook Content: Available fieldsInformation available via `fieldRef` {#downwardapi-fieldRef}For some Pod-level fields, you can provide them to a container either as\nan environment variable or using a `downwardAPI` volume. The fields available\nvia either mechanism are:  \n`metadata.name`\n: the pod's name  \n`metadata.namespace`\n: the pod's {{< glossary_tooltip text=\"namespace\" term_id=\"namespace\" >}}  \n`metadata.uid`\n: the pod's unique ID  \n`metadata.annotations['<KEY>']`\n: the value of the pod's {{< glossary_tooltip text=\"annotation\" term_id=\"annotation\" >}} named `<KEY>` (for example, `metadata.annotations['myannotation']`)  \n`metadata.labels['<KEY>']`\n: the text value of the pod's {{< glossary_tooltip text=\"label\" term_id=\"label\" >}} named `<KEY>` (for example, `metadata.labels['mylabel']`)  \nThe following information is available through environment variables\n**but not as a downwardAPI volume fieldRef**:  \n`spec.serviceAccountName`\n: the name of the pod's {{< glossary_tooltip text=\"service account\" term_id=\"service-account\" >}}  \n`spec.nodeName`\n: the name of the {{< glossary_tooltip term_id=\"node\" text=\"node\">}} where the Pod is executing  \n`status.hostIP`\n: the primary IP address of the node to which the Pod is assigned  \n`status.hostIPs`\n: the IP addresses is a dual-stack version of `status.hostIP`, the first is always the same as `status.hostIP`.  \n`status.podIP`\n: the pod's primary IP address (usually, its IPv4 address)  \n`status.podIPs`\n: the IP addresses is a dual-stack version of `status.podIP`, the first is always the same as `status.podIP`  \nThe following information is available through a `downwardAPI` volume\n`fieldRef`, **but not as environment variables**:  \n`metadata.labels`\n: all of the pod's labels, formatted as `label-key=\"escaped-label-value\"` with one label per line  \n`metadata.annotations`\n: all of the pod's annotations, formatted as `annotation-key=\"escaped-annotation-value\"` with one annotation per line\n", "relevant_passages": ["What information can I get from the pod's annotations?"]}
{"query": "A user asked the following question:\nQuestion: What should I do if I want to connect to a container in a different Pod?\nThis is about the following runbook:\nRunbook Title: Pod networking\nRunbook Content: Resource sharing and communicationPod networkingEach Pod is assigned a unique IP address for each address family. Every\ncontainer in a Pod shares the network namespace, including the IP address and\nnetwork ports. Inside a Pod (and **only** then), the containers that belong to the Pod\ncan communicate with one another using `localhost`. When containers in a Pod communicate\nwith entities *outside the Pod*,\nthey must coordinate how they use the shared network resources (such as ports).\nWithin a Pod, containers share an IP address and port space, and\ncan find each other via `localhost`. The containers in a Pod can also communicate\nwith each other using standard inter-process communications like SystemV semaphores\nor POSIX shared memory.  Containers in different Pods have distinct IP addresses\nand can not communicate by OS-level IPC without special configuration.\nContainers that want to interact with a container running in a different Pod can\nuse IP networking to communicate.  \nContainers within the Pod see the system hostname as being the same as the configured\n`name` for the Pod. There's more about this in the [networking](/docs/concepts/cluster-administration/networking/)\nsection.\n", "relevant_passages": ["What should I do if I want to connect to a container in a different Pod?"]}
{"query": "A user asked the following question:\nQuestion: How do I provision storage for a Pod in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Limitations\nRunbook Content: Limitations* The storage for a given Pod must either be provisioned by a\n[PersistentVolume Provisioner](/docs/concepts/storage/dynamic-provisioning/) ([examples here](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md))\nbased on the requested _storage class_, or pre-provisioned by an admin.\n* Deleting and/or scaling a StatefulSet down will *not* delete the volumes associated with the\nStatefulSet. This is done to ensure data safety, which is generally more valuable than an\nautomatic purge of all related StatefulSet resources.\n* StatefulSets currently require a [Headless Service](/docs/concepts/services-networking/service/#headless-services)\nto be responsible for the network identity of the Pods. You are responsible for creating this\nService.\n* StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is\ndeleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is\npossible to scale the StatefulSet down to 0 prior to deletion.\n* When using [Rolling Updates](#rolling-updates) with the default\n[Pod Management Policy](#pod-management-policies) (`OrderedReady`),\nit's possible to get into a broken state that requires\n[manual intervention to repair](#forced-rollback).\n", "relevant_passages": ["How do I provision storage for a Pod in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I delete a ReplicationController? Can I create a new one?\nThis is about the following runbook:\nRunbook Title: Deleting only a ReplicationController\nRunbook Content: Working with ReplicationControllersDeleting only a ReplicationControllerYou can delete a ReplicationController without affecting any of its pods.  \nUsing kubectl, specify the `--cascade=orphan` option to [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete).  \nWhen using the REST API or [client library](/docs/reference/using-api/client-libraries), you can delete the ReplicationController object.  \nOnce the original is deleted, you can create a new ReplicationController to replace it.  As long\nas the old and new `.spec.selector` are the same, then the new one will adopt the old pods.\nHowever, it will not make any effort to make existing pods match a new, different pod template.\nTo update pods to a new spec in a controlled way, use a [rolling update](#rolling-updates).\n", "relevant_passages": ["What happens if I delete a ReplicationController? Can I create a new one?"]}
{"query": "A user asked the following question:\nQuestion: How do I delete a ReplicaSet without affecting its Pods?\nThis is about the following runbook:\nRunbook Title: Deleting just a ReplicaSet\nRunbook Content: Working with ReplicaSetsDeleting just a ReplicaSetYou can delete a ReplicaSet without affecting any of its Pods using\n[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete)\nwith the `--cascade=orphan` option.\nWhen using the REST API or the `client-go` library, you must set `propagationPolicy` to `Orphan`.\nFor example:  \n```shell\nkubectl proxy --port=8080\ncurl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \\\n-d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Orphan\"}' \\\n-H \"Content-Type: application/json\"\n```  \nOnce the original is deleted, you can create a new ReplicaSet to replace it. As long\nas the old and new `.spec.selector` are the same, then the new one will adopt the old Pods.\nHowever, it will not make any effort to make existing Pods match a new, different pod template.\nTo update Pods to a new spec in a controlled way, use a\n[Deployment](/docs/concepts/workloads/controllers/deployment/#creating-a-deployment), as\nReplicaSets do not support a rolling update directly.\n", "relevant_passages": ["How do I delete a ReplicaSet without affecting its Pods?"]}
{"query": "A user asked the following question:\nQuestion: What should I use instead of a ReplicationController for batch jobs?\nThis is about the following runbook:\nRunbook Title: Job\nRunbook Content: Alternatives to ReplicationControllerJobUse a [`Job`](/docs/concepts/workloads/controllers/job/) instead of a ReplicationController for pods that are expected to terminate on their own\n(that is, batch jobs).\n", "relevant_passages": ["What should I use instead of a ReplicationController for batch jobs?"]}
{"query": "A user asked the following question:\nQuestion: What's the difference between restarting a container and restarting a Pod?\nThis is about the following runbook:\nRunbook Title: Working with Pods\nRunbook Content: Working with PodsYou'll rarely create individual Pods directly in Kubernetes\u2014even singleton Pods. This\nis because Pods are designed as relatively ephemeral, disposable entities. When\na Pod gets created (directly by you, or indirectly by a\n{{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}), the new Pod is\nscheduled to run on a {{< glossary_tooltip term_id=\"node\" >}} in your cluster.\nThe Pod remains on that node until the Pod finishes execution, the Pod object is deleted,\nthe Pod is *evicted* for lack of resources, or the node fails.  \n{{< note >}}\nRestarting a container in a Pod should not be confused with restarting a Pod. A Pod\nis not a process, but an environment for running container(s). A Pod persists until\nit is deleted.\n{{< /note >}}  \nThe name of a Pod must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostname.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\n", "relevant_passages": ["What's the difference between restarting a container and restarting a Pod?"]}
{"query": "A user asked the following question:\nQuestion: How can I ensure my Pod runs as a non-root user?\nThis is about the following runbook:\nRunbook Title: Pod security settings {#pod-security}\nRunbook Content: Pod security settings {#pod-security}To set security constraints on Pods and containers, you use the\n`securityContext` field in the Pod specification. This field gives you\ngranular control over what a Pod or individual containers can do. For example:  \n* Drop specific Linux capabilities to avoid the impact of a CVE.\n* Force all processes in the Pod to run as a non-root user or as a specific\nuser or group ID.\n* Set a specific seccomp profile.\n* Set Windows security options, such as whether containers run as HostProcess.  \n{{< caution >}}\nYou can also use the Pod securityContext to enable\n[_privileged mode_](/docs/concepts/security/linux-kernel-security-constraints/#privileged-containers)\nin Linux containers. Privileged mode overrides many of the other security\nsettings in the securityContext. Avoid using this setting unless you can't grant\nthe equivalent permissions by using other fields in the securityContext.\nIn Kubernetes 1.26 and later, you can run Windows containers in a similarly\nprivileged mode by setting the `windowsOptions.hostProcess` flag on the\nsecurity context of the Pod spec. For details and instructions, see\n[Create a Windows HostProcess Pod](/docs/tasks/configure-pod-container/create-hostprocess-pod/).\n{{< /caution >}}  \n* To learn about kernel-level security constraints that you can use,\nsee [Linux kernel security constraints for Pods and containers](/docs/concepts/security/linux-kernel-security-constraints).\n* To learn more about the Pod security context, see\n[Configure a Security Context for a Pod or Container](/docs/tasks/configure-pod-container/security-context/).\n", "relevant_passages": ["How can I ensure my Pod runs as a non-root user?"]}
{"query": "A user asked the following question:\nQuestion: How can I set up a CronJob to print the current time?\nThis is about the following runbook:\nRunbook Title: Example\nRunbook Content: ExampleThis example CronJob manifest prints the current time and a hello message every minute:  \n{{% code_sample file=\"application/job/cronjob.yaml\" %}}  \n([Running Automated Tasks with a CronJob](/docs/tasks/job/automated-tasks-with-cron-jobs/)\ntakes you through this example in more detail).\n", "relevant_passages": ["How can I set up a CronJob to print the current time?"]}
{"query": "A user asked the following question:\nQuestion: What fields do I need to include when writing a ReplicaSet manifest?\nThis is about the following runbook:\nRunbook Title: Writing a ReplicaSet manifest\nRunbook Content: Writing a ReplicaSet manifestAs with all other Kubernetes API objects, a ReplicaSet needs the `apiVersion`, `kind`, and `metadata` fields.\nFor ReplicaSets, the `kind` is always a ReplicaSet.  \nWhen the control plane creates new Pods for a ReplicaSet, the `.metadata.name` of the\nReplicaSet is part of the basis for naming those Pods. The name of a ReplicaSet must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).  \nA ReplicaSet also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\n", "relevant_passages": ["What fields do I need to include when writing a ReplicaSet manifest?"]}
{"query": "A user asked the following question:\nQuestion: What are the naming constraints for a CronJob in Kubernetes?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- erictune\n- soltysh\n- janetkuo\ntitle: CronJob\napi_metadata:\n- apiVersion: \"batch/v1\"\nkind: \"CronJob\"\ncontent_type: concept\ndescription: >-\nA CronJob starts one-time Jobs on a repeating schedule.\nweight: 80\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \n{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}  \nA _CronJob_ creates {{< glossary_tooltip term_id=\"job\" text=\"Jobs\" >}} on a repeating schedule.  \nCronJob is meant for performing regular scheduled actions such as backups, report generation,\nand so on. One CronJob object is like one line of a _crontab_ (cron table) file on a\nUnix system. It runs a Job periodically on a given schedule, written in\n[Cron](https://en.wikipedia.org/wiki/Cron) format.  \nCronJobs have limitations and idiosyncrasies.\nFor example, in certain circumstances, a single CronJob can create multiple concurrent Jobs. See the [limitations](#cron-job-limitations) below.  \nWhen the control plane creates new Jobs and (indirectly) Pods for a CronJob, the `.metadata.name`\nof the CronJob is part of the basis for naming those Pods.  The name of a CronJob must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\nEven when the name is a DNS subdomain, the name must be no longer than 52\ncharacters.  This is because the CronJob controller will automatically append\n11 characters to the name you provide and there is a constraint that the\nlength of a Job name is no more than 63 characters.  \n<!-- body -->\n", "relevant_passages": ["What are the naming constraints for a CronJob in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What happens to the finalizer on a Pod after it has been accounted for in the Job status?\nThis is about the following runbook:\nRunbook Title: Job tracking with finalizers\nRunbook Content: Advanced usageJob tracking with finalizers{{< feature-state for_k8s_version=\"v1.26\" state=\"stable\" >}}  \nThe control plane keeps track of the Pods that belong to any Job and notices if\nany such Pod is removed from the API server. To do that, the Job controller\ncreates Pods with the finalizer `batch.kubernetes.io/job-tracking`. The\ncontroller removes the finalizer only after the Pod has been accounted for in\nthe Job status, allowing the Pod to be removed by other controllers or users.  \n{{< note >}}\nSee [My pod stays terminating](/docs/tasks/debug/debug-application/debug-pods/) if you\nobserve that pods from a Job are stuck with the tracking finalizer.\n{{< /note >}}\n", "relevant_passages": ["What happens to the finalizer on a Pod after it has been accounted for in the Job status?"]}
{"query": "A user asked the following question:\nQuestion: What happens to init containers in a Pod before the app containers start?\nThis is about the following runbook:\nRunbook Title: Pods with multiple containers {#how-pods-manage-multiple-containers}\nRunbook Content: Pods with multiple containers {#how-pods-manage-multiple-containers}Pods are designed to support multiple cooperating processes (as containers) that form\na cohesive unit of service. The containers in a Pod are automatically co-located and\nco-scheduled on the same physical or virtual machine in the cluster. The containers\ncan share resources and dependencies, communicate with one another, and coordinate\nwhen and how they are terminated.  \n<!--intentionally repeats some text from earlier in the page, with more detail -->\nPods in a Kubernetes cluster are used in two main ways:  \n* **Pods that run a single container**. The \"one-container-per-Pod\" model is the\nmost common Kubernetes use case; in this case, you can think of a Pod as a\nwrapper around a single container; Kubernetes manages Pods rather than managing\nthe containers directly.\n* **Pods that run multiple containers that need to work together**. A Pod can\nencapsulate an application composed of\nmultiple co-located containers that are\ntightly coupled and need to share resources. These co-located containers\nform a single cohesive unit of service\u2014for example, one container serving data\nstored in a shared volume to the public, while a separate\n{{< glossary_tooltip text=\"sidecar container\" term_id=\"sidecar-container\" >}}\nrefreshes or updates those files.\nThe Pod wraps these containers, storage resources, and an ephemeral network\nidentity together as a single unit.  \nFor example, you might have a container that\nacts as a web server for files in a shared volume, and a separate\n[sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nthat updates those files from a remote source, as in the following diagram:  \n{{< figure src=\"/images/docs/pod.svg\" alt=\"Pod creation diagram\" class=\"diagram-medium\" >}}  \nSome Pods have {{< glossary_tooltip text=\"init containers\" term_id=\"init-container\" >}}\nas well as {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}.\nBy default, init containers run and complete before the app containers are started.  \nYou can also have [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\nthat provide auxiliary services to the main application Pod (for example: a service mesh).  \n{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \nEnabled by default, the `SidecarContainers` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nallows you to specify `restartPolicy: Always` for init containers.\nSetting the `Always` restart policy ensures that the containers where you set it are\ntreated as _sidecars_ that are kept running during the entire lifetime of the Pod.\nContainers that you explicitly define as sidecar containers\nstart up before the main application Pod and remain running until the Pod is\nshut down.\n", "relevant_passages": ["What happens to init containers in a Pod before the app containers start?"]}
{"query": "A user asked the following question:\nQuestion: How can I tell if a Pod is still starting up?\nThis is about the following runbook:\nRunbook Title: Pod phase\nRunbook Content: Pod phaseA Pod's `status` field is a\n[PodStatus](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podstatus-v1-core)\nobject, which has a `phase` field.  \nThe phase of a Pod is a simple, high-level summary of where the Pod is in its\nlifecycle. The phase is not intended to be a comprehensive rollup of observations\nof container or Pod state, nor is it intended to be a comprehensive state machine.  \nThe number and meanings of Pod phase values are tightly guarded.\nOther than what is documented here, nothing should be assumed about Pods that\nhave a given `phase` value.  \nHere are the possible values for `phase`:  \nValue       | Description\n:-----------|:-----------\n`Pending`   | The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.\n`Running`   | The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.\n`Succeeded` | All containers in the Pod have terminated in success, and will not be restarted.\n`Failed`    | All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system, and is not set for automatic restarting.\n`Unknown`   | For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.  \n{{< note >}}  \nWhen a pod is failing to start repeatedly, `CrashLoopBackOff` may appear in the `Status` field of some kubectl commands. Similarly, when a pod is being deleted, `Terminating` may appear in the `Status` field of some kubectl commands.  \nMake sure not to confuse _Status_, a kubectl display field for user intuition, with the pod's `phase`.\nPod phase is an explicit part of the Kubernetes data model and of the\n[Pod API](/docs/reference/kubernetes-api/workload-resources/pod-v1/).  \n```\nNAMESPACE               NAME               READY   STATUS             RESTARTS   AGE\nalessandras-namespace   alessandras-pod    0/1     CrashLoopBackOff   200        2d9h\n```  \n---  \nA Pod is granted a term to terminate gracefully, which defaults to 30 seconds.\nYou can use the flag `--force` to [terminate a Pod by force](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced).\n{{< /note >}}  \nSince Kubernetes 1.27, the kubelet transitions deleted Pods, except for\n[static Pods](/docs/tasks/configure-pod-container/static-pod/) and\n[force-deleted Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced)\nwithout a finalizer, to a terminal phase (`Failed` or `Succeeded` depending on\nthe exit statuses of the pod containers) before their deletion from the API server.  \nIf a node dies or is disconnected from the rest of the cluster, Kubernetes\napplies a policy for setting the `phase` of all Pods on the lost node to Failed.\n", "relevant_passages": ["How can I tell if a Pod is still starting up?"]}
{"query": "A user asked the following question:\nQuestion: Can I enable communication between Pods in a Job? How?\nThis is about the following runbook:\nRunbook Title: Job patterns\nRunbook Content: Job patternsThe Job object can be used to process a set of independent but related *work items*.\nThese might be emails to be sent, frames to be rendered, files to be transcoded,\nranges of keys in a NoSQL database to scan, and so on.  \nIn a complex system, there may be multiple different sets of work items. Here we are just\nconsidering one set of work items that the user wants to manage together &mdash; a *batch job*.  \nThere are several different patterns for parallel computation, each with strengths and weaknesses.\nThe tradeoffs are:  \n- One Job object for each work item, versus a single Job object for all work items.\nOne Job per work item creates some overhead for the user and for the system to manage\nlarge numbers of Job objects.\nA single Job for all work items is better for large numbers of items.\n- Number of Pods created equals number of work items, versus each Pod can process multiple work items.\nWhen the number of Pods equals the number of work items, the Pods typically\nrequires less modification to existing code and containers. Having each Pod\nprocess multiple work items is better for large numbers of items.\n- Several approaches use a work queue. This requires running a queue service,\nand modifications to the existing program or container to make it use the work queue.\nOther approaches are easier to adapt to an existing containerised application.\n- When the Job is associated with a\n[headless Service](/docs/concepts/services-networking/service/#headless-services),\nyou can enable the Pods within a Job to communicate with each other to\ncollaborate in a computation.  \nThe tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.\nThe pattern names are also links to examples and more detailed description.  \n|                  Pattern                        | Single Job object | Fewer pods than work items? | Use app unmodified? |\n| ----------------------------------------------- |:-----------------:|:---------------------------:|:-------------------:|\n| [Queue with Pod Per Work Item]                  |         \u2713         |                             |      sometimes      |\n| [Queue with Variable Pod Count]                 |         \u2713         |             \u2713               |                     |\n| [Indexed Job with Static Work Assignment]       |         \u2713         |                             |          \u2713          |\n| [Job with Pod-to-Pod Communication]             |         \u2713         |         sometimes           |      sometimes      |\n| [Job Template Expansion]                        |                   |                             |          \u2713          |  \nWhen you specify completions with `.spec.completions`, each Pod created by the Job controller\nhas an identical [`spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\nThis means that all pods for a task will have the same command line and the same\nimage, the same volumes, and (almost) the same environment variables. These patterns\nare different ways to arrange for pods to work on different things.  \nThis table shows the required settings for `.spec.parallelism` and `.spec.completions` for each of the patterns.\nHere, `W` is the number of work items.  \n|             Pattern                             | `.spec.completions` |  `.spec.parallelism` |\n| ----------------------------------------------- |:-------------------:|:--------------------:|\n| [Queue with Pod Per Work Item]                  |          W          |        any           |\n| [Queue with Variable Pod Count]                 |         null        |        any           |\n| [Indexed Job with Static Work Assignment]       |          W          |        any           |\n| [Job with Pod-to-Pod Communication]             |          W          |         W            |\n| [Job Template Expansion]                        |          1          |     should be 1      |  \n[Queue with Pod Per Work Item]: /docs/tasks/job/coarse-parallel-processing-work-queue/\n[Queue with Variable Pod Count]: /docs/tasks/job/fine-parallel-processing-work-queue/\n[Indexed Job with Static Work Assignment]: /docs/tasks/job/indexed-parallel-processing-static/\n[Job with Pod-to-Pod Communication]: /docs/tasks/job/job-with-pod-to-pod-communication/\n[Job Template Expansion]: /docs/tasks/job/parallel-processing-expansion/\n", "relevant_passages": ["Can I enable communication between Pods in a Job? How?"]}
{"query": "A user asked the following question:\nQuestion: What does a ReplicaSet do to ensure Pods are available?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- Kashomon\n- bprashanth\n- madhusudancs\ntitle: ReplicaSet\napi_metadata:\n- apiVersion: \"apps/v1\"\nkind: \"ReplicaSet\"\nfeature:\ntitle: Self-healing\nanchor: How a ReplicaSet works\ndescription: >\nRestarts containers that fail, replaces and reschedules containers when nodes die,\nkills containers that don't respond to your user-defined health check,\nand doesn't advertise them to clients until they are ready to serve.\ncontent_type: concept\ndescription: >-\nA ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time.\nUsually, you define a Deployment and let that Deployment manage ReplicaSets automatically.\nweight: 20\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nA ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often\nused to guarantee the availability of a specified number of identical Pods.  \n<!-- body -->\n", "relevant_passages": ["What does a ReplicaSet do to ensure Pods are available?"]}
{"query": "A user asked the following question:\nQuestion: What steps do I need to follow to delete a ReplicationController using the REST API?\nThis is about the following runbook:\nRunbook Title: Deleting a ReplicationController and its Pods\nRunbook Content: Working with ReplicationControllersDeleting a ReplicationController and its PodsTo delete a ReplicationController and all its pods, use [`kubectl\ndelete`](/docs/reference/generated/kubectl/kubectl-commands#delete).  Kubectl will scale the ReplicationController to zero and wait\nfor it to delete each pod before deleting the ReplicationController itself.  If this kubectl\ncommand is interrupted, it can be restarted.  \nWhen using the REST API or [client library](/docs/reference/using-api/client-libraries), you need to do the steps explicitly (scale replicas to\n0, wait for pod deletions, then delete the ReplicationController).\n", "relevant_passages": ["What steps do I need to follow to delete a ReplicationController using the REST API?"]}
{"query": "A user asked the following question:\nQuestion: What role does QoS classification play in resource management for Pods?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: Pod Quality of Service Classes\ncontent_type: concept\nweight: 85\n---  \n<!-- overview -->  \nThis page introduces _Quality of Service (QoS) classes_ in Kubernetes, and explains\nhow Kubernetes assigns a QoS class to each Pod as a consequence of the resource\nconstraints that you specify for the containers in that Pod. Kubernetes relies on this\nclassification to make decisions about which Pods to evict when there are not enough\navailable resources on a Node.  \n<!-- body -->\n", "relevant_passages": ["What role does QoS classification play in resource management for Pods?"]}
{"query": "A user asked the following question:\nQuestion: How do I gracefully terminate a Pod in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Termination of Pods {#pod-termination}\nRunbook Content: Termination of Pods {#pod-termination}Because Pods represent processes running on nodes in the cluster, it is important to\nallow those processes to gracefully terminate when they are no longer needed (rather\nthan being abruptly stopped with a `KILL` signal and having no chance to clean up).  \nThe design aim is for you to be able to request deletion and know when processes\nterminate, but also be able to ensure that deletes eventually complete.\nWhen you request deletion of a Pod, the cluster records and tracks the intended grace period\nbefore the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in\nplace, the {{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} attempts graceful\nshutdown.  \nTypically, with this graceful termination of the pod, kubelet makes requests to the container runtime to attempt to stop the containers in the pod by first sending a TERM (aka. SIGTERM) signal, with a grace period timeout, to the main process in each container. The requests to stop the containers are processed by the container runtime asynchronously. There is no guarantee to the order of processing for these requests. Many container runtimes respect the `STOPSIGNAL` value defined in the container image and, if different, send the container image configured STOPSIGNAL instead of TERM.\nOnce the grace period has expired, the KILL signal is sent to any remaining\nprocesses, and the Pod is then deleted from the\n{{< glossary_tooltip text=\"API Server\" term_id=\"kube-apiserver\" >}}. If the kubelet or the\ncontainer runtime's management service is restarted while waiting for processes to terminate, the\ncluster retries from the start including the full original grace period.  \nPod termination flow, illustrated with an example:  \n1. You use the `kubectl` tool to manually delete a specific Pod, with the default grace period\n(30 seconds).  \n1. The Pod in the API server is updated with the time beyond which the Pod is considered \"dead\"\nalong with the grace period.\nIf you use `kubectl describe` to check the Pod you're deleting, that Pod shows up as \"Terminating\".\nOn the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked\nas terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod\nshutdown process.  \n1. If one of the Pod's containers has defined a `preStop`\n[hook](/docs/concepts/containers/container-lifecycle-hooks) and the `terminationGracePeriodSeconds`\nin the Pod spec is not set to 0, the kubelet runs that hook inside of the container.\nThe default `terminationGracePeriodSeconds` setting is 30 seconds.  \nIf the `preStop` hook is still running after the grace period expires, the kubelet requests\na small, one-off grace period extension of 2 seconds.\n{{% note %}}\nIf the `preStop` hook needs longer to complete than the default grace period allows,\nyou must modify `terminationGracePeriodSeconds` to suit this.\n{{% /note %}}  \n1. The kubelet triggers the container runtime to send a TERM signal to process 1 inside each\ncontainer.  \nThere is [special ordering](#termination-with-sidecars) if the Pod has any\n{{< glossary_tooltip text=\"sidecar containers\" term_id=\"sidecar-container\" >}} defined.\nOtherwise, the containers in the Pod receive the TERM signal at different times and in\nan arbitrary order. If the order of shutdowns matters, consider using a `preStop` hook\nto synchronize (or switch to using sidecar containers).  \n1. At the same time as the kubelet is starting graceful shutdown of the Pod, the control plane\nevaluates whether to remove that shutting-down Pod from EndpointSlice (and Endpoints) objects,\nwhere those objects represent a {{< glossary_tooltip term_id=\"service\" text=\"Service\" >}}\nwith a configured {{< glossary_tooltip text=\"selector\" term_id=\"selector\" >}}.\n{{< glossary_tooltip text=\"ReplicaSets\" term_id=\"replica-set\" >}} and other workload resources\nno longer treat the shutting-down Pod as a valid, in-service replica.  \nPods that shut down slowly should not continue to serve regular traffic and should start\nterminating and finish processing open connections.  Some applications need to go beyond\nfinishing open connections and need more graceful termination, for example, session draining\nand completion.  \nAny endpoints that represent the terminating Pods are not immediately removed from\nEndpointSlices, and a status indicating [terminating state](/docs/concepts/services-networking/endpoint-slices/#conditions)\nis exposed from the EndpointSlice API (and the legacy Endpoints API).\nTerminating endpoints always have their `ready` status as `false` (for backward compatibility\nwith versions before 1.26), so load balancers will not use it for regular traffic.  \nIf traffic draining on terminating Pod is needed, the actual readiness can be checked as a\ncondition `serving`.  You can find more details on how to implement connections draining in the\ntutorial [Pods And Endpoints Termination Flow](/docs/tutorials/services/pods-and-endpoint-termination-flow/)  \n<a id=\"pod-termination-beyond-grace-period\" />  \n1. The kubelet ensures the Pod is shut down and terminated\n1. When the grace period expires, if there is still any container running in the Pod, the\nkubelet triggers forcible shutdown.\nThe container runtime sends `SIGKILL` to any processes still running in any container in the Pod.\nThe kubelet also cleans up a hidden `pause` container if that container runtime uses one.\n1. The kubelet transitions the Pod into a terminal phase (`Failed` or `Succeeded` depending on\nthe end state of its containers).\n1. The kubelet triggers forcible removal of the Pod object from the API server, by setting grace period\nto 0 (immediate deletion).\n1. The API server deletes the Pod's API object, which is then no longer visible from any client.\n", "relevant_passages": ["How do I gracefully terminate a Pod in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: How does a StatefulSet label a Pod with its name?\nThis is about the following runbook:\nRunbook Title: Pod Name Label\nRunbook Content: Pod IdentityPod Name LabelWhen the StatefulSet {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} creates a Pod,\nit adds a label, `statefulset.kubernetes.io/pod-name`, that is set to the name of\nthe Pod. This label allows you to attach a Service to a specific Pod in\nthe StatefulSet.\n", "relevant_passages": ["How does a StatefulSet label a Pod with its name?"]}
{"query": "A user asked the following question:\nQuestion: How can I stop automated updates for my StatefulSet Pods?\nThis is about the following runbook:\nRunbook Title: Update strategies\nRunbook Content: Update strategiesA StatefulSet's `.spec.updateStrategy` field allows you to configure\nand disable automated rolling updates for containers, labels, resource request/limits, and\nannotations for the Pods in a StatefulSet. There are two possible values:  \n`OnDelete`\n: When a StatefulSet's `.spec.updateStrategy.type` is set to `OnDelete`,\nthe StatefulSet controller will not automatically update the Pods in a\nStatefulSet. Users must manually delete Pods to cause the controller to\ncreate new Pods that reflect modifications made to a StatefulSet's `.spec.template`.  \n`RollingUpdate`\n: The `RollingUpdate` update strategy implements automated, rolling updates for the Pods in a\nStatefulSet. This is the default update strategy.\n", "relevant_passages": ["How can I stop automated updates for my StatefulSet Pods?"]}
{"query": "A user asked the following question:\nQuestion: What Linux version do I need for idmap mounts to work with tmpfs?\nThis is about the following runbook:\nRunbook Title: {{% heading \"prerequisites\" %}}\nRunbook Content: {{% heading \"prerequisites\" %}}{{% thirdparty-content %}}  \nThis is a Linux-only feature and support is needed in Linux for idmap mounts on\nthe filesystems used. This means:  \n* On the node, the filesystem you use for `/var/lib/kubelet/pods/`, or the\ncustom directory you configure for this, needs idmap mount support.\n* All the filesystems used in the pod's volumes must support idmap mounts.  \nIn practice this means you need at least Linux 6.3, as tmpfs started supporting\nidmap mounts in that version. This is usually needed as several Kubernetes\nfeatures use tmpfs (the service account token that is mounted by default uses a\ntmpfs, Secrets use a tmpfs, etc.)  \nSome popular filesystems that support idmap mounts in Linux 6.3 are: btrfs,\next4, xfs, fat, tmpfs, overlayfs.  \nIn addition, the container runtime and its underlying OCI runtime must support\nuser namespaces. The following OCI runtimes offer support:  \n* [crun](https://github.com/containers/crun) version 1.9 or greater (it's recommend version 1.13+).\n* [runc](https://github.com/opencontainers/runc) version 1.2 or greater  \n{{< note >}}\nSome OCI runtimes do not include the support needed for using user namespaces in\nLinux pods. If you use a managed Kubernetes, or have downloaded it from packages\nand set it up, it's possible that nodes in your cluster use a runtime that doesn't\ninclude this support.\n{{< /note >}}  \nTo use user namespaces with Kubernetes, you also need to use a CRI\n{{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}\nto use this feature with Kubernetes pods:  \n* containerd: version 2.0 (and later) supports user namespaces for containers.\n* CRI-O: version 1.25 (and later) supports user namespaces for containers.  \nYou can see the status of user namespaces support in cri-dockerd tracked in an [issue][CRI-dockerd-issue]\non GitHub.  \n[CRI-dockerd-issue]: https://github.com/Mirantis/cri-dockerd/issues/74\n", "relevant_passages": ["What Linux version do I need for idmap mounts to work with tmpfs?"]}
{"query": "A user asked the following question:\nQuestion: How do I run the example Job that computes \u03c0?\nThis is about the following runbook:\nRunbook Title: Running an example Job\nRunbook Content: Running an example JobHere is an example Job config. It computes \u03c0 to 2000 places and prints it out.\nIt takes around 10s to complete.  \n{{% code_sample file=\"controllers/job.yaml\" %}}  \nYou can run the example with this command:  \n```shell\nkubectl apply -f https://kubernetes.io/examples/controllers/job.yaml\n```  \nThe output is similar to this:  \n```\njob.batch/pi created\n```  \nCheck on the status of the Job with `kubectl`:  \n{{< tabs name=\"Check status of Job\" >}}\n{{< tab name=\"kubectl describe job pi\" codelang=\"bash\" >}}\nName:           pi\nNamespace:      default\nSelector:       batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\nLabels:         batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\nbatch.kubernetes.io/job-name=pi\n...\nAnnotations:    batch.kubernetes.io/job-tracking: \"\"\nParallelism:    1\nCompletions:    1\nStart Time:     Mon, 02 Dec 2019 15:20:11 +0200\nCompleted At:   Mon, 02 Dec 2019 15:21:16 +0200\nDuration:       65s\nPods Statuses:  0 Running / 1 Succeeded / 0 Failed\nPod Template:\nLabels:  batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\nbatch.kubernetes.io/job-name=pi\nContainers:\npi:\nImage:      perl:5.34.0\nPort:       <none>\nHost Port:  <none>\nCommand:\nperl\n-Mbignum=bpi\n-wle\nprint bpi(2000)\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nEvents:\nType    Reason            Age   From            Message\n----    ------            ----  ----            -------\nNormal  SuccessfulCreate  21s   job-controller  Created pod: pi-xf9p4\nNormal  Completed         18s   job-controller  Job completed\n{{< /tab >}}\n{{< tab name=\"kubectl get job pi -o yaml\" codelang=\"bash\" >}}\napiVersion: batch/v1\nkind: Job\nmetadata:\nannotations: batch.kubernetes.io/job-tracking: \"\"\n...\ncreationTimestamp: \"2022-11-10T17:53:53Z\"\ngeneration: 1\nlabels:\nbatch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\nbatch.kubernetes.io/job-name: pi\nname: pi\nnamespace: default\nresourceVersion: \"4751\"\nuid: 204fb678-040b-497f-9266-35ffa8716d14\nspec:\nbackoffLimit: 4\ncompletionMode: NonIndexed\ncompletions: 1\nparallelism: 1\nselector:\nmatchLabels:\nbatch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\nsuspend: false\ntemplate:\nmetadata:\ncreationTimestamp: null\nlabels:\nbatch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\nbatch.kubernetes.io/job-name: pi\nspec:\ncontainers:\n- command:\n- perl\n- -Mbignum=bpi\n- -wle\n- print bpi(2000)\nimage: perl:5.34.0\nimagePullPolicy: IfNotPresent\nname: pi\nresources: {}\nterminationMessagePath: /dev/termination-log\nterminationMessagePolicy: File\ndnsPolicy: ClusterFirst\nrestartPolicy: Never\nschedulerName: default-scheduler\nsecurityContext: {}\nterminationGracePeriodSeconds: 30\nstatus:\nactive: 1\nready: 0\nstartTime: \"2022-11-10T17:53:57Z\"\nuncountedTerminatedPods: {}\n{{< /tab >}}\n{{< /tabs >}}  \nTo view completed Pods of a Job, use `kubectl get pods`.  \nTo list all the Pods that belong to a Job in a machine readable form, you can use a command like this:  \n```shell\npods=$(kubectl get pods --selector=batch.kubernetes.io/job-name=pi --output=jsonpath='{.items[*].metadata.name}')\necho $pods\n```  \nThe output is similar to this:  \n```\npi-5rwd7\n```  \nHere, the selector is the same as the selector for the Job. The `--output=jsonpath` option specifies an expression\nwith the name from each Pod in the returned list.  \nView the standard output of one of the pods:  \n```shell\nkubectl logs $pods\n```  \nAnother way to view the logs of a Job:  \n```shell\nkubectl logs jobs/pi\n```  \nThe output is similar to this:  \n```\n3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901\n```\n", "relevant_passages": ["How do I run the example Job that computes \u03c0?"]}
{"query": "A user asked the following question:\nQuestion: If I have a `postStart` hook, what happens when the container is `Running`?\nThis is about the following runbook:\nRunbook Title: `Running` {#container-state-running}\nRunbook Content: Container states`Running` {#container-state-running}The `Running` status indicates that a container is executing without issues. If there\nwas a `postStart` hook configured, it has already executed and finished. When you use\n`kubectl` to query a Pod with a container that is `Running`, you also see information\nabout when the container entered the `Running` state.\n", "relevant_passages": ["If I have a `postStart` hook, what happens when the container is `Running`?"]}
{"query": "A user asked the following question:\nQuestion: What happens if the main container takes too long to terminate?\nThis is about the following runbook:\nRunbook Title: Pod shutdown and sidecar containers {##termination-with-sidecars}\nRunbook Content: Termination of Pods {#pod-termination}Pod shutdown and sidecar containers {##termination-with-sidecars}If your Pod includes one or more\n[sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\n(init containers with an Always restart policy), the kubelet will delay sending\nthe TERM signal to these sidecar containers until the last main container has fully terminated.\nThe sidecar containers will be terminated in the reverse order they are defined in the Pod spec.\nThis ensures that sidecar containers continue serving the other containers in the Pod until they\nare no longer needed.  \nThis means that slow termination of a main container will also delay the termination of the sidecar containers.\nIf the grace period expires before the termination process is complete, the Pod may enter [forced termination](#pod-termination-beyond-grace-period).\nIn this case, all remaining containers in the Pod will be terminated simultaneously with a short grace period.  \nSimilarly, if the Pod has a `preStop` hook that exceeds the termination grace period, emergency termination may occur.\nIn general, if you have used `preStop` hooks to control the termination order without sidecar containers, you can now\nremove them and allow the kubelet to manage sidecar termination automatically.\n", "relevant_passages": ["What happens if the main container takes too long to terminate?"]}
{"query": "A user asked the following question:\nQuestion: What\u2019s the next step for debugging running pods?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn how to [debug pods using ephemeral containers](/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container).\n", "relevant_passages": ["What\u2019s the next step for debugging running pods?"]}
{"query": "A user asked the following question:\nQuestion: What command can I use to check the status of my ReplicationController?\nThis is about the following runbook:\nRunbook Title: Running an example ReplicationController\nRunbook Content: Running an example ReplicationControllerThis example ReplicationController config runs three copies of the nginx web server.  \n{{% code_sample file=\"controllers/replication.yaml\" %}}  \nRun the example job by downloading the example file and then running this command:  \n```shell\nkubectl apply -f https://k8s.io/examples/controllers/replication.yaml\n```  \nThe output is similar to this:  \n```\nreplicationcontroller/nginx created\n```  \nCheck on the status of the ReplicationController using this command:  \n```shell\nkubectl describe replicationcontrollers/nginx\n```  \nThe output is similar to this:  \n```\nName:        nginx\nNamespace:   default\nSelector:    app=nginx\nLabels:      app=nginx\nAnnotations:    <none>\nReplicas:    3 current / 3 desired\nPods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed\nPod Template:\nLabels:       app=nginx\nContainers:\nnginx:\nImage:              nginx\nPort:               80/TCP\nEnvironment:        <none>\nMounts:             <none>\nVolumes:              <none>\nEvents:\nFirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message\n---------       --------     -----    ----                        -------------    ----      ------              -------\n20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m\n20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0\n20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v\n```  \nHere, three pods are created, but none is running yet, perhaps because the image is being pulled.\nA little later, the same command may show:  \n```shell\nPods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed\n```  \nTo list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:  \n```shell\npods=$(kubectl get pods --selector=app=nginx --output=jsonpath={.items..metadata.name})\necho $pods\n```  \nThe output is similar to this:  \n```\nnginx-3ntk0 nginx-4ok8v nginx-qrm3m\n```  \nHere, the selector is the same as the selector for the ReplicationController (seen in the\n`kubectl describe` output), and in a different form in `replication.yaml`.  The `--output=jsonpath` option\nspecifies an expression with the name from each pod in the returned list.\n", "relevant_passages": ["What command can I use to check the status of my ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: How do I create a Deployment to rollout a ReplicaSet?\nThis is about the following runbook:\nRunbook Title: Use Case\nRunbook Content: Use CaseThe following are typical use cases for Deployments:  \n* [Create a Deployment to rollout a ReplicaSet](#creating-a-deployment). The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.\n* [Declare the new state of the Pods](#updating-a-deployment) by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.\n* [Rollback to an earlier Deployment revision](#rolling-back-a-deployment) if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.\n* [Scale up the Deployment to facilitate more load](#scaling-a-deployment).\n* [Pause the rollout of a Deployment](#pausing-and-resuming-a-deployment) to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.\n* [Use the status of the Deployment](#deployment-status) as an indicator that a rollout has stuck.\n* [Clean up older ReplicaSets](#clean-up-policy) that you don't need anymore.\n", "relevant_passages": ["How do I create a Deployment to rollout a ReplicaSet?"]}
{"query": "A user asked the following question:\nQuestion: What is the role of sidecar containers in a Pod?\nThis is about the following runbook:\nRunbook Title: Pods with multiple containers {#how-pods-manage-multiple-containers}\nRunbook Content: Pods with multiple containers {#how-pods-manage-multiple-containers}Pods are designed to support multiple cooperating processes (as containers) that form\na cohesive unit of service. The containers in a Pod are automatically co-located and\nco-scheduled on the same physical or virtual machine in the cluster. The containers\ncan share resources and dependencies, communicate with one another, and coordinate\nwhen and how they are terminated.  \n<!--intentionally repeats some text from earlier in the page, with more detail -->\nPods in a Kubernetes cluster are used in two main ways:  \n* **Pods that run a single container**. The \"one-container-per-Pod\" model is the\nmost common Kubernetes use case; in this case, you can think of a Pod as a\nwrapper around a single container; Kubernetes manages Pods rather than managing\nthe containers directly.\n* **Pods that run multiple containers that need to work together**. A Pod can\nencapsulate an application composed of\nmultiple co-located containers that are\ntightly coupled and need to share resources. These co-located containers\nform a single cohesive unit of service\u2014for example, one container serving data\nstored in a shared volume to the public, while a separate\n{{< glossary_tooltip text=\"sidecar container\" term_id=\"sidecar-container\" >}}\nrefreshes or updates those files.\nThe Pod wraps these containers, storage resources, and an ephemeral network\nidentity together as a single unit.  \nFor example, you might have a container that\nacts as a web server for files in a shared volume, and a separate\n[sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nthat updates those files from a remote source, as in the following diagram:  \n{{< figure src=\"/images/docs/pod.svg\" alt=\"Pod creation diagram\" class=\"diagram-medium\" >}}  \nSome Pods have {{< glossary_tooltip text=\"init containers\" term_id=\"init-container\" >}}\nas well as {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}.\nBy default, init containers run and complete before the app containers are started.  \nYou can also have [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\nthat provide auxiliary services to the main application Pod (for example: a service mesh).  \n{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \nEnabled by default, the `SidecarContainers` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nallows you to specify `restartPolicy: Always` for init containers.\nSetting the `Always` restart policy ensures that the containers where you set it are\ntreated as _sidecars_ that are kept running during the entire lifetime of the Pod.\nContainers that you explicitly define as sidecar containers\nstart up before the main application Pod and remain running until the Pod is\nshut down.\n", "relevant_passages": ["What is the role of sidecar containers in a Pod?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I manually scale a StatefulSet and then apply a manifest?\nThis is about the following runbook:\nRunbook Title: Replicas\nRunbook Content: PersistentVolumeClaim retentionReplicas`.spec.replicas` is an optional field that specifies the number of desired Pods. It defaults to 1.  \nShould you manually scale a deployment, example via `kubectl scale\nstatefulset statefulset --replicas=X`, and then you update that StatefulSet\nbased on a manifest (for example: by running `kubectl apply -f\nstatefulset.yaml`), then applying that manifest overwrites the manual scaling\nthat you previously did.  \nIf a [HorizontalPodAutoscaler](/docs/tasks/run-application/horizontal-pod-autoscale/)\n(or any similar API for horizontal scaling) is managing scaling for a\nStatefulset, don't set `.spec.replicas`. Instead, allow the Kubernetes\n{{<glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} to manage\nthe `.spec.replicas` field automatically.\n", "relevant_passages": ["What happens if I manually scale a StatefulSet and then apply a manifest?"]}
{"query": "A user asked the following question:\nQuestion: What happens if a Pod's containers are ready but a custom condition is missing?\nThis is about the following runbook:\nRunbook Title: Status for Pod readiness {#pod-readiness-status}\nRunbook Content: Pod conditionsStatus for Pod readiness {#pod-readiness-status}The `kubectl patch` command does not support patching object status.\nTo set these `status.conditions` for the Pod, applications and\n{{< glossary_tooltip term_id=\"operator-pattern\" text=\"operators\">}} should use\nthe `PATCH` action.\nYou can use a [Kubernetes client library](/docs/reference/using-api/client-libraries/) to\nwrite code that sets custom Pod conditions for Pod readiness.  \nFor a Pod that uses custom conditions, that Pod is evaluated to be ready **only**\nwhen both the following statements apply:  \n* All containers in the Pod are ready.\n* All conditions specified in `readinessGates` are `True`.  \nWhen a Pod's containers are Ready but at least one custom condition is missing or\n`False`, the kubelet sets the Pod's [condition](#pod-conditions) to `ContainersReady`.\n", "relevant_passages": ["What happens if a Pod's containers are ready but a custom condition is missing?"]}
{"query": "A user asked the following question:\nQuestion: How do I create a Deployment for nginx?\nThis is about the following runbook:\nRunbook Title: Creating a Deployment\nRunbook Content: Creating a DeploymentThe following is an example of a Deployment. It creates a ReplicaSet to bring up three `nginx` Pods:  \n{{% code_sample file=\"controllers/nginx-deployment.yaml\" %}}  \nIn this example:  \n* A Deployment named `nginx-deployment` is created, indicated by the\n`.metadata.name` field. This name will become the basis for the ReplicaSets\nand Pods which are created later. See [Writing a Deployment Spec](#writing-a-deployment-spec)\nfor more details.\n* The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the `.spec.replicas` field.\n* The `.spec.selector` field defines how the created ReplicaSet finds which Pods to manage.\nIn this case, you select a label that is defined in the Pod template (`app: nginx`).\nHowever, more sophisticated selection rules are possible,\nas long as the Pod template itself satisfies the rule.  \n{{< note >}}\nThe `.spec.selector.matchLabels` field is a map of {key,value} pairs.\nA single {key,value} in the `matchLabels` map is equivalent to an element of `matchExpressions`,\nwhose `key` field is \"key\", the `operator` is \"In\", and the `values` array contains only \"value\".\nAll of the requirements, from both `matchLabels` and `matchExpressions`, must be satisfied in order to match.\n{{< /note >}}  \n* The `template` field contains the following sub-fields:\n* The Pods are labeled `app: nginx`using the `.metadata.labels` field.\n* The Pod template's specification, or `.template.spec` field, indicates that\nthe Pods run one container, `nginx`, which runs the `nginx`\n[Docker Hub](https://hub.docker.com/) image at version 1.14.2.\n* Create one container and name it `nginx` using the `.spec.template.spec.containers[0].name` field.  \nBefore you begin, make sure your Kubernetes cluster is up and running.\nFollow the steps given below to create the above Deployment:  \n1. Create the Deployment by running the following command:  \n```shell\nkubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml\n```  \n2. Run `kubectl get deployments` to check if the Deployment was created.  \nIf the Deployment is still being created, the output is similar to the following:\n```\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   0/3     0            0           1s\n```\nWhen you inspect the Deployments in your cluster, the following fields are displayed:\n* `NAME` lists the names of the Deployments in the namespace.\n* `READY` displays how many replicas of the application are available to your users. It follows the pattern ready/desired.\n* `UP-TO-DATE` displays the number of replicas that have been updated to achieve the desired state.\n* `AVAILABLE` displays how many replicas of the application are available to your users.\n* `AGE` displays the amount of time that the application has been running.  \nNotice how the number of desired replicas is 3 according to `.spec.replicas` field.  \n3. To see the Deployment rollout status, run `kubectl rollout status deployment/nginx-deployment`.  \nThe output is similar to:\n```\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\ndeployment \"nginx-deployment\" successfully rolled out\n```  \n4. Run the `kubectl get deployments` again a few seconds later.\nThe output is similar to this:\n```\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           18s\n```\nNotice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.  \n5. To see the ReplicaSet (`rs`) created by the Deployment, run `kubectl get rs`. The output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY   AGE\nnginx-deployment-75675f5897   3         3         3       18s\n```\nReplicaSet output shows the following fields:  \n* `NAME` lists the names of the ReplicaSets in the namespace.\n* `DESIRED` displays the desired number of _replicas_ of the application, which you define when you create the Deployment. This is the _desired state_.\n* `CURRENT` displays how many replicas are currently running.\n* `READY` displays how many replicas of the application are available to your users.\n* `AGE` displays the amount of time that the application has been running.  \nNotice that the name of the ReplicaSet is always formatted as\n`[DEPLOYMENT-NAME]-[HASH]`. This name will become the basis for the Pods\nwhich are created.  \nThe `HASH` string is the same as the `pod-template-hash` label on the ReplicaSet.  \n6. To see the labels automatically generated for each Pod, run `kubectl get pods --show-labels`.\nThe output is similar to:\n```\nNAME                                READY     STATUS    RESTARTS   AGE       LABELS\nnginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\nnginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\nnginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\n```\nThe created ReplicaSet ensures that there are three `nginx` Pods.  \n{{< note >}}\nYou must specify an appropriate selector and Pod template labels in a Deployment\n(in this case, `app: nginx`).  \nDo not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.\n{{< /note >}}\n", "relevant_passages": ["How do I create a Deployment for nginx?"]}
{"query": "A user asked the following question:\nQuestion: Is there anything else I need to add to the Job spec besides the basic fields?\nThis is about the following runbook:\nRunbook Title: Writing a Job spec\nRunbook Content: Writing a Job specAs with all other Kubernetes config, a Job needs `apiVersion`, `kind`, and `metadata` fields.  \nWhen the control plane creates new Pods for a Job, the `.metadata.name` of the\nJob is part of the basis for naming those Pods. The name of a Job must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\nEven when the name is a DNS subdomain, the name must be no longer than 63\ncharacters.  \nA Job also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\n", "relevant_passages": ["Is there anything else I need to add to the Job spec besides the basic fields?"]}
{"query": "A user asked the following question:\nQuestion: How do I set the number of pods to run concurrently in a ReplicationController?\nThis is about the following runbook:\nRunbook Title: Multiple Replicas\nRunbook Content: Writing a ReplicationController ManifestMultiple ReplicasYou can specify how many pods should run concurrently by setting `.spec.replicas` to the number\nof pods you would like to have running concurrently.  The number running at any time may be higher\nor lower, such as if the replicas were just increased or decreased, or if a pod is gracefully\nshutdown, and a replacement starts early.  \nIf you do not specify `.spec.replicas`, then it defaults to 1.\n", "relevant_passages": ["How do I set the number of pods to run concurrently in a ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: What does it mean if a container is in the Waiting state?\nThis is about the following runbook:\nRunbook Title: `Waiting` {#container-state-waiting}\nRunbook Content: Container states`Waiting` {#container-state-waiting}If a container is not in either the `Running` or `Terminated` state, it is `Waiting`.\nA container in the `Waiting` state is still running the operations it requires in\norder to complete start up: for example, pulling the container image from a container\nimage registry, or applying {{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}\ndata.\nWhen you use `kubectl` to query a Pod with a container that is `Waiting`, you also see\na Reason field to summarize why the container is in that state.\n", "relevant_passages": ["What does it mean if a container is in the Waiting state?"]}
{"query": "A user asked the following question:\nQuestion: What command do I use to check the status of the Job after running it?\nThis is about the following runbook:\nRunbook Title: Running an example Job\nRunbook Content: Running an example JobHere is an example Job config. It computes \u03c0 to 2000 places and prints it out.\nIt takes around 10s to complete.  \n{{% code_sample file=\"controllers/job.yaml\" %}}  \nYou can run the example with this command:  \n```shell\nkubectl apply -f https://kubernetes.io/examples/controllers/job.yaml\n```  \nThe output is similar to this:  \n```\njob.batch/pi created\n```  \nCheck on the status of the Job with `kubectl`:  \n{{< tabs name=\"Check status of Job\" >}}\n{{< tab name=\"kubectl describe job pi\" codelang=\"bash\" >}}\nName:           pi\nNamespace:      default\nSelector:       batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\nLabels:         batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\nbatch.kubernetes.io/job-name=pi\n...\nAnnotations:    batch.kubernetes.io/job-tracking: \"\"\nParallelism:    1\nCompletions:    1\nStart Time:     Mon, 02 Dec 2019 15:20:11 +0200\nCompleted At:   Mon, 02 Dec 2019 15:21:16 +0200\nDuration:       65s\nPods Statuses:  0 Running / 1 Succeeded / 0 Failed\nPod Template:\nLabels:  batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\nbatch.kubernetes.io/job-name=pi\nContainers:\npi:\nImage:      perl:5.34.0\nPort:       <none>\nHost Port:  <none>\nCommand:\nperl\n-Mbignum=bpi\n-wle\nprint bpi(2000)\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nEvents:\nType    Reason            Age   From            Message\n----    ------            ----  ----            -------\nNormal  SuccessfulCreate  21s   job-controller  Created pod: pi-xf9p4\nNormal  Completed         18s   job-controller  Job completed\n{{< /tab >}}\n{{< tab name=\"kubectl get job pi -o yaml\" codelang=\"bash\" >}}\napiVersion: batch/v1\nkind: Job\nmetadata:\nannotations: batch.kubernetes.io/job-tracking: \"\"\n...\ncreationTimestamp: \"2022-11-10T17:53:53Z\"\ngeneration: 1\nlabels:\nbatch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\nbatch.kubernetes.io/job-name: pi\nname: pi\nnamespace: default\nresourceVersion: \"4751\"\nuid: 204fb678-040b-497f-9266-35ffa8716d14\nspec:\nbackoffLimit: 4\ncompletionMode: NonIndexed\ncompletions: 1\nparallelism: 1\nselector:\nmatchLabels:\nbatch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\nsuspend: false\ntemplate:\nmetadata:\ncreationTimestamp: null\nlabels:\nbatch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\nbatch.kubernetes.io/job-name: pi\nspec:\ncontainers:\n- command:\n- perl\n- -Mbignum=bpi\n- -wle\n- print bpi(2000)\nimage: perl:5.34.0\nimagePullPolicy: IfNotPresent\nname: pi\nresources: {}\nterminationMessagePath: /dev/termination-log\nterminationMessagePolicy: File\ndnsPolicy: ClusterFirst\nrestartPolicy: Never\nschedulerName: default-scheduler\nsecurityContext: {}\nterminationGracePeriodSeconds: 30\nstatus:\nactive: 1\nready: 0\nstartTime: \"2022-11-10T17:53:57Z\"\nuncountedTerminatedPods: {}\n{{< /tab >}}\n{{< /tabs >}}  \nTo view completed Pods of a Job, use `kubectl get pods`.  \nTo list all the Pods that belong to a Job in a machine readable form, you can use a command like this:  \n```shell\npods=$(kubectl get pods --selector=batch.kubernetes.io/job-name=pi --output=jsonpath='{.items[*].metadata.name}')\necho $pods\n```  \nThe output is similar to this:  \n```\npi-5rwd7\n```  \nHere, the selector is the same as the selector for the Job. The `--output=jsonpath` option specifies an expression\nwith the name from each Pod in the returned list.  \nView the standard output of one of the pods:  \n```shell\nkubectl logs $pods\n```  \nAnother way to view the logs of a Job:  \n```shell\nkubectl logs jobs/pi\n```  \nThe output is similar to this:  \n```\n3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901\n```\n", "relevant_passages": ["What command do I use to check the status of the Job after running it?"]}
{"query": "A user asked the following question:\nQuestion: Can sidecar containers and main containers communicate with each other?\nThis is about the following runbook:\nRunbook Title: Differences from init containers\nRunbook Content: Differences from init containersSidecar containers work alongside the main container, extending its functionality and\nproviding additional services.  \nSidecar containers run concurrently with the main application container. They are active\nthroughout the lifecycle of the pod and can be started and stopped independently of the\nmain container. Unlike [init containers](/docs/concepts/workloads/pods/init-containers/),\nsidecar containers support [probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe) to control their lifecycle.  \nSidecar containers can interact directly with the main application containers, because\nlike init containers they always share the same network, and can optionally also share\nvolumes (filesystems).  \nInit containers stop before the main containers start up, so init containers cannot\nexchange messages with the app container in a Pod. Any data passing is one-way\n(for example, an init container can put information inside an `emptyDir` volume).\n", "relevant_passages": ["Can sidecar containers and main containers communicate with each other?"]}
{"query": "A user asked the following question:\nQuestion: How can I specify my own Pod selector when creating a Job in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Specifying your own Pod selector\nRunbook Content: Advanced usageSpecifying your own Pod selectorNormally, when you create a Job object, you do not specify `.spec.selector`.\nThe system defaulting logic adds this field when the Job is created.\nIt picks a selector value that will not overlap with any other jobs.  \nHowever, in some cases, you might need to override this automatically set selector.\nTo do this, you can specify the `.spec.selector` of the Job.  \nBe very careful when doing this. If you specify a label selector which is not\nunique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelated\njob may be deleted, or this Job may count other Pods as completing it, or one or both\nJobs may refuse to create Pods or run to completion. If a non-unique selector is\nchosen, then other controllers (e.g. ReplicationController) and their Pods may behave\nin unpredictable ways too. Kubernetes will not stop you from making a mistake when\nspecifying `.spec.selector`.  \nHere is an example of a case when you might want to use this feature.  \nSay Job `old` is already running. You want existing Pods\nto keep running, but you want the rest of the Pods it creates\nto use a different pod template and for the Job to have a new name.\nYou cannot update the Job because these fields are not updatable.\nTherefore, you delete Job `old` but _leave its pods\nrunning_, using `kubectl delete jobs/old --cascade=orphan`.\nBefore deleting it, you make a note of what selector it uses:  \n```shell\nkubectl get job old -o yaml\n```  \nThe output is similar to this:  \n```yaml\nkind: Job\nmetadata:\nname: old\n...\nspec:\nselector:\nmatchLabels:\nbatch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002\n...\n```  \nThen you create a new Job with name `new` and you explicitly specify the same selector.\nSince the existing Pods have label `batch.kubernetes.io/controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002`,\nthey are controlled by Job `new` as well.  \nYou need to specify `manualSelector: true` in the new Job since you are not using\nthe selector that the system normally generates for you automatically.  \n```yaml\nkind: Job\nmetadata:\nname: new\n...\nspec:\nmanualSelector: true\nselector:\nmatchLabels:\nbatch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002\n...\n```  \nThe new Job itself will have a different uid from `a8f3d00d-c6d2-11e5-9f87-42010af00002`. Setting\n`manualSelector: true` tells the system that you know what you are doing and to allow this\nmismatch.\n", "relevant_passages": ["How can I specify my own Pod selector when creating a Job in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: Should I use a Job or a Bare Pod for my application?\nThis is about the following runbook:\nRunbook Title: Bare Pods\nRunbook Content: AlternativesBare PodsWhen the node that a Pod is running on reboots or fails, the pod is terminated\nand will not be restarted. However, a Job will create new Pods to replace terminated ones.\nFor this reason, we recommend that you use a Job rather than a bare Pod, even if your application\nrequires only a single Pod.\n", "relevant_passages": ["Should I use a Job or a Bare Pod for my application?"]}
{"query": "A user asked the following question:\nQuestion: Can I modify the Pods created by a DaemonSet?\nThis is about the following runbook:\nRunbook Title: Updating a DaemonSet\nRunbook Content: Updating a DaemonSetIf node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete\nPods from newly not-matching nodes.  \nYou can modify the Pods that a DaemonSet creates.  However, Pods do not allow all\nfields to be updated.  Also, the DaemonSet controller will use the original template the next\ntime a node (even with the same name) is created.  \nYou can delete a DaemonSet.  If you specify `--cascade=orphan` with `kubectl`, then the Pods\nwill be left on the nodes.  If you subsequently create a new DaemonSet with the same selector,\nthe new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces\nthem according to its `updateStrategy`.  \nYou can [perform a rolling update](/docs/tasks/manage-daemon/update-daemon-set/) on a DaemonSet.\n", "relevant_passages": ["Can I modify the Pods created by a DaemonSet?"]}
{"query": "A user asked the following question:\nQuestion: How do I set the selector for my Deployment?\nThis is about the following runbook:\nRunbook Title: Selector\nRunbook Content: Writing a Deployment SpecSelector`.spec.selector` is a required field that specifies a [label selector](/docs/concepts/overview/working-with-objects/labels/)\nfor the Pods targeted by this Deployment.  \n`.spec.selector` must match `.spec.template.metadata.labels`, or it will be rejected by the API.  \nIn API version `apps/v1`, `.spec.selector` and `.metadata.labels` do not default to `.spec.template.metadata.labels` if not set. So they must be set explicitly. Also note that `.spec.selector` is immutable after creation of the Deployment in `apps/v1`.  \nA Deployment may terminate Pods whose labels match the selector if their template is different\nfrom `.spec.template` or if the total number of such Pods exceeds `.spec.replicas`. It brings up new\nPods with `.spec.template` if the number of Pods is less than the desired number.  \n{{< note >}}\nYou should not create other Pods whose labels match this selector, either directly, by creating\nanother Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you\ndo so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.\n{{< /note >}}  \nIf you have multiple controllers that have overlapping selectors, the controllers will fight with each\nother and won't behave correctly.\n", "relevant_passages": ["How do I set the selector for my Deployment?"]}
{"query": "A user asked the following question:\nQuestion: What actions can I use for container probes in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Container probes\nRunbook Content: Container probesA _probe_ is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet can invoke different actions:  \n- `ExecAction` (performed with the help of the container runtime)\n- `TCPSocketAction` (checked directly by the kubelet)\n- `HTTPGetAction` (checked directly by the kubelet)  \nYou can read more about [probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)\nin the Pod Lifecycle documentation.\n", "relevant_passages": ["What actions can I use for container probes in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: Can I use ephemeral containers to build applications?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- verb\n- yujuhong\ntitle: Ephemeral Containers\ncontent_type: concept\nweight: 60\n---  \n<!-- overview -->  \n{{< feature-state state=\"stable\" for_k8s_version=\"v1.25\" >}}  \nThis page provides an overview of ephemeral containers: a special type of container\nthat runs temporarily in an existing {{< glossary_tooltip term_id=\"pod\" >}} to\naccomplish user-initiated actions such as troubleshooting. You use ephemeral\ncontainers to inspect services rather than to build applications.  \n<!-- body -->\n", "relevant_passages": ["Can I use ephemeral containers to build applications?"]}
{"query": "A user asked the following question:\nQuestion: What is the downward API in Kubernetes?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: Downward API\ncontent_type: concept\nweight: 170\ndescription: >\nThere are two ways to expose Pod and container fields to a running container:\nenvironment variables, and as files that are populated by a special volume type.\nTogether, these two ways of exposing Pod and container fields are called the downward API.\n---  \n<!-- overview -->  \nIt is sometimes useful for a container to have information about itself, without\nbeing overly coupled to Kubernetes. The _downward API_ allows containers to consume\ninformation about themselves or the cluster without using the Kubernetes client\nor API server.  \nAn example is an existing application that assumes a particular well-known\nenvironment variable holds a unique identifier. One possibility is to wrap the\napplication, but that is tedious and error-prone, and it violates the goal of low\ncoupling. A better option would be to use the Pod's name as an identifier, and\ninject the Pod's name into the well-known environment variable.  \nIn Kubernetes, there are two ways to expose Pod and container fields to a running container:  \n* as [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)\n* as [files in a `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)  \nTogether, these two ways of exposing Pod and container fields are called the\n_downward API_.  \n<!-- body -->\n", "relevant_passages": ["What is the downward API in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What happens to Pods that are in a non-terminal phase during garbage collection?\nThis is about the following runbook:\nRunbook Title: Garbage collection of Pods {#pod-garbage-collection}\nRunbook Content: Termination of Pods {#pod-termination}Garbage collection of Pods {#pod-garbage-collection}For failed Pods, the API objects remain in the cluster's API until a human or\n{{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}} process\nexplicitly removes them.  \nThe Pod garbage collector (PodGC), which is a controller in the control plane, cleans up\nterminated Pods (with a phase of `Succeeded` or `Failed`), when the number of Pods exceeds the\nconfigured threshold (determined by `terminated-pod-gc-threshold` in the kube-controller-manager).\nThis avoids a resource leak as Pods are created and terminated over time.  \nAdditionally, PodGC cleans up any Pods which satisfy any of the following conditions:  \n1. are orphan Pods - bound to a node which no longer exists,\n1. are unscheduled terminating Pods,\n1. are terminating Pods, bound to a non-ready node tainted with\n[`node.kubernetes.io/out-of-service`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-out-of-service),\nwhen the `NodeOutOfServiceVolumeDetach` feature gate is enabled.  \nAlong with cleaning up the Pods, PodGC will also mark them as failed if they are in a non-terminal\nphase. Also, PodGC adds a Pod disruption condition when cleaning up an orphan Pod.\nSee [Pod disruption conditions](/docs/concepts/workloads/pods/disruptions#pod-disruption-conditions)\nfor more details.\n", "relevant_passages": ["What happens to Pods that are in a non-terminal phase during garbage collection?"]}
{"query": "A user asked the following question:\nQuestion: Can I use set-based selectors with ReplicationController?\nThis is about the following runbook:\nRunbook Title: ReplicationController\nRunbook Content: Alternatives to ReplicaSetReplicationControllerReplicaSets are the successors to [ReplicationControllers](/docs/concepts/workloads/controllers/replicationcontroller/).\nThe two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-based\nselector requirements as described in the [labels user guide](/docs/concepts/overview/working-with-objects/labels/#label-selectors).\nAs such, ReplicaSets are preferred over ReplicationControllers\n", "relevant_passages": ["Can I use set-based selectors with ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: Can you show me an example of deleting a ReplicaSet with curl?\nThis is about the following runbook:\nRunbook Title: Deleting a ReplicaSet and its Pods\nRunbook Content: Working with ReplicaSetsDeleting a ReplicaSet and its PodsTo delete a ReplicaSet and all of its Pods, use\n[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete). The\n[Garbage collector](/docs/concepts/architecture/garbage-collection/) automatically deletes all of\nthe dependent Pods by default.  \nWhen using the REST API or the `client-go` library, you must set `propagationPolicy` to\n`Background` or `Foreground` in the `-d` option. For example:  \n```shell\nkubectl proxy --port=8080\ncurl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \\\n-d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}' \\\n-H \"Content-Type: application/json\"\n```\n", "relevant_passages": ["Can you show me an example of deleting a ReplicaSet with curl?"]}
{"query": "A user asked the following question:\nQuestion: How do Pods manage multiple containers that need to work together?\nThis is about the following runbook:\nRunbook Title: Pods with multiple containers {#how-pods-manage-multiple-containers}\nRunbook Content: Pods with multiple containers {#how-pods-manage-multiple-containers}Pods are designed to support multiple cooperating processes (as containers) that form\na cohesive unit of service. The containers in a Pod are automatically co-located and\nco-scheduled on the same physical or virtual machine in the cluster. The containers\ncan share resources and dependencies, communicate with one another, and coordinate\nwhen and how they are terminated.  \n<!--intentionally repeats some text from earlier in the page, with more detail -->\nPods in a Kubernetes cluster are used in two main ways:  \n* **Pods that run a single container**. The \"one-container-per-Pod\" model is the\nmost common Kubernetes use case; in this case, you can think of a Pod as a\nwrapper around a single container; Kubernetes manages Pods rather than managing\nthe containers directly.\n* **Pods that run multiple containers that need to work together**. A Pod can\nencapsulate an application composed of\nmultiple co-located containers that are\ntightly coupled and need to share resources. These co-located containers\nform a single cohesive unit of service\u2014for example, one container serving data\nstored in a shared volume to the public, while a separate\n{{< glossary_tooltip text=\"sidecar container\" term_id=\"sidecar-container\" >}}\nrefreshes or updates those files.\nThe Pod wraps these containers, storage resources, and an ephemeral network\nidentity together as a single unit.  \nFor example, you might have a container that\nacts as a web server for files in a shared volume, and a separate\n[sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nthat updates those files from a remote source, as in the following diagram:  \n{{< figure src=\"/images/docs/pod.svg\" alt=\"Pod creation diagram\" class=\"diagram-medium\" >}}  \nSome Pods have {{< glossary_tooltip text=\"init containers\" term_id=\"init-container\" >}}\nas well as {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}.\nBy default, init containers run and complete before the app containers are started.  \nYou can also have [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\nthat provide auxiliary services to the main application Pod (for example: a service mesh).  \n{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \nEnabled by default, the `SidecarContainers` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nallows you to specify `restartPolicy: Always` for init containers.\nSetting the `Always` restart policy ensures that the containers where you set it are\ntreated as _sidecars_ that are kept running during the entire lifetime of the Pod.\nContainers that you explicitly define as sidecar containers\nstart up before the main application Pod and remain running until the Pod is\nshut down.\n", "relevant_passages": ["How do Pods manage multiple containers that need to work together?"]}
{"query": "A user asked the following question:\nQuestion: What are some common use cases for using a CronJob?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- erictune\n- soltysh\n- janetkuo\ntitle: CronJob\napi_metadata:\n- apiVersion: \"batch/v1\"\nkind: \"CronJob\"\ncontent_type: concept\ndescription: >-\nA CronJob starts one-time Jobs on a repeating schedule.\nweight: 80\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \n{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}  \nA _CronJob_ creates {{< glossary_tooltip term_id=\"job\" text=\"Jobs\" >}} on a repeating schedule.  \nCronJob is meant for performing regular scheduled actions such as backups, report generation,\nand so on. One CronJob object is like one line of a _crontab_ (cron table) file on a\nUnix system. It runs a Job periodically on a given schedule, written in\n[Cron](https://en.wikipedia.org/wiki/Cron) format.  \nCronJobs have limitations and idiosyncrasies.\nFor example, in certain circumstances, a single CronJob can create multiple concurrent Jobs. See the [limitations](#cron-job-limitations) below.  \nWhen the control plane creates new Jobs and (indirectly) Pods for a CronJob, the `.metadata.name`\nof the CronJob is part of the basis for naming those Pods.  The name of a CronJob must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\nEven when the name is a DNS subdomain, the name must be no longer than 52\ncharacters.  This is because the CronJob controller will automatically append\n11 characters to the name you provide and there is a constraint that the\nlength of a Job name is no more than 63 characters.  \n<!-- body -->\n", "relevant_passages": ["What are some common use cases for using a CronJob?"]}
{"query": "A user asked the following question:\nQuestion: How do I create a Pod using a Pod template?\nThis is about the following runbook:\nRunbook Title: Pod templates\nRunbook Content: Working with PodsPod templatesControllers for {{< glossary_tooltip text=\"workload\" term_id=\"workload\" >}} resources create Pods\nfrom a _pod template_ and manage those Pods on your behalf.  \nPodTemplates are specifications for creating Pods, and are included in workload resources such as\n[Deployments](/docs/concepts/workloads/controllers/deployment/),\n[Jobs](/docs/concepts/workloads/controllers/job/), and\n[DaemonSets](/docs/concepts/workloads/controllers/daemonset/).  \nEach controller for a workload resource uses the `PodTemplate` inside the workload\nobject to make actual Pods. The `PodTemplate` is part of the desired state of whatever\nworkload resource you used to run your app.  \nWhen you create a Pod, you can include\n[environment variables](/docs/tasks/inject-data-application/define-environment-variable-container/)\nin the Pod template for the containers that run in the Pod.  \nThe sample below is a manifest for a simple Job with a `template` that starts one\ncontainer. The container in that Pod prints a message then pauses.  \n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: hello\nspec:\ntemplate:\n# This is the pod template\nspec:\ncontainers:\n- name: hello\nimage: busybox:1.28\ncommand: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600']\nrestartPolicy: OnFailure\n# The pod template ends here\n```  \nModifying the pod template or switching to a new pod template has no direct effect\non the Pods that already exist. If you change the pod template for a workload\nresource, that resource needs to create replacement Pods that use the updated template.  \nFor example, the StatefulSet controller ensures that the running Pods match the current\npod template for each StatefulSet object. If you edit the StatefulSet to change its pod\ntemplate, the StatefulSet starts to create new Pods based on the updated template.\nEventually, all of the old Pods are replaced with new Pods, and the update is complete.  \nEach workload resource implements its own rules for handling changes to the Pod template.\nIf you want to read more about StatefulSet specifically, read\n[Update strategy](/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets) in the StatefulSet Basics tutorial.  \nOn Nodes, the {{< glossary_tooltip term_id=\"kubelet\" text=\"kubelet\" >}} does not\ndirectly observe or manage any of the details around pod templates and updates; those\ndetails are abstracted away. That abstraction and separation of concerns simplifies\nsystem semantics, and makes it feasible to extend the cluster's behavior without\nchanging existing code.\n", "relevant_passages": ["How do I create a Pod using a Pod template?"]}
{"query": "A user asked the following question:\nQuestion: How can I check if my Deployment has failed to progress?\nThis is about the following runbook:\nRunbook Title: Failed Deployment\nRunbook Content: Deployment statusFailed DeploymentYour Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur\ndue to some of the following factors:  \n* Insufficient quota\n* Readiness probe failures\n* Image pull errors\n* Insufficient permissions\n* Limit ranges\n* Application runtime misconfiguration  \nOne way you can detect this condition is to specify a deadline parameter in your Deployment spec:\n([`.spec.progressDeadlineSeconds`](#progress-deadline-seconds)). `.spec.progressDeadlineSeconds` denotes the\nnumber of seconds the Deployment controller waits before indicating (in the Deployment status) that the\nDeployment progress has stalled.  \nThe following `kubectl` command sets the spec with `progressDeadlineSeconds` to make the controller report\nlack of progress of a rollout for a Deployment after 10 minutes:  \n```shell\nkubectl patch deployment/nginx-deployment -p '{\"spec\":{\"progressDeadlineSeconds\":600}}'\n```\nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment patched\n```\nOnce the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following\nattributes to the Deployment's `.status.conditions`:  \n* `type: Progressing`\n* `status: \"False\"`\n* `reason: ProgressDeadlineExceeded`  \nThis condition can also fail early and is then set to status value of `\"False\"` due to reasons as `ReplicaSetCreateError`.\nAlso, the deadline is not taken into account anymore once the Deployment rollout completes.  \nSee the [Kubernetes API conventions](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties) for more information on status conditions.  \n{{< note >}}\nKubernetes takes no action on a stalled Deployment other than to report a status condition with\n`reason: ProgressDeadlineExceeded`. Higher level orchestrators can take advantage of it and act accordingly, for\nexample, rollback the Deployment to its previous version.\n{{< /note >}}  \n{{< note >}}\nIf you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline.\nYou can safely pause a Deployment rollout in the middle of a rollout and resume without triggering\nthe condition for exceeding the deadline.\n{{< /note >}}  \nYou may experience transient errors with your Deployments, either due to a low timeout that you have set or\ndue to any other kind of error that can be treated as transient. For example, let's suppose you have\ninsufficient quota. If you describe the Deployment you will notice the following section:  \n```shell\nkubectl describe deployment nginx-deployment\n```\nThe output is similar to this:\n```\n<...>\nConditions:\nType            Status  Reason\n----            ------  ------\nAvailable       True    MinimumReplicasAvailable\nProgressing     True    ReplicaSetUpdated\nReplicaFailure  True    FailedCreate\n<...>\n```  \nIf you run `kubectl get deployment nginx-deployment -o yaml`, the Deployment status is similar to this:  \n```\nstatus:\navailableReplicas: 2\nconditions:\n- lastTransitionTime: 2016-10-04T12:25:39Z\nlastUpdateTime: 2016-10-04T12:25:39Z\nmessage: Replica set \"nginx-deployment-4262182780\" is progressing.\nreason: ReplicaSetUpdated\nstatus: \"True\"\ntype: Progressing\n- lastTransitionTime: 2016-10-04T12:25:42Z\nlastUpdateTime: 2016-10-04T12:25:42Z\nmessage: Deployment has minimum availability.\nreason: MinimumReplicasAvailable\nstatus: \"True\"\ntype: Available\n- lastTransitionTime: 2016-10-04T12:25:39Z\nlastUpdateTime: 2016-10-04T12:25:39Z\nmessage: 'Error creating: pods \"nginx-deployment-4262182780-\" is forbidden: exceeded quota:\nobject-counts, requested: pods=1, used: pods=3, limited: pods=2'\nreason: FailedCreate\nstatus: \"True\"\ntype: ReplicaFailure\nobservedGeneration: 3\nreplicas: 2\nunavailableReplicas: 2\n```  \nEventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the\nreason for the Progressing condition:  \n```\nConditions:\nType            Status  Reason\n----            ------  ------\nAvailable       True    MinimumReplicasAvailable\nProgressing     False   ProgressDeadlineExceeded\nReplicaFailure  True    FailedCreate\n```  \nYou can address an issue of insufficient quota by scaling down your Deployment, by scaling down other\ncontrollers you may be running, or by increasing quota in your namespace. If you satisfy the quota\nconditions and the Deployment controller then completes the Deployment rollout, you'll see the\nDeployment's status update with a successful condition (`status: \"True\"` and `reason: NewReplicaSetAvailable`).  \n```\nConditions:\nType          Status  Reason\n----          ------  ------\nAvailable     True    MinimumReplicasAvailable\nProgressing   True    NewReplicaSetAvailable\n```  \n`type: Available` with `status: \"True\"` means that your Deployment has minimum availability. Minimum availability is dictated\nby the parameters specified in the deployment strategy. `type: Progressing` with `status: \"True\"` means that your Deployment\nis either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum\nrequired new replicas are available (see the Reason of the condition for the particulars - in our case\n`reason: NewReplicaSetAvailable` means that the Deployment is complete).  \nYou can check if a Deployment has failed to progress by using `kubectl rollout status`. `kubectl rollout status`\nreturns a non-zero exit code if the Deployment has exceeded the progression deadline.  \n```shell\nkubectl rollout status deployment/nginx-deployment\n```\nThe output is similar to this:\n```\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nerror: deployment \"nginx\" exceeded its progress deadline\n```\nand the exit status from `kubectl rollout` is 1 (indicating an error):\n```shell\necho $?\n```\n```\n1\n```\n", "relevant_passages": ["How can I check if my Deployment has failed to progress?"]}
{"query": "A user asked the following question:\nQuestion: What happens if a Container exceeds its resource limit in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Some behavior is independent of QoS class {#class-independent-behavior}\nRunbook Content: Some behavior is independent of QoS class {#class-independent-behavior}Certain behavior is independent of the QoS class assigned by Kubernetes. For example:  \n* Any Container exceeding a resource limit will be killed and restarted by the kubelet without\naffecting other Containers in that Pod.  \n* If a Container exceeds its resource request and the node it runs on faces\nresource pressure, the Pod it is in becomes a candidate for [eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\nIf this occurs, all Containers in the Pod will be terminated. Kubernetes may create a\nreplacement Pod, usually on a different node.  \n* The resource request of a Pod is equal to the sum of the resource requests of\nits component Containers, and the resource limit of a Pod is equal to the sum of\nthe resource limits of its component Containers.  \n* The kube-scheduler does not consider QoS class when selecting which Pods to\n[preempt](/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption).\nPreemption can occur when a cluster does not have enough resources to run all the Pods\nyou defined.\n", "relevant_passages": ["What happens if a Container exceeds its resource limit in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What happens if the backoff limit per index is exceeded?\nThis is about the following runbook:\nRunbook Title: Backoff limit per index {#backoff-limit-per-index}\nRunbook Content: Handling Pod and container failuresBackoff limit per index {#backoff-limit-per-index}{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \n{{< note >}}\nYou can only configure the backoff limit per index for an [Indexed](#completion-mode) Job, if you\nhave the `JobBackoffLimitPerIndex` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nenabled in your cluster.\n{{< /note >}}  \nWhen you run an [indexed](#completion-mode) Job, you can choose to handle retries\nfor pod failures independently for each index. To do so, set the\n`.spec.backoffLimitPerIndex` to specify the maximal number of pod failures\nper index.  \nWhen the per-index backoff limit is exceeded for an index, Kubernetes considers the index as failed and adds it to the\n`.status.failedIndexes` field. The succeeded indexes, those with a successfully\nexecuted pods, are recorded in the `.status.completedIndexes` field, regardless of whether you set\nthe `backoffLimitPerIndex` field.  \nNote that a failing index does not interrupt execution of other indexes.\nOnce all indexes finish for a Job where you specified a backoff limit per index,\nif at least one of those indexes did fail, the Job controller marks the overall\nJob as failed, by setting the Failed condition in the status. The Job gets\nmarked as failed even if some, potentially nearly all, of the indexes were\nprocessed successfully.  \nYou can additionally limit the maximal number of indexes marked failed by\nsetting the `.spec.maxFailedIndexes` field.\nWhen the number of failed indexes exceeds the `maxFailedIndexes` field, the\nJob controller triggers termination of all remaining running Pods for that Job.\nOnce all pods are terminated, the entire Job is marked failed by the Job\ncontroller, by setting the Failed condition in the Job status.  \nHere is an example manifest for a Job that defines a `backoffLimitPerIndex`:  \n{{< code_sample file=\"/controllers/job-backoff-limit-per-index-example.yaml\" >}}  \nIn the example above, the Job controller allows for one restart for each\nof the indexes. When the total number of failed indexes exceeds 5, then\nthe entire Job is terminated.  \nOnce the job is finished, the Job status looks as follows:  \n```sh\nkubectl get -o yaml job job-backoff-limit-per-index-example\n```  \n```yaml\nstatus:\ncompletedIndexes: 1,3,5,7,9\nfailedIndexes: 0,2,4,6,8\nsucceeded: 5          # 1 succeeded pod for each of 5 succeeded indexes\nfailed: 10            # 2 failed pods (1 retry) for each of 5 failed indexes\nconditions:\n- message: Job has failed indexes\nreason: FailedIndexes\nstatus: \"True\"\ntype: FailureTarget\n- message: Job has failed indexes\nreason: FailedIndexes\nstatus: \"True\"\ntype: Failed\n```  \nThe Job controller adds the `FailureTarget` Job condition to trigger\n[Job termination and cleanup](#job-termination-and-cleanup). When all of the\nJob Pods are terminated, the Job controller adds the `Failed` condition\nwith the same values for `reason` and `message` as the `FailureTarget` Job\ncondition. For details, see [Termination of Job Pods](#termination-of-job-pods).  \nAdditionally, you may want to use the per-index backoff along with a\n[pod failure policy](#pod-failure-policy). When using\nper-index backoff, there is a new `FailIndex` action available which allows you to\navoid unnecessary retries within an index.\n", "relevant_passages": ["What happens if the backoff limit per index is exceeded?"]}
{"query": "A user asked the following question:\nQuestion: How do Guaranteed Pods handle resource limits?\nThis is about the following runbook:\nRunbook Title: Guaranteed\nRunbook Content: Quality of Service classesGuaranteedPods that are `Guaranteed` have the strictest resource limits and are least likely\nto face eviction. They are guaranteed not to be killed until they exceed their limits\nor there are no lower-priority Pods that can be preempted from the Node. They may\nnot acquire resources beyond their specified limits. These Pods can also make\nuse of exclusive CPUs using the\n[`static`](/docs/tasks/administer-cluster/cpu-management-policies/#static-policy) CPU management policy.  \n#### Criteria  \nFor a Pod to be given a QoS class of `Guaranteed`:  \n* Every Container in the Pod must have a memory limit and a memory request.\n* For every Container in the Pod, the memory limit must equal the memory request.\n* Every Container in the Pod must have a CPU limit and a CPU request.\n* For every Container in the Pod, the CPU limit must equal the CPU request.\n", "relevant_passages": ["How do Guaranteed Pods handle resource limits?"]}
{"query": "A user asked the following question:\nQuestion: Can a volume be reused if the Pod it was associated with is deleted?\nThis is about the following runbook:\nRunbook Title: Associated lifetimes\nRunbook Content: Pod lifetimeAssociated lifetimesWhen something is said to have the same lifetime as a Pod, such as a\n{{< glossary_tooltip term_id=\"volume\" text=\"volume\" >}},\nthat means that the thing exists as long as that specific Pod (with that exact UID)\nexists. If that Pod is deleted for any reason, and even if an identical replacement\nis created, the related thing (a volume, in this example) is also destroyed and\ncreated anew.  \n{{< figure src=\"/images/docs/pod.svg\" title=\"Figure 1.\" class=\"diagram-medium\" caption=\"A multi-container Pod that contains a file puller [sidecar](/docs/concepts/workloads/pods/sidecar-containers/) and a web server. The Pod uses an [ephemeral `emptyDir` volume](/docs/concepts/storage/volumes/#emptydir) for shared storage between the containers.\" >}}\n", "relevant_passages": ["Can a volume be reused if the Pod it was associated with is deleted?"]}
{"query": "A user asked the following question:\nQuestion: Is it common to manage ReplicaSets directly?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- Kashomon\n- bprashanth\n- madhusudancs\ntitle: ReplicaSet\napi_metadata:\n- apiVersion: \"apps/v1\"\nkind: \"ReplicaSet\"\nfeature:\ntitle: Self-healing\nanchor: How a ReplicaSet works\ndescription: >\nRestarts containers that fail, replaces and reschedules containers when nodes die,\nkills containers that don't respond to your user-defined health check,\nand doesn't advertise them to clients until they are ready to serve.\ncontent_type: concept\ndescription: >-\nA ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time.\nUsually, you define a Deployment and let that Deployment manage ReplicaSets automatically.\nweight: 20\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nA ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often\nused to guarantee the availability of a specified number of identical Pods.  \n<!-- body -->\n", "relevant_passages": ["Is it common to manage ReplicaSets directly?"]}
{"query": "A user asked the following question:\nQuestion: What do I need to ensure about the external controller when using the managedBy field?\nThis is about the following runbook:\nRunbook Title: Delegation of managing a Job object to external controller\nRunbook Content: Advanced usageDelegation of managing a Job object to external controller{{< feature-state feature_gate_name=\"JobManagedBy\" >}}  \n{{< note >}}\nYou can only set the `managedBy` field on Jobs if you enable the `JobManagedBy`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n(disabled by default).\n{{< /note >}}  \nThis feature allows you to disable the built-in Job controller, for a specific\nJob, and delegate reconciliation of the Job to an external controller.  \nYou indicate the controller that reconciles the Job by setting a custom value\nfor the `spec.managedBy` field - any value\nother than `kubernetes.io/job-controller`. The value of the field is immutable.  \n{{< note >}}\nWhen using this feature, make sure the controller indicated by the field is\ninstalled, otherwise the Job may not be reconciled at all.\n{{< /note >}}  \n{{< note >}}\nWhen developing an external Job controller be aware that your controller needs\nto operate in a fashion conformant with the definitions of the API spec and\nstatus fields of the Job object.  \nPlease review these in detail in the [Job API](/docs/reference/kubernetes-api/workload-resources/job-v1/).\nWe also recommend that you run the e2e conformance tests for the Job object to\nverify your implementation.  \nFinally, when developing an external Job controller make sure it does not use the\n`batch.kubernetes.io/job-tracking` finalizer, reserved for the built-in controller.\n{{< /note >}}  \n{{< warning >}}\nIf you are considering to disable the `JobManagedBy` feature gate, or to\ndowngrade the cluster to a version without the feature gate enabled, check if\nthere are jobs with a custom value of the `spec.managedBy` field. If there\nare such jobs, there is a risk that they might be reconciled by two controllers\nafter the operation: the built-in Job controller and the external controller\nindicated by the field value.\n{{< /warning >}}\n", "relevant_passages": ["What do I need to ensure about the external controller when using the managedBy field?"]}
{"query": "A user asked the following question:\nQuestion: What's the main difference in lifecycle between sidecar containers and init containers?\nThis is about the following runbook:\nRunbook Title: Differences from init containers\nRunbook Content: Differences from init containersSidecar containers work alongside the main container, extending its functionality and\nproviding additional services.  \nSidecar containers run concurrently with the main application container. They are active\nthroughout the lifecycle of the pod and can be started and stopped independently of the\nmain container. Unlike [init containers](/docs/concepts/workloads/pods/init-containers/),\nsidecar containers support [probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe) to control their lifecycle.  \nSidecar containers can interact directly with the main application containers, because\nlike init containers they always share the same network, and can optionally also share\nvolumes (filesystems).  \nInit containers stop before the main containers start up, so init containers cannot\nexchange messages with the app container in a Pod. Any data passing is one-way\n(for example, an init container can put information inside an `emptyDir` volume).\n", "relevant_passages": ["What's the main difference in lifecycle between sidecar containers and init containers?"]}
{"query": "A user asked the following question:\nQuestion: What are the tradeoffs when creating Pods for work items in a Job?\nThis is about the following runbook:\nRunbook Title: Job patterns\nRunbook Content: Job patternsThe Job object can be used to process a set of independent but related *work items*.\nThese might be emails to be sent, frames to be rendered, files to be transcoded,\nranges of keys in a NoSQL database to scan, and so on.  \nIn a complex system, there may be multiple different sets of work items. Here we are just\nconsidering one set of work items that the user wants to manage together &mdash; a *batch job*.  \nThere are several different patterns for parallel computation, each with strengths and weaknesses.\nThe tradeoffs are:  \n- One Job object for each work item, versus a single Job object for all work items.\nOne Job per work item creates some overhead for the user and for the system to manage\nlarge numbers of Job objects.\nA single Job for all work items is better for large numbers of items.\n- Number of Pods created equals number of work items, versus each Pod can process multiple work items.\nWhen the number of Pods equals the number of work items, the Pods typically\nrequires less modification to existing code and containers. Having each Pod\nprocess multiple work items is better for large numbers of items.\n- Several approaches use a work queue. This requires running a queue service,\nand modifications to the existing program or container to make it use the work queue.\nOther approaches are easier to adapt to an existing containerised application.\n- When the Job is associated with a\n[headless Service](/docs/concepts/services-networking/service/#headless-services),\nyou can enable the Pods within a Job to communicate with each other to\ncollaborate in a computation.  \nThe tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.\nThe pattern names are also links to examples and more detailed description.  \n|                  Pattern                        | Single Job object | Fewer pods than work items? | Use app unmodified? |\n| ----------------------------------------------- |:-----------------:|:---------------------------:|:-------------------:|\n| [Queue with Pod Per Work Item]                  |         \u2713         |                             |      sometimes      |\n| [Queue with Variable Pod Count]                 |         \u2713         |             \u2713               |                     |\n| [Indexed Job with Static Work Assignment]       |         \u2713         |                             |          \u2713          |\n| [Job with Pod-to-Pod Communication]             |         \u2713         |         sometimes           |      sometimes      |\n| [Job Template Expansion]                        |                   |                             |          \u2713          |  \nWhen you specify completions with `.spec.completions`, each Pod created by the Job controller\nhas an identical [`spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\nThis means that all pods for a task will have the same command line and the same\nimage, the same volumes, and (almost) the same environment variables. These patterns\nare different ways to arrange for pods to work on different things.  \nThis table shows the required settings for `.spec.parallelism` and `.spec.completions` for each of the patterns.\nHere, `W` is the number of work items.  \n|             Pattern                             | `.spec.completions` |  `.spec.parallelism` |\n| ----------------------------------------------- |:-------------------:|:--------------------:|\n| [Queue with Pod Per Work Item]                  |          W          |        any           |\n| [Queue with Variable Pod Count]                 |         null        |        any           |\n| [Indexed Job with Static Work Assignment]       |          W          |        any           |\n| [Job with Pod-to-Pod Communication]             |          W          |         W            |\n| [Job Template Expansion]                        |          1          |     should be 1      |  \n[Queue with Pod Per Work Item]: /docs/tasks/job/coarse-parallel-processing-work-queue/\n[Queue with Variable Pod Count]: /docs/tasks/job/fine-parallel-processing-work-queue/\n[Indexed Job with Static Work Assignment]: /docs/tasks/job/indexed-parallel-processing-static/\n[Job with Pod-to-Pod Communication]: /docs/tasks/job/job-with-pod-to-pod-communication/\n[Job Template Expansion]: /docs/tasks/job/parallel-processing-expansion/\n", "relevant_passages": ["What are the tradeoffs when creating Pods for work items in a Job?"]}
{"query": "A user asked the following question:\nQuestion: Which fields are not checked for Pods using user namespaces?\nThis is about the following runbook:\nRunbook Title: Integration with Pod security admission checks\nRunbook Content: Integration with Pod security admission checks{{< feature-state state=\"alpha\" for_k8s_version=\"v1.29\" >}}  \nFor Linux Pods that enable user namespaces, Kubernetes relaxes the application of\n[Pod Security Standards](/docs/concepts/security/pod-security-standards) in a controlled way.\nThis behavior can be controlled by the [feature\ngate](/docs/reference/command-line-tools-reference/feature-gates/)\n`UserNamespacesPodSecurityStandards`, which allows an early opt-in for end\nusers. Admins have to ensure that user namespaces are enabled by all nodes\nwithin the cluster if using the feature gate.  \nIf you enable the associated feature gate and create a Pod that uses user\nnamespaces, the following fields won't be constrained even in contexts that enforce the\n_Baseline_ or _Restricted_ pod security standard. This behavior does not\npresent a security concern because `root` inside a Pod with user namespaces\nactually refers to the user inside the container, that is never mapped to a\nprivileged user on the host. Here's the list of fields that are **not** checks for Pods in those\ncircumstances:  \n- `spec.securityContext.runAsNonRoot`\n- `spec.containers[*].securityContext.runAsNonRoot`\n- `spec.initContainers[*].securityContext.runAsNonRoot`\n- `spec.ephemeralContainers[*].securityContext.runAsNonRoot`\n- `spec.securityContext.runAsUser`\n- `spec.containers[*].securityContext.runAsUser`\n- `spec.initContainers[*].securityContext.runAsUser`\n- `spec.ephemeralContainers[*].securityContext.runAsUser`\n", "relevant_passages": ["Which fields are not checked for Pods using user namespaces?"]}
{"query": "A user asked the following question:\nQuestion: When should I consider using a partition for rolling updates?\nThis is about the following runbook:\nRunbook Title: Partitioned rolling updates {#partitions}\nRunbook Content: Rolling UpdatesPartitioned rolling updates {#partitions}The `RollingUpdate` update strategy can be partitioned, by specifying a\n`.spec.updateStrategy.rollingUpdate.partition`. If a partition is specified, all Pods with an\nordinal that is greater than or equal to the partition will be updated when the StatefulSet's\n`.spec.template` is updated. All Pods with an ordinal that is less than the partition will not\nbe updated, and, even if they are deleted, they will be recreated at the previous version. If a\nStatefulSet's `.spec.updateStrategy.rollingUpdate.partition` is greater than its `.spec.replicas`,\nupdates to its `.spec.template` will not be propagated to its Pods.\nIn most cases you will not need to use a partition, but they are useful if you want to stage an\nupdate, roll out a canary, or perform a phased roll out.\n", "relevant_passages": ["When should I consider using a partition for rolling updates?"]}
{"query": "A user asked the following question:\nQuestion: Can I specify a different scheduler for my DaemonSet Pods?\nThis is about the following runbook:\nRunbook Title: How Daemon Pods are scheduled\nRunbook Content: How Daemon Pods are scheduledA DaemonSet can be used to ensure that all eligible nodes run a copy of a Pod.\nThe DaemonSet controller creates a Pod for each eligible node and adds the\n`spec.affinity.nodeAffinity` field of the Pod to match the target host. After\nthe Pod is created, the default scheduler typically takes over and then binds\nthe Pod to the target host by setting the `.spec.nodeName` field.  If the new\nPod cannot fit on the node, the default scheduler may preempt (evict) some of\nthe existing Pods based on the\n[priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority)\nof the new Pod.  \n{{< note >}}\nIf it's important that the DaemonSet pod run on each node, it's often desirable\nto set the `.spec.template.spec.priorityClassName` of the DaemonSet to a\n[PriorityClass](/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass)\nwith a higher priority to ensure that this eviction occurs.\n{{< /note >}}  \nThe user can specify a different scheduler for the Pods of the DaemonSet, by\nsetting the `.spec.template.spec.schedulerName` field of the DaemonSet.  \nThe original node affinity specified at the\n`.spec.template.spec.affinity.nodeAffinity` field (if specified) is taken into\nconsideration by the DaemonSet controller when evaluating the eligible nodes,\nbut is replaced on the created Pod with the node affinity that matches the name\nof the eligible node.  \n```yaml\nnodeAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\nnodeSelectorTerms:\n- matchFields:\n- key: metadata.name\noperator: In\nvalues:\n- target-host-name\n```\n", "relevant_passages": ["Can I specify a different scheduler for my DaemonSet Pods?"]}
{"query": "A user asked the following question:\nQuestion: How can I check if my container is healthy using a command?\nThis is about the following runbook:\nRunbook Title: Check mechanisms {#probe-check-methods}\nRunbook Content: Container probesCheck mechanisms {#probe-check-methods}There are four different ways to check a container using a probe.\nEach probe must define exactly one of these four mechanisms:  \n`exec`\n: Executes a specified command inside the container. The diagnostic\nis considered successful if the command exits with a status code of 0.  \n`grpc`\n: Performs a remote procedure call using [gRPC](https://grpc.io/).\nThe target should implement\n[gRPC health checks](https://grpc.io/grpc/core/md_doc_health-checking.html).\nThe diagnostic is considered successful if the `status`\nof the response is `SERVING`.  \n`httpGet`\n: Performs an HTTP `GET` request against the Pod's IP\naddress on a specified port and path. The diagnostic is\nconsidered successful if the response has a status code\ngreater than or equal to 200 and less than 400.  \n`tcpSocket`\n: Performs a TCP check against the Pod's IP address on\na specified port. The diagnostic is considered successful if\nthe port is open. If the remote system (the container) closes\nthe connection immediately after it opens, this counts as healthy.  \n{{< caution >}} Unlike the other mechanisms, `exec` probe's implementation involves the creation/forking of multiple processes each time when executed.\nAs a result, in case of the clusters having higher pod densities, lower intervals of `initialDelaySeconds`, `periodSeconds`, configuring any probe with exec mechanism might introduce an overhead on the cpu usage of the node.\nIn such scenarios, consider using the alternative probe mechanisms to avoid the overhead.{{< /caution >}}\n", "relevant_passages": ["How can I check if my container is healthy using a command?"]}
{"query": "A user asked the following question:\nQuestion: What do I need to do if I want to create a new Job with the same selector as an existing Job?\nThis is about the following runbook:\nRunbook Title: Specifying your own Pod selector\nRunbook Content: Advanced usageSpecifying your own Pod selectorNormally, when you create a Job object, you do not specify `.spec.selector`.\nThe system defaulting logic adds this field when the Job is created.\nIt picks a selector value that will not overlap with any other jobs.  \nHowever, in some cases, you might need to override this automatically set selector.\nTo do this, you can specify the `.spec.selector` of the Job.  \nBe very careful when doing this. If you specify a label selector which is not\nunique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelated\njob may be deleted, or this Job may count other Pods as completing it, or one or both\nJobs may refuse to create Pods or run to completion. If a non-unique selector is\nchosen, then other controllers (e.g. ReplicationController) and their Pods may behave\nin unpredictable ways too. Kubernetes will not stop you from making a mistake when\nspecifying `.spec.selector`.  \nHere is an example of a case when you might want to use this feature.  \nSay Job `old` is already running. You want existing Pods\nto keep running, but you want the rest of the Pods it creates\nto use a different pod template and for the Job to have a new name.\nYou cannot update the Job because these fields are not updatable.\nTherefore, you delete Job `old` but _leave its pods\nrunning_, using `kubectl delete jobs/old --cascade=orphan`.\nBefore deleting it, you make a note of what selector it uses:  \n```shell\nkubectl get job old -o yaml\n```  \nThe output is similar to this:  \n```yaml\nkind: Job\nmetadata:\nname: old\n...\nspec:\nselector:\nmatchLabels:\nbatch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002\n...\n```  \nThen you create a new Job with name `new` and you explicitly specify the same selector.\nSince the existing Pods have label `batch.kubernetes.io/controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002`,\nthey are controlled by Job `new` as well.  \nYou need to specify `manualSelector: true` in the new Job since you are not using\nthe selector that the system normally generates for you automatically.  \n```yaml\nkind: Job\nmetadata:\nname: new\n...\nspec:\nmanualSelector: true\nselector:\nmatchLabels:\nbatch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002\n...\n```  \nThe new Job itself will have a different uid from `a8f3d00d-c6d2-11e5-9f87-42010af00002`. Setting\n`manualSelector: true` tells the system that you know what you are doing and to allow this\nmismatch.\n", "relevant_passages": ["What do I need to do if I want to create a new Job with the same selector as an existing Job?"]}
{"query": "A user asked the following question:\nQuestion: What should I set the `ttlSecondsAfterFinished` field to for immediate deletion of a Job?\nThis is about the following runbook:\nRunbook Title: TTL mechanism for finished Jobs\nRunbook Content: Clean up finished jobs automaticallyTTL mechanism for finished Jobs{{< feature-state for_k8s_version=\"v1.23\" state=\"stable\" >}}  \nAnother way to clean up finished Jobs (either `Complete` or `Failed`)\nautomatically is to use a TTL mechanism provided by a\n[TTL controller](/docs/concepts/workloads/controllers/ttlafterfinished/) for\nfinished resources, by specifying the `.spec.ttlSecondsAfterFinished` field of\nthe Job.  \nWhen the TTL controller cleans up the Job, it will delete the Job cascadingly,\ni.e. delete its dependent objects, such as Pods, together with the Job. Note\nthat when the Job is deleted, its lifecycle guarantees, such as finalizers, will\nbe honored.  \nFor example:  \n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: pi-with-ttl\nspec:\nttlSecondsAfterFinished: 100\ntemplate:\nspec:\ncontainers:\n- name: pi\nimage: perl:5.34.0\ncommand: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\nrestartPolicy: Never\n```  \nThe Job `pi-with-ttl` will be eligible to be automatically deleted, `100`\nseconds after it finishes.  \nIf the field is set to `0`, the Job will be eligible to be automatically deleted\nimmediately after it finishes. If the field is unset, this Job won't be cleaned\nup by the TTL controller after it finishes.  \n{{< note >}}\nIt is recommended to set `ttlSecondsAfterFinished` field because unmanaged jobs\n(Jobs that you created directly, and not indirectly through other workload APIs\nsuch as CronJob) have a default deletion\npolicy of `orphanDependents` causing Pods created by an unmanaged Job to be left around\nafter that Job is fully deleted.\nEven though the {{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} eventually\n[garbage collects](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)\nthe Pods from a deleted Job after they either fail or complete, sometimes those\nlingering pods may cause cluster performance degradation or in worst case cause the\ncluster to go offline due to this degradation.  \nYou can use [LimitRanges](/docs/concepts/policy/limit-range/) and\n[ResourceQuotas](/docs/concepts/policy/resource-quotas/) to place a\ncap on the amount of resources that a particular namespace can\nconsume.\n{{< /note >}}\n", "relevant_passages": ["What should I set the `ttlSecondsAfterFinished` field to for immediate deletion of a Job?"]}
{"query": "A user asked the following question:\nQuestion: Can I add my own tolerations to DaemonSet Pods? How?\nThis is about the following runbook:\nRunbook Title: Taints and tolerations\nRunbook Content: How Daemon Pods are scheduledTaints and tolerationsThe DaemonSet controller automatically adds a set of {{< glossary_tooltip\ntext=\"tolerations\" term_id=\"toleration\" >}} to DaemonSet Pods:  \n{{< table caption=\"Tolerations for DaemonSet pods\" >}}  \n| Toleration key                                                                                                        | Effect       | Details                                                                                                                                       |\n| --------------------------------------------------------------------------------------------------------------------- | ------------ | --------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`node.kubernetes.io/not-ready`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-not-ready)             | `NoExecute`  | DaemonSet Pods can be scheduled onto nodes that are not healthy or ready to accept Pods. Any DaemonSet Pods running on such nodes will not be evicted. |\n| [`node.kubernetes.io/unreachable`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-unreachable)         | `NoExecute`  | DaemonSet Pods can be scheduled onto nodes that are unreachable from the node controller. Any DaemonSet Pods running on such nodes will not be evicted. |\n| [`node.kubernetes.io/disk-pressure`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-disk-pressure)     | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with disk pressure issues.                                                                         |\n| [`node.kubernetes.io/memory-pressure`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-memory-pressure) | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with memory pressure issues.                                                                        |\n| [`node.kubernetes.io/pid-pressure`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-pid-pressure) | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with process pressure issues.                                                                        |\n| [`node.kubernetes.io/unschedulable`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-unschedulable)   | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes that are unschedulable.                                                                            |\n| [`node.kubernetes.io/network-unavailable`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-network-unavailable) | `NoSchedule` | **Only added for DaemonSet Pods that request host networking**, i.e., Pods having `spec.hostNetwork: true`. Such DaemonSet Pods can be scheduled onto nodes with unavailable network.|  \n{{< /table >}}  \nYou can add your own tolerations to the Pods of a DaemonSet as well, by\ndefining these in the Pod template of the DaemonSet.  \nBecause the DaemonSet controller sets the\n`node.kubernetes.io/unschedulable:NoSchedule` toleration automatically,\nKubernetes can run DaemonSet Pods on nodes that are marked as _unschedulable_.  \nIf you use a DaemonSet to provide an important node-level function, such as\n[cluster networking](/docs/concepts/cluster-administration/networking/), it is\nhelpful that Kubernetes places DaemonSet Pods on nodes before they are ready.\nFor example, without that special toleration, you could end up in a deadlock\nsituation where the node is not marked as ready because the network plugin is\nnot running there, and at the same time the network plugin is not running on\nthat node because the node is not yet ready.\n", "relevant_passages": ["Can I add my own tolerations to DaemonSet Pods? How?"]}
{"query": "A user asked the following question:\nQuestion: What is RuntimeClass and how do I use it for Pods?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about the [lifecycle of a Pod](/docs/concepts/workloads/pods/pod-lifecycle/).\n* Learn about [RuntimeClass](/docs/concepts/containers/runtime-class/) and how you can use it to\nconfigure different Pods with different container runtime configurations.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how you can use it to manage application availability during disruptions.\n* Pod is a top-level resource in the Kubernetes REST API.\nThe {{< api-reference page=\"workload-resources/pod-v1\" >}}\nobject definition describes the object in detail.\n* [The Distributed System Toolkit: Patterns for Composite Containers](/blog/2015/06/the-distributed-system-toolkit-patterns/) explains common layouts for Pods with more than one container.\n* Read about [Pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/)  \nTo understand the context for why Kubernetes wraps a common Pod API in other resources (such as {{< glossary_tooltip text=\"StatefulSets\" term_id=\"statefulset\" >}} or {{< glossary_tooltip text=\"Deployments\" term_id=\"deployment\" >}}), you can read about the prior art, including:  \n* [Aurora](https://aurora.apache.org/documentation/latest/reference/configuration/#job-schema)\n* [Borg](https://research.google.com/pubs/pub43438.html)\n* [Marathon](https://github.com/d2iq-archive/marathon)\n* [Omega](https://research.google/pubs/pub41684/)\n* [Tupperware](https://engineering.fb.com/data-center-engineering/tupperware/).\n", "relevant_passages": ["What is RuntimeClass and how do I use it for Pods?"]}
{"query": "A user asked the following question:\nQuestion: How do I assign memory resources to my containers and pods?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [resource management for Pods and Containers](/docs/concepts/configuration/manage-resources-containers/).\n* Learn about [Node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\n* Learn about [Pod priority and preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).\n* Learn about [Pod disruptions](/docs/concepts/workloads/pods/disruptions/).\n* Learn how to [assign memory resources to containers and pods](/docs/tasks/configure-pod-container/assign-memory-resource/).\n* Learn how to [assign CPU resources to containers and pods](/docs/tasks/configure-pod-container/assign-cpu-resource/).\n* Learn how to [configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/).\n", "relevant_passages": ["How do I assign memory resources to my containers and pods?"]}
{"query": "A user asked the following question:\nQuestion: How does Kubernetes handle a Pod that is unhealthy due to node failure?\nThis is about the following runbook:\nRunbook Title: Pods and fault recovery {#pod-fault-recovery}\nRunbook Content: Pod lifetimePods and fault recovery {#pod-fault-recovery}If one of the containers in the Pod fails, then Kubernetes may try to restart that\nspecific container.\nRead [How Pods handle problems with containers](#container-restarts) to learn more.  \nPods can however fail in a way that the cluster cannot recover from, and in that case\nKubernetes does not attempt to heal the Pod further; instead, Kubernetes deletes the\nPod and relies on other components to provide automatic healing.  \nIf a Pod is scheduled to a {{< glossary_tooltip text=\"node\" term_id=\"node\" >}} and that\nnode then fails, the Pod is treated as unhealthy and Kubernetes eventually deletes the Pod.\nA Pod won't survive an {{< glossary_tooltip text=\"eviction\" term_id=\"eviction\" >}} due to\na lack of resources or Node maintenance.  \nKubernetes uses a higher-level abstraction, called a\n{{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}}, that handles the work of\nmanaging the relatively disposable Pod instances.  \nA given Pod (as defined by a UID) is never \"rescheduled\" to a different node; instead,\nthat Pod can be replaced by a new, near-identical Pod. If you make a replacement Pod, it can\neven have same name (as in `.metadata.name`) that the old Pod had, but the replacement\nwould have a different `.metadata.uid` from the old Pod.  \nKubernetes does not guarantee that a replacement for an existing Pod would be scheduled to\nthe same node as the old Pod that was being replaced.\n", "relevant_passages": ["How does Kubernetes handle a Pod that is unhealthy due to node failure?"]}
{"query": "A user asked the following question:\nQuestion: What tolerations do DaemonSet Pods automatically get for nodes that are not ready?\nThis is about the following runbook:\nRunbook Title: Taints and tolerations\nRunbook Content: How Daemon Pods are scheduledTaints and tolerationsThe DaemonSet controller automatically adds a set of {{< glossary_tooltip\ntext=\"tolerations\" term_id=\"toleration\" >}} to DaemonSet Pods:  \n{{< table caption=\"Tolerations for DaemonSet pods\" >}}  \n| Toleration key                                                                                                        | Effect       | Details                                                                                                                                       |\n| --------------------------------------------------------------------------------------------------------------------- | ------------ | --------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`node.kubernetes.io/not-ready`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-not-ready)             | `NoExecute`  | DaemonSet Pods can be scheduled onto nodes that are not healthy or ready to accept Pods. Any DaemonSet Pods running on such nodes will not be evicted. |\n| [`node.kubernetes.io/unreachable`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-unreachable)         | `NoExecute`  | DaemonSet Pods can be scheduled onto nodes that are unreachable from the node controller. Any DaemonSet Pods running on such nodes will not be evicted. |\n| [`node.kubernetes.io/disk-pressure`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-disk-pressure)     | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with disk pressure issues.                                                                         |\n| [`node.kubernetes.io/memory-pressure`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-memory-pressure) | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with memory pressure issues.                                                                        |\n| [`node.kubernetes.io/pid-pressure`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-pid-pressure) | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with process pressure issues.                                                                        |\n| [`node.kubernetes.io/unschedulable`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-unschedulable)   | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes that are unschedulable.                                                                            |\n| [`node.kubernetes.io/network-unavailable`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-network-unavailable) | `NoSchedule` | **Only added for DaemonSet Pods that request host networking**, i.e., Pods having `spec.hostNetwork: true`. Such DaemonSet Pods can be scheduled onto nodes with unavailable network.|  \n{{< /table >}}  \nYou can add your own tolerations to the Pods of a DaemonSet as well, by\ndefining these in the Pod template of the DaemonSet.  \nBecause the DaemonSet controller sets the\n`node.kubernetes.io/unschedulable:NoSchedule` toleration automatically,\nKubernetes can run DaemonSet Pods on nodes that are marked as _unschedulable_.  \nIf you use a DaemonSet to provide an important node-level function, such as\n[cluster networking](/docs/concepts/cluster-administration/networking/), it is\nhelpful that Kubernetes places DaemonSet Pods on nodes before they are ready.\nFor example, without that special toleration, you could end up in a deadlock\nsituation where the node is not marked as ready because the network plugin is\nnot running there, and at the same time the network plugin is not running on\nthat node because the node is not yet ready.\n", "relevant_passages": ["What tolerations do DaemonSet Pods automatically get for nodes that are not ready?"]}
{"query": "A user asked the following question:\nQuestion: How should I name my ReplicaSet to avoid issues with Pod hostnames?\nThis is about the following runbook:\nRunbook Title: Writing a ReplicaSet manifest\nRunbook Content: Writing a ReplicaSet manifestAs with all other Kubernetes API objects, a ReplicaSet needs the `apiVersion`, `kind`, and `metadata` fields.\nFor ReplicaSets, the `kind` is always a ReplicaSet.  \nWhen the control plane creates new Pods for a ReplicaSet, the `.metadata.name` of the\nReplicaSet is part of the basis for naming those Pods. The name of a ReplicaSet must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).  \nA ReplicaSet also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\n", "relevant_passages": ["How should I name my ReplicaSet to avoid issues with Pod hostnames?"]}
{"query": "A user asked the following question:\nQuestion: What happens when I run 'kubectl drain' on a node?\nThis is about the following runbook:\nRunbook Title: Pod disruption budgets\nRunbook Content: Pod disruption budgets{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}  \nKubernetes offers features to help you run highly available applications even when you\nintroduce frequent voluntary disruptions.  \nAs an application owner, you can create a PodDisruptionBudget (PDB) for each application.\nA PDB limits the number of Pods of a replicated application that are down simultaneously from\nvoluntary disruptions. For example, a quorum-based application would\nlike to ensure that the number of replicas running is never brought below the\nnumber needed for a quorum. A web front end might want to\nensure that the number of replicas serving load never falls below a certain\npercentage of the total.  \nCluster managers and hosting providers should use tools which\nrespect PodDisruptionBudgets by calling the [Eviction API](/docs/tasks/administer-cluster/safely-drain-node/#eviction-api)\ninstead of directly deleting pods or deployments.  \nFor example, the `kubectl drain` subcommand lets you mark a node as going out of\nservice. When you run `kubectl drain`, the tool tries to evict all of the Pods on\nthe Node you're taking out of service. The eviction request that `kubectl` submits on\nyour behalf may be temporarily rejected, so the tool periodically retries all failed\nrequests until all Pods on the target node are terminated, or until a configurable timeout\nis reached.  \nA PDB specifies the number of replicas that an application can tolerate having, relative to how\nmany it is intended to have.  For example, a Deployment which has a `.spec.replicas: 5` is\nsupposed to have 5 pods at any given time.  If its PDB allows for there to be 4 at a time,\nthen the Eviction API will allow voluntary disruption of one (but not two) pods at a time.  \nThe group of pods that comprise the application is specified using a label selector, the same\nas the one used by the application's controller (deployment, stateful-set, etc).  \nThe \"intended\" number of pods is computed from the `.spec.replicas` of the workload resource\nthat is managing those pods. The control plane discovers the owning workload resource by\nexamining the `.metadata.ownerReferences` of the Pod.  \n[Involuntary disruptions](#voluntary-and-involuntary-disruptions) cannot be prevented by PDBs; however they\ndo count against the budget.  \nPods which are deleted or unavailable due to a rolling upgrade to an application do count\nagainst the disruption budget, but workload resources (such as Deployment and StatefulSet)\nare not limited by PDBs when doing rolling upgrades. Instead, the handling of failures\nduring application updates is configured in the spec for the specific workload resource.  \nIt is recommended to set `AlwaysAllow` [Unhealthy Pod Eviction Policy](/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy)\nto your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain.\nThe default behavior is to wait for the application pods to become [healthy](/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod)\nbefore the drain can proceed.  \nWhen a pod is evicted using the eviction API, it is gracefully\n[terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination), honoring the\n`terminationGracePeriodSeconds` setting in its [PodSpec](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podspec-v1-core).\n", "relevant_passages": ["What happens when I run 'kubectl drain' on a node?"]}
{"query": "A user asked the following question:\nQuestion: What happens to PersistentVolumes when a StatefulSet is deleted?\nThis is about the following runbook:\nRunbook Title: Stable Storage\nRunbook Content: Pod IdentityStable StorageFor each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one\nPersistentVolumeClaim. In the nginx example above, each Pod receives a single PersistentVolume\nwith a StorageClass of `my-storage-class` and 1 GiB of provisioned storage. If no StorageClass\nis specified, then the default StorageClass will be used. When a Pod is (re)scheduled\nonto a node, its `volumeMounts` mount the PersistentVolumes associated with its\nPersistentVolume Claims. Note that, the PersistentVolumes associated with the\nPods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.\nThis must be done manually.\n", "relevant_passages": ["What happens to PersistentVolumes when a StatefulSet is deleted?"]}
{"query": "A user asked the following question:\nQuestion: Is there a limit to how many pods I can manage with a ReplicationController?\nThis is about the following runbook:\nRunbook Title: Rescheduling\nRunbook Content: Common usage patternsReschedulingAs mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).\n", "relevant_passages": ["Is there a limit to how many pods I can manage with a ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: How can I verify the owner reference of a Pod in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Example\nRunbook Content: Example{{% code_sample file=\"controllers/frontend.yaml\" %}}  \nSaving this manifest into `frontend.yaml` and submitting it to a Kubernetes cluster will\ncreate the defined ReplicaSet and the Pods that it manages.  \n```shell\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\n```  \nYou can then get the current ReplicaSets deployed:  \n```shell\nkubectl get rs\n```  \nAnd see the frontend one you created:  \n```\nNAME       DESIRED   CURRENT   READY   AGE\nfrontend   3         3         3       6s\n```  \nYou can also check on the state of the ReplicaSet:  \n```shell\nkubectl describe rs/frontend\n```  \nAnd you will see output similar to:  \n```\nName:         frontend\nNamespace:    default\nSelector:     tier=frontend\nLabels:       app=guestbook\ntier=frontend\nAnnotations:  <none>\nReplicas:     3 current / 3 desired\nPods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\nLabels:  tier=frontend\nContainers:\nphp-redis:\nImage:        us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5\nPort:         <none>\nHost Port:    <none>\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nEvents:\nType    Reason            Age   From                   Message\n----    ------            ----  ----                   -------\nNormal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-gbgfx\nNormal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-rwz57\nNormal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-wkl7w\n```  \nAnd lastly you can check for the Pods brought up:  \n```shell\nkubectl get pods\n```  \nYou should see Pod information similar to:  \n```\nNAME             READY   STATUS    RESTARTS   AGE\nfrontend-gbgfx   1/1     Running   0          10m\nfrontend-rwz57   1/1     Running   0          10m\nfrontend-wkl7w   1/1     Running   0          10m\n```  \nYou can also verify that the owner reference of these pods is set to the frontend ReplicaSet.\nTo do this, get the yaml of one of the Pods running:  \n```shell\nkubectl get pods frontend-gbgfx -o yaml\n```  \nThe output will look similar to this, with the frontend ReplicaSet's info set in the metadata's ownerReferences field:  \n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: \"2024-02-28T22:30:44Z\"\ngenerateName: frontend-\nlabels:\ntier: frontend\nname: frontend-gbgfx\nnamespace: default\nownerReferences:\n- apiVersion: apps/v1\nblockOwnerDeletion: true\ncontroller: true\nkind: ReplicaSet\nname: frontend\nuid: e129deca-f864-481b-bb16-b27abfd92292\n...\n```\n", "relevant_passages": ["How can I verify the owner reference of a Pod in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What are ephemeral containers used for in Kubernetes?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- verb\n- yujuhong\ntitle: Ephemeral Containers\ncontent_type: concept\nweight: 60\n---  \n<!-- overview -->  \n{{< feature-state state=\"stable\" for_k8s_version=\"v1.25\" >}}  \nThis page provides an overview of ephemeral containers: a special type of container\nthat runs temporarily in an existing {{< glossary_tooltip term_id=\"pod\" >}} to\naccomplish user-initiated actions such as troubleshooting. You use ephemeral\ncontainers to inspect services rather than to build applications.  \n<!-- body -->\n", "relevant_passages": ["What are ephemeral containers used for in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What happens to Pods when a Node is under resource pressure?\nThis is about the following runbook:\nRunbook Title: Quality of Service classes\nRunbook Content: Quality of Service classesKubernetes classifies the Pods that you run and allocates each Pod into a specific\n_quality of service (QoS) class_. Kubernetes uses that classification to influence how different\npods are handled. Kubernetes does this classification based on the\n[resource requests](/docs/concepts/configuration/manage-resources-containers/)\nof the {{< glossary_tooltip text=\"Containers\" term_id=\"container\" >}} in that Pod, along with\nhow those requests relate to resource limits.\nThis is known as {{< glossary_tooltip text=\"Quality of Service\" term_id=\"qos-class\" >}}\n(QoS) class. Kubernetes assigns every Pod a QoS class based on the resource requests\nand limits of its component Containers. QoS classes are used by Kubernetes to decide\nwhich Pods to evict from a Node experiencing\n[Node Pressure](/docs/concepts/scheduling-eviction/node-pressure-eviction/). The possible\nQoS classes are `Guaranteed`, `Burstable`, and `BestEffort`. When a Node runs out of resources,\nKubernetes will first evict `BestEffort` Pods running on that Node, followed by `Burstable` and\nfinally `Guaranteed` Pods. When this eviction is due to resource pressure, only Pods exceeding\nresource requests are candidates for eviction.\n", "relevant_passages": ["What happens to Pods when a Node is under resource pressure?"]}
{"query": "A user asked the following question:\nQuestion: What happens if my CronJob misses the starting deadline?\nThis is about the following runbook:\nRunbook Title: Deadline for delayed Job start {#starting-deadline}\nRunbook Content: Writing a CronJob specDeadline for delayed Job start {#starting-deadline}The `.spec.startingDeadlineSeconds` field is optional.\nThis field defines a deadline (in whole seconds) for starting the Job, if that Job misses its scheduled time\nfor any reason.  \nAfter missing the deadline, the CronJob skips that instance of the Job (future occurrences are still scheduled).\nFor example, if you have a backup Job that runs twice a day, you might allow it to start up to 8 hours late,\nbut no later, because a backup taken any later wouldn't be useful: you would instead prefer to wait for\nthe next scheduled run.  \nFor Jobs that miss their configured deadline, Kubernetes treats them as failed Jobs.\nIf you don't specify `startingDeadlineSeconds` for a CronJob, the Job occurrences have no deadline.  \nIf the `.spec.startingDeadlineSeconds` field is set (not null), the CronJob\ncontroller measures the time between when a Job is expected to be created and\nnow. If the difference is higher than that limit, it will skip this execution.  \nFor example, if it is set to `200`, it allows a Job to be created for up to 200\nseconds after the actual schedule.\n", "relevant_passages": ["What happens if my CronJob misses the starting deadline?"]}
{"query": "A user asked the following question:\nQuestion: What resource limits apply to Burstable Pods?\nThis is about the following runbook:\nRunbook Title: Burstable\nRunbook Content: Quality of Service classesBurstablePods that are `Burstable` have some lower-bound resource guarantees based on the request, but\ndo not require a specific limit. If a limit is not specified, it defaults to a\nlimit equivalent to the capacity of the Node, which allows the Pods to flexibly increase\ntheir resources if resources are available. In the event of Pod eviction due to Node\nresource pressure, these Pods are evicted only after all `BestEffort` Pods are evicted.\nBecause a `Burstable` Pod can include a Container that has no resource limits or requests, a Pod\nthat is `Burstable` can try to use any amount of node resources.  \n#### Criteria  \nA Pod is given a QoS class of `Burstable` if:  \n* The Pod does not meet the criteria for QoS class `Guaranteed`.\n* At least one Container in the Pod has a memory or CPU request or limit.\n", "relevant_passages": ["What resource limits apply to Burstable Pods?"]}
{"query": "A user asked the following question:\nQuestion: Is there a way to control which nodes my DaemonSet Pods are scheduled on?\nThis is about the following runbook:\nRunbook Title: Running Pods on select Nodes\nRunbook Content: Writing a DaemonSet SpecRunning Pods on select NodesIf you specify a `.spec.template.spec.nodeSelector`, then the DaemonSet controller will\ncreate Pods on nodes which match that [node selector](/docs/concepts/scheduling-eviction/assign-pod-node/).\nLikewise if you specify a `.spec.template.spec.affinity`,\nthen DaemonSet controller will create Pods on nodes which match that\n[node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/).\nIf you do not specify either, then the DaemonSet controller will create Pods on all nodes.\n", "relevant_passages": ["Is there a way to control which nodes my DaemonSet Pods are scheduled on?"]}
{"query": "A user asked the following question:\nQuestion: Are there any restrictions on using hostIPC or hostPID when hostUsers is false?\nThis is about the following runbook:\nRunbook Title: Limitations\nRunbook Content: LimitationsWhen using a user namespace for the pod, it is disallowed to use other host\nnamespaces. In particular, if you set `hostUsers: false` then you are not\nallowed to set any of:  \n* `hostNetwork: true`\n* `hostIPC: true`\n* `hostPID: true`\n", "relevant_passages": ["Are there any restrictions on using hostIPC or hostPID when hostUsers is false?"]}
{"query": "A user asked the following question:\nQuestion: Can I change the init container image without restarting the Pod?\nThis is about the following runbook:\nRunbook Title: Detailed behavior\nRunbook Content: Detailed behaviorDuring Pod startup, the kubelet delays running init containers until the networking\nand storage are ready. Then the kubelet runs the Pod's init containers in the order\nthey appear in the Pod's spec.  \nEach init container must exit successfully before\nthe next container starts. If a container fails to start due to the runtime or\nexits with failure, it is retried according to the Pod `restartPolicy`. However,\nif the Pod `restartPolicy` is set to Always, the init containers use\n`restartPolicy` OnFailure.  \nA Pod cannot be `Ready` until all init containers have succeeded. The ports on an\ninit container are not aggregated under a Service. A Pod that is initializing\nis in the `Pending` state but should have a condition `Initialized` set to false.  \nIf the Pod [restarts](#pod-restart-reasons), or is restarted, all init containers\nmust execute again.  \nChanges to the init container spec are limited to the container image field.\nDirectly altering the `image` field of  an init container does _not_ restart the\nPod or trigger its recreation. If the Pod has yet to start, that change may\nhave an effect on how the Pod boots up.  \nFor a [pod template](/docs/concepts/workloads/pods/#pod-templates)\nyou can typically change any field for an init container; the impact of making\nthat change depends on where the pod template is used.  \nBecause init containers can be restarted, retried, or re-executed, init container\ncode should be idempotent. In particular, code that writes into any `emptyDir` volume\nshould be prepared for the possibility that an output file already exists.  \nInit containers have all of the fields of an app container. However, Kubernetes\nprohibits `readinessProbe` from being used because init containers cannot\ndefine readiness distinct from completion. This is enforced during validation.  \nUse `activeDeadlineSeconds` on the Pod to prevent init containers from failing forever.\nThe active deadline includes init containers.\nHowever it is recommended to use `activeDeadlineSeconds` only if teams deploy their application\nas a Job, because `activeDeadlineSeconds` has an effect even after initContainer finished.\nThe Pod which is already running correctly would be killed by `activeDeadlineSeconds` if you set.  \nThe name of each app and init container in a Pod must be unique; a\nvalidation error is thrown for any container sharing a name with another.\n", "relevant_passages": ["Can I change the init container image without restarting the Pod?"]}
{"query": "A user asked the following question:\nQuestion: How does the pod-template-hash label help with ReplicaSets in a Deployment?\nThis is about the following runbook:\nRunbook Title: Pod-template-hash label\nRunbook Content: Creating a DeploymentPod-template-hash label{{< caution >}}\nDo not change this label.\n{{< /caution >}}  \nThe `pod-template-hash` label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.  \nThis label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the `PodTemplate` of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,\nand in any existing Pods that the ReplicaSet might have.\n", "relevant_passages": ["How does the pod-template-hash label help with ReplicaSets in a Deployment?"]}
{"query": "A user asked the following question:\nQuestion: What should I use if my Pods need to track state?\nThis is about the following runbook:\nRunbook Title: Workload resources for managing pods\nRunbook Content: Using PodsWorkload resources for managing podsUsually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as {{< glossary_tooltip text=\"Deployment\"\nterm_id=\"deployment\" >}} or {{< glossary_tooltip text=\"Job\" term_id=\"job\" >}}.\nIf your Pods need to track state, consider the\n{{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}} resource.  \nEach Pod is meant to run a single instance of a given application. If you want to\nscale your application horizontally (to provide more overall resources by running\nmore instances), you should use multiple Pods, one for each instance. In\nKubernetes, this is typically referred to as _replication_.\nReplicated Pods are usually created and managed as a group by a workload resource\nand its {{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}.  \nSee [Pods and controllers](#pods-and-controllers) for more information on how\nKubernetes uses workload resources, and their controllers, to implement application\nscaling and auto-healing.  \nPods natively provide two kinds of shared resources for their constituent containers:\n[networking](#pod-networking) and [storage](#pod-storage).\n", "relevant_passages": ["What should I use if my Pods need to track state?"]}
{"query": "A user asked the following question:\nQuestion: How can I use a user namespace with a pod?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Take a look at [Use a User Namespace With a Pod](/docs/tasks/configure-pod-container/user-namespaces/)\n", "relevant_passages": ["How can I use a user namespace with a pod?"]}
{"query": "A user asked the following question:\nQuestion: How does resource scheduling work for containers in a Pod?\nThis is about the following runbook:\nRunbook Title: Resource sharing within containers\nRunbook Content: Detailed behaviorResource sharing within containersGiven the order of execution for init, sidecar and app containers, the following rules\nfor resource usage apply:  \n* The highest of any particular resource request or limit defined on all init\ncontainers is the *effective init request/limit*. If any resource has no\nresource limit specified this is considered as the highest limit.\n* The Pod's *effective request/limit* for a resource is the higher of:\n* the sum of all app containers request/limit for a resource\n* the effective init request/limit for a resource\n* Scheduling is done based on effective requests/limits, which means\ninit containers can reserve resources for initialization that are not used\nduring the life of the Pod.\n* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the\nQoS tier for init containers and app containers alike.  \nQuota and limits are applied based on the effective Pod request and\nlimit.\n", "relevant_passages": ["How does resource scheduling work for containers in a Pod?"]}
{"query": "A user asked the following question:\nQuestion: How does the kubelet handle init containers during Pod startup?\nThis is about the following runbook:\nRunbook Title: Detailed behavior\nRunbook Content: Detailed behaviorDuring Pod startup, the kubelet delays running init containers until the networking\nand storage are ready. Then the kubelet runs the Pod's init containers in the order\nthey appear in the Pod's spec.  \nEach init container must exit successfully before\nthe next container starts. If a container fails to start due to the runtime or\nexits with failure, it is retried according to the Pod `restartPolicy`. However,\nif the Pod `restartPolicy` is set to Always, the init containers use\n`restartPolicy` OnFailure.  \nA Pod cannot be `Ready` until all init containers have succeeded. The ports on an\ninit container are not aggregated under a Service. A Pod that is initializing\nis in the `Pending` state but should have a condition `Initialized` set to false.  \nIf the Pod [restarts](#pod-restart-reasons), or is restarted, all init containers\nmust execute again.  \nChanges to the init container spec are limited to the container image field.\nDirectly altering the `image` field of  an init container does _not_ restart the\nPod or trigger its recreation. If the Pod has yet to start, that change may\nhave an effect on how the Pod boots up.  \nFor a [pod template](/docs/concepts/workloads/pods/#pod-templates)\nyou can typically change any field for an init container; the impact of making\nthat change depends on where the pod template is used.  \nBecause init containers can be restarted, retried, or re-executed, init container\ncode should be idempotent. In particular, code that writes into any `emptyDir` volume\nshould be prepared for the possibility that an output file already exists.  \nInit containers have all of the fields of an app container. However, Kubernetes\nprohibits `readinessProbe` from being used because init containers cannot\ndefine readiness distinct from completion. This is enforced during validation.  \nUse `activeDeadlineSeconds` on the Pod to prevent init containers from failing forever.\nThe active deadline includes init containers.\nHowever it is recommended to use `activeDeadlineSeconds` only if teams deploy their application\nas a Job, because `activeDeadlineSeconds` has an effect even after initContainer finished.\nThe Pod which is already running correctly would be killed by `activeDeadlineSeconds` if you set.  \nThe name of each app and init container in a Pod must be unique; a\nvalidation error is thrown for any container sharing a name with another.\n", "relevant_passages": ["How does the kubelet handle init containers during Pod startup?"]}
{"query": "A user asked the following question:\nQuestion: What do I need to set when using the REST API to delete a ReplicaSet?\nThis is about the following runbook:\nRunbook Title: Deleting just a ReplicaSet\nRunbook Content: Working with ReplicaSetsDeleting just a ReplicaSetYou can delete a ReplicaSet without affecting any of its Pods using\n[`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete)\nwith the `--cascade=orphan` option.\nWhen using the REST API or the `client-go` library, you must set `propagationPolicy` to `Orphan`.\nFor example:  \n```shell\nkubectl proxy --port=8080\ncurl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \\\n-d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Orphan\"}' \\\n-H \"Content-Type: application/json\"\n```  \nOnce the original is deleted, you can create a new ReplicaSet to replace it. As long\nas the old and new `.spec.selector` are the same, then the new one will adopt the old Pods.\nHowever, it will not make any effort to make existing Pods match a new, different pod template.\nTo update Pods to a new spec in a controlled way, use a\n[Deployment](/docs/concepts/workloads/controllers/deployment/#creating-a-deployment), as\nReplicaSets do not support a rolling update directly.\n", "relevant_passages": ["What do I need to set when using the REST API to delete a ReplicaSet?"]}
{"query": "A user asked the following question:\nQuestion: Should I create a replacement Job immediately after a failure condition appears?\nThis is about the following runbook:\nRunbook Title: Termination of Job pods\nRunbook Content: Job termination and cleanupTermination of Job podsThe Job controller adds the `FailureTarget` condition or the `SuccessCriteriaMet`\ncondition to the Job to trigger Pod termination after a Job meets either the\nsuccess or failure criteria.  \nFactors like `terminationGracePeriodSeconds` might increase the amount of time\nfrom the moment that the Job controller adds the `FailureTarget` condition or the\n`SuccessCriteriaMet` condition to the moment that all of the Job Pods terminate\nand the Job controller adds a [terminal condition](#terminal-job-conditions)\n(`Failed` or `Complete`).  \nYou can use the `FailureTarget` or the `SuccessCriteriaMet` condition to evaluate\nwhether the Job has failed or succeeded without having to wait for the controller\nto add a terminal condition.  \nFor example, you might want to decide when to create a replacement Job\nthat replaces a failed Job. If you replace the failed Job when the `FailureTarget`\ncondition appears, your replacement Job runs sooner, but could result in Pods\nfrom the failed and the replacement Job running at the same time, using\nextra compute resources.  \nAlternatively, if your cluster has limited resource capacity, you could choose to\nwait until the `Failed` condition appears on the Job, which would delay your\nreplacement Job but would ensure that you conserve resources by waiting\nuntil all of the failed Pods are removed.\n", "relevant_passages": ["Should I create a replacement Job immediately after a failure condition appears?"]}
{"query": "A user asked the following question:\nQuestion: How are Pods created in a StatefulSet during deployment?\nThis is about the following runbook:\nRunbook Title: Deployment and Scaling Guarantees\nRunbook Content: Deployment and Scaling Guarantees* For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.\n* When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.\n* Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.\n* Before a Pod is terminated, all of its successors must be completely shutdown.  \nThe StatefulSet should not specify a `pod.Spec.TerminationGracePeriodSeconds` of 0. This practice\nis unsafe and strongly discouraged. For further explanation, please refer to\n[force deleting StatefulSet Pods](/docs/tasks/run-application/force-delete-stateful-set-pod/).  \nWhen the nginx example above is created, three Pods will be deployed in the order\nweb-0, web-1, web-2. web-1 will not be deployed before web-0 is\n[Running and Ready](/docs/concepts/workloads/pods/pod-lifecycle/), and web-2 will not be deployed until\nweb-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before\nweb-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and\nbecomes Running and Ready.  \nIf a user were to scale the deployed example by patching the StatefulSet such that\n`replicas=1`, web-2 would be terminated first. web-1 would not be terminated until web-2\nis fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and\nis completely shutdown, but prior to web-1's termination, web-1 would not be terminated\nuntil web-0 is Running and Ready.\n", "relevant_passages": ["How are Pods created in a StatefulSet during deployment?"]}
{"query": "A user asked the following question:\nQuestion: Can I filter logs based on the Pod index label?\nThis is about the following runbook:\nRunbook Title: Pod index label\nRunbook Content: Pod IdentityPod index label{{< feature-state for_k8s_version=\"v1.28\" state=\"beta\" >}}  \nWhen the StatefulSet {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} creates a Pod,\nthe new Pod is labelled with `apps.kubernetes.io/pod-index`. The value of this label is the ordinal index of\nthe Pod. This label allows you to route traffic to a particular pod index, filter logs/metrics\nusing the pod index label, and more. Note the feature gate `PodIndexLabel` must be enabled for this\nfeature, and it is enabled by default.\n", "relevant_passages": ["Can I filter logs based on the Pod index label?"]}
{"query": "A user asked the following question:\nQuestion: What are the main benefits of using StatefulSets for my application?\nThis is about the following runbook:\nRunbook Title: Using StatefulSets\nRunbook Content: Using StatefulSetsStatefulSets are valuable for applications that require one or more of the\nfollowing.  \n* Stable, unique network identifiers.\n* Stable, persistent storage.\n* Ordered, graceful deployment and scaling.\n* Ordered, automated rolling updates.  \nIn the above, stable is synonymous with persistence across Pod (re)scheduling.\nIf an application doesn't require any stable identifiers or ordered deployment,\ndeletion, or scaling, you should deploy your application using a workload object\nthat provides a set of stateless replicas.\n[Deployment](/docs/concepts/workloads/controllers/deployment/) or\n[ReplicaSet](/docs/concepts/workloads/controllers/replicaset/) may be better suited to your stateless needs.\n", "relevant_passages": ["What are the main benefits of using StatefulSets for my application?"]}
{"query": "A user asked the following question:\nQuestion: What are the key differences in resource handling between init containers and regular containers?\nThis is about the following runbook:\nRunbook Title: Differences from regular containers\nRunbook Content: Understanding init containersDifferences from regular containersInit containers support all the fields and features of app containers,\nincluding resource limits, [volumes](/docs/concepts/storage/volumes/), and security settings. However, the\nresource requests and limits for an init container are handled differently,\nas documented in [Resource sharing within containers](#resource-sharing-within-containers).  \nRegular init containers (in other words: excluding sidecar containers) do not support the\n`lifecycle`, `livenessProbe`, `readinessProbe`, or `startupProbe` fields. Init containers\nmust run to completion before the Pod can be ready; sidecar containers continue running\nduring a Pod's lifetime, and _do_ support some probes. See [sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nfor further details about sidecar containers.  \nIf you specify multiple init containers for a Pod, kubelet runs each init\ncontainer sequentially. Each init container must succeed before the next can run.\nWhen all of the init containers have run to completion, kubelet initializes\nthe application containers for the Pod and runs them as usual.\n", "relevant_passages": ["What are the key differences in resource handling between init containers and regular containers?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I stop or restart a sidecar container?\nThis is about the following runbook:\nRunbook Title: Sidecar containers in Kubernetes {#pod-sidecar-containers}\nRunbook Content: Sidecar containers in Kubernetes {#pod-sidecar-containers}Kubernetes implements sidecar containers as a special case of\n[init containers](/docs/concepts/workloads/pods/init-containers/); sidecar containers remain\nrunning after Pod startup. This document uses the term _regular init containers_ to clearly\nrefer to containers that only run during Pod startup.  \nProvided that your cluster has the `SidecarContainers`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) enabled\n(the feature is active by default since Kubernetes v1.29), you can specify a `restartPolicy`\nfor containers listed in a Pod's `initContainers` field.\nThese restartable _sidecar_ containers are independent from other init containers and from\nthe main application container(s) within the same pod.\nThese can be started, stopped, or restarted without effecting the main application container\nand other init containers.  \nYou can also run a Pod with multiple containers that are not marked as init or sidecar\ncontainers. This is appropriate if the containers within the Pod are required for the\nPod to work overall, but you don't need to control which containers start or stop first.\nYou could also do this if you need to support older versions of Kubernetes that don't\nsupport a container-level `restartPolicy` field.\n", "relevant_passages": ["What happens if I stop or restart a sidecar container?"]}
{"query": "A user asked the following question:\nQuestion: What does it mean if a Pod's status is 'PodScheduled'?\nThis is about the following runbook:\nRunbook Title: Pod conditions\nRunbook Content: Pod conditionsA Pod has a PodStatus, which has an array of\n[PodConditions](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podcondition-v1-core)\nthrough which the Pod has or has not passed. Kubelet manages the following\nPodConditions:  \n* `PodScheduled`: the Pod has been scheduled to a node.\n* `PodReadyToStartContainers`: (beta feature; enabled by [default](#pod-has-network)) the\nPod sandbox has been successfully created and networking configured.\n* `ContainersReady`: all containers in the Pod are ready.\n* `Initialized`: all [init containers](/docs/concepts/workloads/pods/init-containers/)\nhave completed successfully.\n* `Ready`: the Pod is able to serve requests and should be added to the load\nbalancing pools of all matching Services.  \nField name           | Description\n:--------------------|:-----------\n`type`               | Name of this Pod condition.\n`status`             | Indicates whether that condition is applicable, with possible values \"`True`\", \"`False`\", or \"`Unknown`\".\n`lastProbeTime`      | Timestamp of when the Pod condition was last probed.\n`lastTransitionTime` | Timestamp for when the Pod last transitioned from one status to another.\n`reason`             | Machine-readable, UpperCamelCase text indicating the reason for the condition's last transition.\n`message`            | Human-readable message indicating details about the last status transition.\n", "relevant_passages": ["What does it mean if a Pod's status is 'PodScheduled'?"]}
{"query": "A user asked the following question:\nQuestion: What happens when I change the Pod template for a workload resource?\nThis is about the following runbook:\nRunbook Title: Pod update and replacement\nRunbook Content: Pod update and replacementAs mentioned in the previous section, when the Pod template for a workload\nresource is changed, the controller creates new Pods based on the updated\ntemplate instead of updating or patching the existing Pods.  \nKubernetes doesn't prevent you from managing Pods directly. It is possible to\nupdate some fields of a running Pod, in place. However, Pod update operations\nlike\n[`patch`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#patch-pod-v1-core), and\n[`replace`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#replace-pod-v1-core)\nhave some limitations:  \n- Most of the metadata about a Pod is immutable. For example, you cannot\nchange the `namespace`, `name`, `uid`, or `creationTimestamp` fields;\nthe `generation` field is unique. It only accepts updates that increment the\nfield's current value.\n- If the `metadata.deletionTimestamp` is set, no new entry can be added to the\n`metadata.finalizers` list.\n- Pod updates may not change fields other than `spec.containers[*].image`,\n`spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or\n`spec.tolerations`. For `spec.tolerations`, you can only add new entries.\n- When updating the `spec.activeDeadlineSeconds` field, two types of updates\nare allowed:  \n1. setting the unassigned field to a positive number;\n1. updating the field from a positive number to a smaller, non-negative\nnumber.\n", "relevant_passages": ["What happens when I change the Pod template for a workload resource?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I specify both `succeededIndexes` and `succeededCount` in my success policy?\nThis is about the following runbook:\nRunbook Title: Success policy {#success-policy}\nRunbook Content: Success policy {#success-policy}{{< feature-state feature_gate_name=\"JobSuccessPolicy\" >}}  \n{{< note >}}\nYou can only configure a success policy for an Indexed Job if you have the\n`JobSuccessPolicy` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nenabled in your cluster.\n{{< /note >}}  \nWhen creating an Indexed Job, you can define when a Job can be declared as succeeded using a `.spec.successPolicy`,\nbased on the pods that succeeded.  \nBy default, a Job succeeds when the number of succeeded Pods equals `.spec.completions`.\nThese are some situations where you might want additional control for declaring a Job succeeded:  \n* When running simulations with different parameters,\nyou might not need all the simulations to succeed for the overall Job to be successful.\n* When following a leader-worker pattern, only the success of the leader determines the success or\nfailure of a Job. Examples of this are frameworks like MPI and PyTorch etc.  \nYou can configure a success policy, in the `.spec.successPolicy` field,\nto meet the above use cases. This policy can handle Job success based on the\nsucceeded pods. After the Job meets the success policy, the job controller terminates the lingering Pods.\nA success policy is defined by rules. Each rule can take one of the following forms:  \n* When you specify the `succeededIndexes` only,\nonce all indexes specified in the `succeededIndexes` succeed, the job controller marks the Job as succeeded.\nThe `succeededIndexes` must be a list of intervals between 0 and `.spec.completions-1`.\n* When you specify the `succeededCount` only,\nonce the number of succeeded indexes reaches the `succeededCount`, the job controller marks the Job as succeeded.\n* When you specify both `succeededIndexes` and `succeededCount`,\nonce the number of succeeded indexes from the subset of indexes specified in the `succeededIndexes` reaches the `succeededCount`,\nthe job controller marks the Job as succeeded.  \nNote that when you specify multiple rules in the `.spec.successPolicy.rules`,\nthe job controller evaluates the rules in order. Once the Job meets a rule, the job controller ignores remaining rules.  \nHere is a manifest for a Job with `successPolicy`:  \n{{% code_sample file=\"/controllers/job-success-policy.yaml\" %}}  \nIn the example above, both `succeededIndexes` and `succeededCount` have been specified.\nTherefore, the job controller will mark the Job as succeeded and terminate the lingering Pods\nwhen either of the specified indexes, 0, 2, or 3, succeed.\nThe Job that meets the success policy gets the `SuccessCriteriaMet` condition with a `SuccessPolicy` reason.\nAfter the removal of the lingering Pods is issued, the Job gets the `Complete` condition.  \nNote that the `succeededIndexes` is represented as intervals separated by a hyphen.\nThe number are listed in represented by the first and last element of the series, separated by a hyphen.  \n{{< note >}}\nWhen you specify both a success policy and some terminating policies such as `.spec.backoffLimit` and `.spec.podFailurePolicy`,\nonce the Job meets either policy, the job controller respects the terminating policy and ignores the success policy.\n{{< /note >}}\n", "relevant_passages": ["What happens if I specify both `succeededIndexes` and `succeededCount` in my success policy?"]}
{"query": "A user asked the following question:\nQuestion: What happens when a container in a Pod exits?\nThis is about the following runbook:\nRunbook Title: Container restart policy {#restart-policy}\nRunbook Content: How Pods handle problems with containers {#container-restarts}Container restart policy {#restart-policy}The `spec` of a Pod has a `restartPolicy` field with possible values Always, OnFailure,\nand Never. The default value is Always.  \nThe `restartPolicy` for a Pod applies to {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}\nin the Pod and to regular [init containers](/docs/concepts/workloads/pods/init-containers/).\n[Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\nignore the Pod-level `restartPolicy` field: in Kubernetes, a sidecar is defined as an\nentry inside `initContainers` that has its container-level `restartPolicy` set to `Always`.\nFor init containers that exit with an error, the kubelet restarts the init container if\nthe Pod level `restartPolicy` is either `OnFailure` or `Always`:  \n* `Always`: Automatically restarts the container after any termination.\n* `OnFailure`: Only restarts the container if it exits with an error (non-zero exit status).\n* `Never`: Does not automatically restart the terminated container.  \nWhen the kubelet is handling container restarts according to the configured restart\npolicy, that only applies to restarts that make replacement containers inside the\nsame Pod and running on the same node. After containers in a Pod exit, the kubelet\nrestarts them with an exponential backoff delay (10s, 20s, 40s, \u2026), that is capped at\n300 seconds (5 minutes). Once a container has executed for 10 minutes without any\nproblems, the kubelet resets the restart backoff timer for that container.\n[Sidecar containers and Pod lifecycle](/docs/concepts/workloads/pods/sidecar-containers/#sidecar-containers-and-pod-lifecycle)\nexplains the behaviour of `init containers` when specify `restartpolicy` field on it.\n", "relevant_passages": ["What happens when a container in a Pod exits?"]}
{"query": "A user asked the following question:\nQuestion: What are the benefits of using DaemonSetInit scripts instead of just starting daemons directly on a node?\nThis is about the following runbook:\nRunbook Title: Init scripts\nRunbook Content: Alternatives to DaemonSetInit scriptsIt is certainly possible to run daemon processes by directly starting them on a node (e.g. using\n`init`, `upstartd`, or `systemd`).  This is perfectly fine.  However, there are several advantages to\nrunning such processes via a DaemonSet:  \n- Ability to monitor and manage logs for daemons in the same way as applications.\n- Same config language and tools (e.g. Pod templates, `kubectl`) for daemons and applications.\n- Running daemons in containers with resource limits increases isolation between daemons from app\ncontainers.  However, this can also be accomplished by running the daemons in a container but not in a Pod.\n", "relevant_passages": ["What are the benefits of using DaemonSetInit scripts instead of just starting daemons directly on a node?"]}
{"query": "A user asked the following question:\nQuestion: How does the StatefulSet controller handle updates when using RollingUpdate?\nThis is about the following runbook:\nRunbook Title: Rolling Updates\nRunbook Content: Rolling UpdatesWhen a StatefulSet's `.spec.updateStrategy.type` is set to `RollingUpdate`, the\nStatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed\nin the same order as Pod termination (from the largest ordinal to the smallest), updating\neach Pod one at a time.  \nThe Kubernetes control plane waits until an updated Pod is Running and Ready prior\nto updating its predecessor. If you have set `.spec.minReadySeconds` (see\n[Minimum Ready Seconds](#minimum-ready-seconds)), the control plane additionally waits that\namount of time after the Pod turns ready, before moving on.\n", "relevant_passages": ["How does the StatefulSet controller handle updates when using RollingUpdate?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I don't set a pod failure policy for my Job?\nThis is about the following runbook:\nRunbook Title: Delayed creation of replacement pods {#pod-replacement-policy}\nRunbook Content: Advanced usageDelayed creation of replacement pods {#pod-replacement-policy}{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \n{{< note >}}\nYou can only set `podReplacementPolicy` on Jobs if you enable the `JobPodReplacementPolicy`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n(enabled by default).\n{{< /note >}}  \nBy default, the Job controller recreates Pods as soon they either fail or are terminating (have a deletion timestamp).\nThis means that, at a given time, when some of the Pods are terminating, the number of running Pods for a Job\ncan be greater than `parallelism` or greater than one Pod per index (if you are using an Indexed Job).  \nYou may choose to create replacement Pods only when the terminating Pod is fully terminal (has `status.phase: Failed`).\nTo do this, set the `.spec.podReplacementPolicy: Failed`.\nThe default replacement policy depends on whether the Job has a `podFailurePolicy` set.\nWith no Pod failure policy defined for a Job, omitting the `podReplacementPolicy` field selects the\n`TerminatingOrFailed` replacement policy:\nthe control plane creates replacement Pods immediately upon Pod deletion\n(as soon as the control plane sees that a Pod for this Job has `deletionTimestamp` set).\nFor Jobs with a Pod failure policy set, the default  `podReplacementPolicy` is `Failed`, and no other\nvalue is permitted.\nSee [Pod failure policy](#pod-failure-policy) to learn more about Pod failure policies for Jobs.  \n```yaml\nkind: Job\nmetadata:\nname: new\n...\nspec:\npodReplacementPolicy: Failed\n...\n```  \nProvided your cluster has the feature gate enabled, you can inspect the `.status.terminating` field of a Job.\nThe value of the field is the number of Pods owned by the Job that are currently terminating.  \n```shell\nkubectl get jobs/myjob -o yaml\n```  \n```yaml\napiVersion: batch/v1\nkind: Job\n# .metadata and .spec omitted\nstatus:\nterminating: 3 # three Pods are terminating and have not yet reached the Failed phase\n```\n", "relevant_passages": ["What happens if I don't set a pod failure policy for my Job?"]}
{"query": "A user asked the following question:\nQuestion: Can the ReplicationController perform readiness or liveness probes?\nThis is about the following runbook:\nRunbook Title: Responsibilities of the ReplicationController\nRunbook Content: Responsibilities of the ReplicationControllerThe ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, [readiness](https://issue.k8s.io/620) and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.  \nThe ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in [#492](https://issue.k8s.io/492)), which would change its `replicas` field. We will not add scheduling policies (for example, [spreading](https://issue.k8s.io/367#issuecomment-48428019)) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation ([#170](https://issue.k8s.io/170)).  \nThe ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The \"macro\" operations currently supported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like [Asgard](https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1) managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.\n", "relevant_passages": ["Can the ReplicationController perform readiness or liveness probes?"]}
{"query": "A user asked the following question:\nQuestion: What are the possible states a deployment can be in?\nThis is about the following runbook:\nRunbook Title: Deployment status\nRunbook Content: Deployment statusA Deployment enters various states during its lifecycle. It can be [progressing](#progressing-deployment) while\nrolling out a new ReplicaSet, it can be [complete](#complete-deployment), or it can [fail to progress](#failed-deployment).\n", "relevant_passages": ["What are the possible states a deployment can be in?"]}
{"query": "A user asked the following question:\nQuestion: How do I know if I need to rollback a Deployment?\nThis is about the following runbook:\nRunbook Title: Rolling Back a Deployment\nRunbook Content: Rolling Back a DeploymentSometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.\nBy default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want\n(you can change that by modifying revision history limit).  \n{{< note >}}\nA Deployment's revision is created when a Deployment's rollout is triggered. This means that the\nnew revision is created if and only if the Deployment's Pod template (`.spec.template`) is changed,\nfor example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,\ndo not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.\nThis means that when you roll back to an earlier revision, only the Deployment's Pod template part is\nrolled back.\n{{< /note >}}  \n* Suppose that you made a typo while updating the Deployment, by putting the image name as `nginx:1.161` instead of `nginx:1.16.1`:  \n```shell\nkubectl set image deployment/nginx-deployment nginx=nginx:1.161\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment image updated\n```  \n* The rollout gets stuck. You can verify it by checking the rollout status:  \n```shell\nkubectl rollout status deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\n```  \n* Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,\n[read more here](#deployment-status).  \n* You see that the number of old replicas (adding the replica count from\n`nginx-deployment-1564180365` and `nginx-deployment-2035384211`) is 3, and the number of\nnew replicas (from `nginx-deployment-3066724191`) is 1.  \n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY   AGE\nnginx-deployment-1564180365   3         3         3       25s\nnginx-deployment-2035384211   0         0         0       36s\nnginx-deployment-3066724191   1         1         0       6s\n```  \n* Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.  \n```shell\nkubectl get pods\n```  \nThe output is similar to this:\n```\nNAME                                READY     STATUS             RESTARTS   AGE\nnginx-deployment-1564180365-70iae   1/1       Running            0          25s\nnginx-deployment-1564180365-jbqqo   1/1       Running            0          25s\nnginx-deployment-1564180365-hysrc   1/1       Running            0          25s\nnginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s\n```  \n{{< note >}}\nThe Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (`maxUnavailable` specifically) that you have specified. Kubernetes by default sets the value to 25%.\n{{< /note >}}  \n* Get the description of the Deployment:\n```shell\nkubectl describe deployment\n```  \nThe output is similar to this:\n```\nName:           nginx-deployment\nNamespace:      default\nCreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700\nLabels:         app=nginx\nSelector:       app=nginx\nReplicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable\nStrategyType:       RollingUpdate\nMinReadySeconds:    0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx\nContainers:\nnginx:\nImage:        nginx:1.161\nPort:         80/TCP\nHost Port:    0/TCP\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    ReplicaSetUpdated\nOldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)\nNewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)\nEvents:\nFirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message\n--------- --------    -----   ----                    -------------   --------    ------              -------\n1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3\n22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1\n22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2\n22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2\n21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1\n21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3\n13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0\n13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1\n```  \nTo fix this, you need to rollback to a previous revision of Deployment that is stable.\n", "relevant_passages": ["How do I know if I need to rollback a Deployment?"]}
{"query": "A user asked the following question:\nQuestion: What does the HPA do for a ReplicaSet?\nThis is about the following runbook:\nRunbook Title: ReplicaSet as a Horizontal Pod Autoscaler Target\nRunbook Content: Working with ReplicaSetsReplicaSet as a Horizontal Pod Autoscaler TargetA ReplicaSet can also be a target for\n[Horizontal Pod Autoscalers (HPA)](/docs/tasks/run-application/horizontal-pod-autoscale/). That is,\na ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting\nthe ReplicaSet we created in the previous example.  \n{{% code_sample file=\"controllers/hpa-rs.yaml\" %}}  \nSaving this manifest into `hpa-rs.yaml` and submitting it to a Kubernetes cluster should\ncreate the defined HPA that autoscales the target ReplicaSet depending on the CPU usage\nof the replicated Pods.  \n```shell\nkubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml\n```  \nAlternatively, you can use the `kubectl autoscale` command to accomplish the same\n(and it's easier!)  \n```shell\nkubectl autoscale rs frontend --max=10 --min=3 --cpu-percent=50\n```\n", "relevant_passages": ["What does the HPA do for a ReplicaSet?"]}
{"query": "A user asked the following question:\nQuestion: How does the kubelet perform a diagnostic on a container?\nThis is about the following runbook:\nRunbook Title: Container probes\nRunbook Content: Container probesA _probe_ is a diagnostic performed periodically by the [kubelet](/docs/reference/command-line-tools-reference/kubelet/)\non a container. To perform a diagnostic, the kubelet either executes code within the container,\nor makes a network request.\n", "relevant_passages": ["How does the kubelet perform a diagnostic on a container?"]}
{"query": "A user asked the following question:\nQuestion: When should I use a Job instead of a ReplicaSet for my Pods?\nThis is about the following runbook:\nRunbook Title: Job\nRunbook Content: Alternatives to ReplicaSetJobUse a [`Job`](/docs/concepts/workloads/controllers/job/) instead of a ReplicaSet for Pods that are\nexpected to terminate on their own (that is, batch jobs).\n", "relevant_passages": ["When should I use a Job instead of a ReplicaSet for my Pods?"]}
{"query": "A user asked the following question:\nQuestion: What are the benefits of using init containers for setup tasks?\nThis is about the following runbook:\nRunbook Title: Using init containers\nRunbook Content: Using init containersBecause init containers have separate images from app containers, they\nhave some advantages for start-up related code:  \n* Init containers can contain utilities or custom code for setup that are not present in an app\nimage. For example, there is no need to make an image `FROM` another image just to use a tool like\n`sed`, `awk`, `python`, or `dig` during setup.\n* The application image builder and deployer roles can work independently without\nthe need to jointly build a single app image.\n* Init containers can run with a different view of the filesystem than app containers in the\nsame Pod. Consequently, they can be given access to\n{{< glossary_tooltip text=\"Secrets\" term_id=\"secret\" >}} that app containers cannot access.\n* Because init containers run to completion before any app containers start, init containers offer\na mechanism to block or delay app container startup until a set of preconditions are met. Once\npreconditions are met, all of the app containers in a Pod can start in parallel.\n* Init containers can securely run utilities or custom code that would otherwise make an app\ncontainer image less secure. By keeping unnecessary tools separate you can limit the attack\nsurface of your app container image.\n", "relevant_passages": ["What are the benefits of using init containers for setup tasks?"]}
{"query": "A user asked the following question:\nQuestion: What should I consider if I want to perform a node upgrade on my cluster?\nThis is about the following runbook:\nRunbook Title: How to perform Disruptive Actions on your Cluster\nRunbook Content: How to perform Disruptive Actions on your ClusterIf you are a Cluster Administrator, and you need to perform a disruptive action on all\nthe nodes in your cluster, such as a node or system software upgrade, here are some options:  \n- Accept downtime during the upgrade.\n- Failover to another complete replica cluster.\n-  No downtime, but may be costly both for the duplicated nodes\nand for human effort to orchestrate the switchover.\n- Write disruption tolerant applications and use PDBs.\n- No downtime.\n- Minimal resource duplication.\n- Allows more automation of cluster administration.\n- Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary\ndisruptions largely overlaps with work to support autoscaling and tolerating\ninvoluntary disruptions.\n", "relevant_passages": ["What should I consider if I want to perform a node upgrade on my cluster?"]}
{"query": "A user asked the following question:\nQuestion: How can I wait for a service to be created using init containers?\nThis is about the following runbook:\nRunbook Title: Examples\nRunbook Content: Using init containersExamplesHere are some ideas for how to use init containers:  \n* Wait for a {{< glossary_tooltip text=\"Service\" term_id=\"service\">}} to\nbe created, using a shell one-line command like:\n```shell\nfor i in {1..100}; do sleep 1; if nslookup myservice; then exit 0; fi; done; exit 1\n```  \n* Register this Pod with a remote server from the downward API with a command like:\n```shell\ncurl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$(<POD_NAME>)&ip=$(<POD_IP>)'\n```  \n* Wait for some time before starting the app container with a command like\n```shell\nsleep 60\n```  \n* Clone a Git repository into a {{< glossary_tooltip text=\"Volume\" term_id=\"volume\" >}}  \n* Place values into a configuration file and run a template tool to dynamically\ngenerate a configuration file for the main app container. For example,\nplace the `POD_IP` value in a configuration and generate the main app\nconfiguration file using Jinja.  \n#### Init containers in use  \nThis example defines a simple Pod that has two init containers.\nThe first waits for `myservice`, and the second waits for `mydb`. Once both\ninit containers complete, the Pod runs the app container from its `spec` section.  \n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: myapp-pod\nlabels:\napp.kubernetes.io/name: MyApp\nspec:\ncontainers:\n- name: myapp-container\nimage: busybox:1.28\ncommand: ['sh', '-c', 'echo The app is running! && sleep 3600']\ninitContainers:\n- name: init-myservice\nimage: busybox:1.28\ncommand: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"]\n- name: init-mydb\nimage: busybox:1.28\ncommand: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"]\n```  \nYou can start this Pod by running:  \n```shell\nkubectl apply -f myapp.yaml\n```\nThe output is similar to this:\n```\npod/myapp-pod created\n```  \nAnd check on its status with:\n```shell\nkubectl get -f myapp.yaml\n```\nThe output is similar to this:\n```\nNAME        READY     STATUS     RESTARTS   AGE\nmyapp-pod   0/1       Init:0/2   0          6m\n```  \nor for more details:\n```shell\nkubectl describe -f myapp.yaml\n```\nThe output is similar to this:\n```\nName:          myapp-pod\nNamespace:     default\n[...]\nLabels:        app.kubernetes.io/name=MyApp\nStatus:        Pending\n[...]\nInit Containers:\ninit-myservice:\n[...]\nState:         Running\n[...]\ninit-mydb:\n[...]\nState:         Waiting\nReason:      PodInitializing\nReady:         False\n[...]\nContainers:\nmyapp-container:\n[...]\nState:         Waiting\nReason:      PodInitializing\nReady:         False\n[...]\nEvents:\nFirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message\n---------    --------    -----    ----                      -------------                           --------      ------        -------\n16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201\n16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image \"busybox\"\n13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image \"busybox\"\n13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container init-myservice\n13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container init-myservice\n```  \nTo see logs for the init containers in this Pod, run:\n```shell\nkubectl logs myapp-pod -c init-myservice # Inspect the first init container\nkubectl logs myapp-pod -c init-mydb      # Inspect the second init container\n```  \nAt this point, those init containers will be waiting to discover {{< glossary_tooltip text=\"Services\" term_id=\"service\" >}} named\n`mydb` and `myservice`.  \nHere's a configuration you can use to make those Services appear:  \n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: myservice\nspec:\nports:\n- protocol: TCP\nport: 80\ntargetPort: 9376\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: mydb\nspec:\nports:\n- protocol: TCP\nport: 80\ntargetPort: 9377\n```  \nTo create the `mydb` and `myservice` services:  \n```shell\nkubectl apply -f services.yaml\n```\nThe output is similar to this:\n```\nservice/myservice created\nservice/mydb created\n```  \nYou'll then see that those init containers complete, and that the `myapp-pod`\nPod moves into the Running state:  \n```shell\nkubectl get -f myapp.yaml\n```\nThe output is similar to this:\n```\nNAME        READY     STATUS    RESTARTS   AGE\nmyapp-pod   1/1       Running   0          9m\n```  \nThis simple example should provide some inspiration for you to create your own\ninit containers. [What's next](#what-s-next) contains a link to a more detailed example.\n", "relevant_passages": ["How can I wait for a service to be created using init containers?"]}
{"query": "A user asked the following question:\nQuestion: Can I control the number of Pods running in a Parallel Job?\nThis is about the following runbook:\nRunbook Title: Parallel execution for Jobs {#parallel-jobs}\nRunbook Content: Writing a Job specParallel execution for Jobs {#parallel-jobs}There are three main types of task suitable to run as a Job:  \n1. Non-parallel Jobs\n- normally, only one Pod is started, unless the Pod fails.\n- the Job is complete as soon as its Pod terminates successfully.\n1. Parallel Jobs with a *fixed completion count*:\n- specify a non-zero positive value for `.spec.completions`.\n- the Job represents the overall task, and is complete when there are `.spec.completions` successful Pods.\n- when using `.spec.completionMode=\"Indexed\"`, each Pod gets a different index in the range 0 to `.spec.completions-1`.\n1. Parallel Jobs with a *work queue*:\n- do not specify `.spec.completions`, default to `.spec.parallelism`.\n- the Pods must coordinate amongst themselves or an external service to determine\nwhat each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.\n- each Pod is independently capable of determining whether or not all its peers are done,\nand thus that the entire Job is done.\n- when _any_ Pod from the Job terminates with success, no new Pods are created.\n- once at least one Pod has terminated with success and all Pods are terminated,\nthen the Job is completed with success.\n- once any Pod has exited with success, no other Pod should still be doing any work\nfor this task or writing any output. They should all be in the process of exiting.  \nFor a _non-parallel_ Job, you can leave both `.spec.completions` and `.spec.parallelism` unset.\nWhen both are unset, both are defaulted to 1.  \nFor a _fixed completion count_ Job, you should set `.spec.completions` to the number of completions needed.\nYou can set `.spec.parallelism`, or leave it unset and it will default to 1.  \nFor a _work queue_ Job, you must leave `.spec.completions` unset, and set `.spec.parallelism` to\na non-negative integer.  \nFor more information about how to make use of the different types of job,\nsee the [job patterns](#job-patterns) section.  \n#### Controlling parallelism  \nThe requested parallelism (`.spec.parallelism`) can be set to any non-negative value.\nIf it is unspecified, it defaults to 1.\nIf it is specified as 0, then the Job is effectively paused until it is increased.  \nActual parallelism (number of pods running at any instant) may be more or less than requested\nparallelism, for a variety of reasons:  \n- For _fixed completion count_ Jobs, the actual number of pods running in parallel will not exceed the number of\nremaining completions. Higher values of `.spec.parallelism` are effectively ignored.\n- For _work queue_ Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete, however.\n- If the Job {{< glossary_tooltip term_id=\"controller\" >}} has not had time to react.\n- If the Job controller failed to create Pods for any reason (lack of `ResourceQuota`, lack of permission, etc.),\nthen there may be fewer pods than requested.\n- The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.\n- When a Pod is gracefully shut down, it takes time to stop.\n", "relevant_passages": ["Can I control the number of Pods running in a Parallel Job?"]}
{"query": "A user asked the following question:\nQuestion: Does the kube-scheduler consider QoS class when preempting Pods?\nThis is about the following runbook:\nRunbook Title: Some behavior is independent of QoS class {#class-independent-behavior}\nRunbook Content: Some behavior is independent of QoS class {#class-independent-behavior}Certain behavior is independent of the QoS class assigned by Kubernetes. For example:  \n* Any Container exceeding a resource limit will be killed and restarted by the kubelet without\naffecting other Containers in that Pod.  \n* If a Container exceeds its resource request and the node it runs on faces\nresource pressure, the Pod it is in becomes a candidate for [eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\nIf this occurs, all Containers in the Pod will be terminated. Kubernetes may create a\nreplacement Pod, usually on a different node.  \n* The resource request of a Pod is equal to the sum of the resource requests of\nits component Containers, and the resource limit of a Pod is equal to the sum of\nthe resource limits of its component Containers.  \n* The kube-scheduler does not consider QoS class when selecting which Pods to\n[preempt](/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption).\nPreemption can occur when a cluster does not have enough resources to run all the Pods\nyou defined.\n", "relevant_passages": ["Does the kube-scheduler consider QoS class when preempting Pods?"]}
{"query": "A user asked the following question:\nQuestion: How can I get hands-on experience with container lifecycle events?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Get hands-on experience\n[attaching handlers to container lifecycle events](/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/).  \n* Get hands-on experience\n[configuring Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).  \n* Learn more about [container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/).  \n* Learn more about [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/).  \n* For detailed information about Pod and container status in the API, see\nthe API reference documentation covering\n[`status`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus) for Pod.\n", "relevant_passages": ["How can I get hands-on experience with container lifecycle events?"]}
{"query": "A user asked the following question:\nQuestion: What are the benefits of using a single Job to start a controller Pod?\nThis is about the following runbook:\nRunbook Title: Single Job starts controller Pod\nRunbook Content: AlternativesSingle Job starts controller PodAnother pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort\nof custom controller for those Pods. This allows the most flexibility, but may be somewhat\ncomplicated to get started with and offers less integration with Kubernetes.  \nOne example of this pattern would be a Job which starts a Pod which runs a script that in turn\nstarts a Spark master controller (see [spark example](https://github.com/kubernetes/examples/tree/master/staging/spark/README.md)),\nruns a spark driver, and then cleans up.  \nAn advantage of this approach is that the overall process gets the completion guarantee of a Job\nobject, but maintains complete control over what Pods are created and how work is assigned to them.\n", "relevant_passages": ["What are the benefits of using a single Job to start a controller Pod?"]}
{"query": "A user asked the following question:\nQuestion: Where can I find more details about running automated tasks with a CronJob?\nThis is about the following runbook:\nRunbook Title: Example\nRunbook Content: ExampleThis example CronJob manifest prints the current time and a hello message every minute:  \n{{% code_sample file=\"application/job/cronjob.yaml\" %}}  \n([Running Automated Tasks with a CronJob](/docs/tasks/job/automated-tasks-with-cron-jobs/)\ntakes you through this example in more detail).\n", "relevant_passages": ["Where can I find more details about running automated tasks with a CronJob?"]}
{"query": "A user asked the following question:\nQuestion: What happens if a Node fails and my Pods stop working?\nThis is about the following runbook:\nRunbook Title: Pods and controllers\nRunbook Content: Working with PodsPods and controllersYou can use workload resources to create and manage multiple Pods for you. A controller\nfor the resource handles replication and rollout and automatic healing in case of\nPod failure. For example, if a Node fails, a controller notices that Pods on that\nNode have stopped working and creates a replacement Pod. The scheduler places the\nreplacement Pod onto a healthy Node.  \nHere are some examples of workload resources that manage one or more Pods:  \n* {{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}\n* {{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}}\n* {{< glossary_tooltip text=\"DaemonSet\" term_id=\"daemonset\" >}}\n", "relevant_passages": ["What happens if a Node fails and my Pods stop working?"]}
{"query": "A user asked the following question:\nQuestion: How can I list all the pods created by my ReplicationController?\nThis is about the following runbook:\nRunbook Title: Running an example ReplicationController\nRunbook Content: Running an example ReplicationControllerThis example ReplicationController config runs three copies of the nginx web server.  \n{{% code_sample file=\"controllers/replication.yaml\" %}}  \nRun the example job by downloading the example file and then running this command:  \n```shell\nkubectl apply -f https://k8s.io/examples/controllers/replication.yaml\n```  \nThe output is similar to this:  \n```\nreplicationcontroller/nginx created\n```  \nCheck on the status of the ReplicationController using this command:  \n```shell\nkubectl describe replicationcontrollers/nginx\n```  \nThe output is similar to this:  \n```\nName:        nginx\nNamespace:   default\nSelector:    app=nginx\nLabels:      app=nginx\nAnnotations:    <none>\nReplicas:    3 current / 3 desired\nPods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed\nPod Template:\nLabels:       app=nginx\nContainers:\nnginx:\nImage:              nginx\nPort:               80/TCP\nEnvironment:        <none>\nMounts:             <none>\nVolumes:              <none>\nEvents:\nFirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message\n---------       --------     -----    ----                        -------------    ----      ------              -------\n20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m\n20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0\n20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v\n```  \nHere, three pods are created, but none is running yet, perhaps because the image is being pulled.\nA little later, the same command may show:  \n```shell\nPods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed\n```  \nTo list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:  \n```shell\npods=$(kubectl get pods --selector=app=nginx --output=jsonpath={.items..metadata.name})\necho $pods\n```  \nThe output is similar to this:  \n```\nnginx-3ntk0 nginx-4ok8v nginx-qrm3m\n```  \nHere, the selector is the same as the selector for the ReplicationController (seen in the\n`kubectl describe` output), and in a different form in `replication.yaml`.  The `--output=jsonpath` option\nspecifies an expression with the name from each pod in the returned list.\n", "relevant_passages": ["How can I list all the pods created by my ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: What happens if I delete a DaemonSet with --cascade=orphan?\nThis is about the following runbook:\nRunbook Title: Updating a DaemonSet\nRunbook Content: Updating a DaemonSetIf node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete\nPods from newly not-matching nodes.  \nYou can modify the Pods that a DaemonSet creates.  However, Pods do not allow all\nfields to be updated.  Also, the DaemonSet controller will use the original template the next\ntime a node (even with the same name) is created.  \nYou can delete a DaemonSet.  If you specify `--cascade=orphan` with `kubectl`, then the Pods\nwill be left on the nodes.  If you subsequently create a new DaemonSet with the same selector,\nthe new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces\nthem according to its `updateStrategy`.  \nYou can [perform a rolling update](/docs/tasks/manage-daemon/update-daemon-set/) on a DaemonSet.\n", "relevant_passages": ["What happens if I delete a DaemonSet with --cascade=orphan?"]}
{"query": "A user asked the following question:\nQuestion: How do I delegate a Job object to an external controller?\nThis is about the following runbook:\nRunbook Title: Delegation of managing a Job object to external controller\nRunbook Content: Advanced usageDelegation of managing a Job object to external controller{{< feature-state feature_gate_name=\"JobManagedBy\" >}}  \n{{< note >}}\nYou can only set the `managedBy` field on Jobs if you enable the `JobManagedBy`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n(disabled by default).\n{{< /note >}}  \nThis feature allows you to disable the built-in Job controller, for a specific\nJob, and delegate reconciliation of the Job to an external controller.  \nYou indicate the controller that reconciles the Job by setting a custom value\nfor the `spec.managedBy` field - any value\nother than `kubernetes.io/job-controller`. The value of the field is immutable.  \n{{< note >}}\nWhen using this feature, make sure the controller indicated by the field is\ninstalled, otherwise the Job may not be reconciled at all.\n{{< /note >}}  \n{{< note >}}\nWhen developing an external Job controller be aware that your controller needs\nto operate in a fashion conformant with the definitions of the API spec and\nstatus fields of the Job object.  \nPlease review these in detail in the [Job API](/docs/reference/kubernetes-api/workload-resources/job-v1/).\nWe also recommend that you run the e2e conformance tests for the Job object to\nverify your implementation.  \nFinally, when developing an external Job controller make sure it does not use the\n`batch.kubernetes.io/job-tracking` finalizer, reserved for the built-in controller.\n{{< /note >}}  \n{{< warning >}}\nIf you are considering to disable the `JobManagedBy` feature gate, or to\ndowngrade the cluster to a version without the feature gate enabled, check if\nthere are jobs with a custom value of the `spec.managedBy` field. If there\nare such jobs, there is a risk that they might be reconciled by two controllers\nafter the operation: the built-in Job controller and the external controller\nindicated by the field value.\n{{< /warning >}}\n", "relevant_passages": ["How do I delegate a Job object to an external controller?"]}
{"query": "A user asked the following question:\nQuestion: How can I automatically clean up finished jobs in the system?\nThis is about the following runbook:\nRunbook Title: Clean up finished jobs automatically\nRunbook Content: Clean up finished jobs automaticallyFinished Jobs are usually no longer needed in the system. Keeping them around in\nthe system will put pressure on the API server. If the Jobs are managed directly\nby a higher level controller, such as\n[CronJobs](/docs/concepts/workloads/controllers/cron-jobs/), the Jobs can be\ncleaned up by CronJobs based on the specified capacity-based cleanup policy.\n", "relevant_passages": ["How can I automatically clean up finished jobs in the system?"]}
{"query": "A user asked the following question:\nQuestion: How can I scale up my Deployment to handle more load?\nThis is about the following runbook:\nRunbook Title: Use Case\nRunbook Content: Use CaseThe following are typical use cases for Deployments:  \n* [Create a Deployment to rollout a ReplicaSet](#creating-a-deployment). The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.\n* [Declare the new state of the Pods](#updating-a-deployment) by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.\n* [Rollback to an earlier Deployment revision](#rolling-back-a-deployment) if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.\n* [Scale up the Deployment to facilitate more load](#scaling-a-deployment).\n* [Pause the rollout of a Deployment](#pausing-and-resuming-a-deployment) to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.\n* [Use the status of the Deployment](#deployment-status) as an indicator that a rollout has stuck.\n* [Clean up older ReplicaSets](#clean-up-policy) that you don't need anymore.\n", "relevant_passages": ["How can I scale up my Deployment to handle more load?"]}
{"query": "A user asked the following question:\nQuestion: What are init containers and when do they run?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- erictune\ntitle: Init Containers\ncontent_type: concept\nweight: 40\n---  \n<!-- overview -->\nThis page provides an overview of init containers: specialized containers that run\nbefore app containers in a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}.\nInit containers can contain utilities or setup scripts not present in an app image.  \nYou can specify init containers in the Pod specification alongside the `containers`\narray (which describes app containers).  \nIn Kubernetes, a [sidecar container](/docs/concepts/workloads/pods/sidecar-containers/) is a container that\nstarts before the main application container and _continues to run_. This document is about init containers:\ncontainers that run to completion during Pod initialization.  \n<!-- body -->\n", "relevant_passages": ["What are init containers and when do they run?"]}
{"query": "A user asked the following question:\nQuestion: What tool is responsible for running container probes?\nThis is about the following runbook:\nRunbook Title: Container probes\nRunbook Content: Container probesA _probe_ is a diagnostic performed periodically by the [kubelet](/docs/reference/command-line-tools-reference/kubelet/)\non a container. To perform a diagnostic, the kubelet either executes code within the container,\nor makes a network request.\n", "relevant_passages": ["What tool is responsible for running container probes?"]}
{"query": "A user asked the following question:\nQuestion: Can you explain the limitations of capabilities granted to a pod using user namespaces?\nThis is about the following runbook:\nRunbook Title: Understanding user namespaces for pods {#pods-and-userns}\nRunbook Content: Understanding user namespaces for pods {#pods-and-userns}Several container runtimes with their default configuration (like Docker Engine,\ncontainerd, CRI-O) use Linux namespaces for isolation. Other technologies exist\nand can be used with those runtimes too (e.g. Kata Containers uses VMs instead of\nLinux namespaces). This page is applicable for container runtimes using Linux\nnamespaces for isolation.  \nWhen creating a pod, by default, several new namespaces are used for isolation:\na network namespace to isolate the network of the container, a PID namespace to\nisolate the view of processes, etc. If a user namespace is used, this will\nisolate the users in the container from the users in the node.  \nThis means containers can run as root and be mapped to a non-root user on the\nhost. Inside the container the process will think it is running as root (and\ntherefore tools like `apt`, `yum`, etc. work fine), while in reality the process\ndoesn't have privileges on the host. You can verify this, for example, if you\ncheck which user the container process is running by executing `ps aux` from\nthe host. The user `ps` shows is not the same as the user you see if you\nexecute inside the container the command `id`.  \nThis abstraction limits what can happen, for example, if the container manages\nto escape to the host. Given that the container is running as a non-privileged\nuser on the host, it is limited what it can do to the host.  \nFurthermore, as users on each pod will be mapped to different non-overlapping\nusers in the host, it is limited what they can do to other pods too.  \nCapabilities granted to a pod are also limited to the pod user namespace and\nmostly invalid out of it, some are even completely void. Here are two examples:\n- `CAP_SYS_MODULE` does not have any effect if granted to a pod using user\nnamespaces, the pod isn't able to load kernel modules.\n- `CAP_SYS_ADMIN` is limited to the pod's user namespace and invalid outside\nof it.  \nWithout using a user namespace a container running as root, in the case of a\ncontainer breakout, has root privileges on the node. And if some capability were\ngranted to the container, the capabilities are valid on the host too. None of\nthis is true when we use user namespaces.  \nIf you want to know more details about what changes when user namespaces are in\nuse, see `man 7 user_namespaces`.\n", "relevant_passages": ["Can you explain the limitations of capabilities granted to a pod using user namespaces?"]}
{"query": "A user asked the following question:\nQuestion: How can I troubleshoot a container that has crashed when `kubectl exec` isn't working?\nThis is about the following runbook:\nRunbook Title: Uses for ephemeral containers\nRunbook Content: Uses for ephemeral containersEphemeral containers are useful for interactive troubleshooting when `kubectl\nexec` is insufficient because a container has crashed or a container image\ndoesn't include debugging utilities.  \nIn particular, [distroless images](https://github.com/GoogleContainerTools/distroless)\nenable you to deploy minimal container images that reduce attack surface\nand exposure to bugs and vulnerabilities. Since distroless images do not include a\nshell or any debugging utilities, it's difficult to troubleshoot distroless\nimages using `kubectl exec` alone.  \nWhen using ephemeral containers, it's helpful to enable [process namespace\nsharing](/docs/tasks/configure-pod-container/share-process-namespace/) so\nyou can view processes in other containers.\n", "relevant_passages": ["How can I troubleshoot a container that has crashed when `kubectl exec` isn't working?"]}
{"query": "A user asked the following question:\nQuestion: What are the essential fields I need to include in a DaemonSet spec?\nThis is about the following runbook:\nRunbook Title: Required Fields\nRunbook Content: Writing a DaemonSet SpecRequired FieldsAs with all other Kubernetes config, a DaemonSet needs `apiVersion`, `kind`, and `metadata` fields.  For\ngeneral information about working with config files, see\n[running stateless applications](/docs/tasks/run-application/run-stateless-application-deployment/)\nand [object management using kubectl](/docs/concepts/overview/working-with-objects/object-management/).  \nThe name of a DaemonSet object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).  \nA DaemonSet also needs a\n[`.spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)\nsection.\n", "relevant_passages": ["What are the essential fields I need to include in a DaemonSet spec?"]}
{"query": "A user asked the following question:\nQuestion: Should I set .spec.replicas if I'm using a HorizontalPodAutoscaler?\nThis is about the following runbook:\nRunbook Title: Replicas\nRunbook Content: Writing a Deployment SpecReplicas`.spec.replicas` is an optional field that specifies the number of desired Pods. It defaults to 1.  \nShould you manually scale a Deployment, example via `kubectl scale deployment\ndeployment --replicas=X`, and then you update that Deployment based on a manifest\n(for example: by running `kubectl apply -f deployment.yaml`),\nthen applying that manifest overwrites the manual scaling that you previously did.  \nIf a [HorizontalPodAutoscaler](/docs/tasks/run-application/horizontal-pod-autoscale/) (or any\nsimilar API for horizontal scaling) is managing scaling for a Deployment, don't set `.spec.replicas`.  \nInstead, allow the Kubernetes\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} to manage the\n`.spec.replicas` field automatically.\n", "relevant_passages": ["Should I set .spec.replicas if I'm using a HorizontalPodAutoscaler?"]}
{"query": "A user asked the following question:\nQuestion: How does a CronJob create Jobs in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Job creation\nRunbook Content: CronJob limitations {#cron-job-limitations}Job creationA CronJob creates a Job object approximately once per execution time of its schedule.\nThe scheduling is approximate because there\nare certain circumstances where two Jobs might be created, or no Job might be created.\nKubernetes tries to avoid those situations, but does not completely prevent them. Therefore,\nthe Jobs that you define should be _idempotent_.  \nIf `startingDeadlineSeconds` is set to a large value or left unset (the default)\nand if `concurrencyPolicy` is set to `Allow`, the Jobs will always run\nat least once.  \n{{< caution >}}\nIf `startingDeadlineSeconds` is set to a value less than 10 seconds, the CronJob may not be scheduled. This is because the CronJob controller checks things every 10 seconds.\n{{< /caution >}}  \nFor every CronJob, the CronJob {{< glossary_tooltip term_id=\"controller\" >}} checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the Job and logs the error.  \n```\nCannot determine if job needs to be started. Too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.\n```  \nIt is important to note that if the `startingDeadlineSeconds` field is set (not `nil`), the controller counts how many missed Jobs occurred from the value of `startingDeadlineSeconds` until now rather than from the last scheduled time until now. For example, if `startingDeadlineSeconds` is `200`, the controller counts how many missed Jobs occurred in the last 200 seconds.  \nA CronJob is counted as missed if it has failed to be created at its scheduled time. For example, if `concurrencyPolicy` is set to `Forbid` and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed.  \nFor example, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its\n`startingDeadlineSeconds` field is not set. If the CronJob controller happens to\nbe down from `08:29:00` to `10:21:00`, the Job will not start as the number of missed Jobs which missed their schedule is greater than 100.  \nTo illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its\n`startingDeadlineSeconds` is set to 200 seconds. If the CronJob controller happens to\nbe down for the same period as the previous example (`08:29:00` to `10:21:00`,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (i.e., 3 missed schedules), rather than from the last scheduled time until now.  \nThe CronJob is only responsible for creating Jobs that match its schedule, and\nthe Job in turn is responsible for the management of the Pods it represents.\n", "relevant_passages": ["How does a CronJob create Jobs in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: How can I learn about Pods in Kubernetes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Learn about [Deployments](/docs/concepts/workloads/controllers/deployment/).\n* [Run a Stateless Application Using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/),\nwhich relies on ReplicaSets to work.\n* `ReplicaSet` is a top-level resource in the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/replica-set-v1\" >}}\nobject definition to understand the API for replica sets.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how\nyou can use it to manage application availability during disruptions.\n", "relevant_passages": ["How can I learn about Pods in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What factors does the ReplicaSet controller consider when deciding which pods to delete during scaling down?\nThis is about the following runbook:\nRunbook Title: Scaling a ReplicaSet\nRunbook Content: Working with ReplicaSetsScaling a ReplicaSetA ReplicaSet can be easily scaled up or down by simply updating the `.spec.replicas` field. The ReplicaSet controller\nensures that a desired number of Pods with a matching label selector are available and operational.  \nWhen scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods to\nprioritize scaling down pods based on the following general algorithm:  \n1. Pending (and unschedulable) pods are scaled down first\n1. If `controller.kubernetes.io/pod-deletion-cost` annotation is set, then\nthe pod with the lower value will come first.\n1. Pods on nodes with more replicas come before pods on nodes with fewer replicas.\n1. If the pods' creation times differ, the pod that was created more recently\ncomes before the older pod (the creation times are bucketed on an integer log scale).  \nIf all of the above match, then selection is random.\n", "relevant_passages": ["What factors does the ReplicaSet controller consider when deciding which pods to delete during scaling down?"]}
{"query": "A user asked the following question:\nQuestion: How do I identify a specific Pod in a StatefulSet using its index?\nThis is about the following runbook:\nRunbook Title: Pod index label\nRunbook Content: Pod IdentityPod index label{{< feature-state for_k8s_version=\"v1.28\" state=\"beta\" >}}  \nWhen the StatefulSet {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} creates a Pod,\nthe new Pod is labelled with `apps.kubernetes.io/pod-index`. The value of this label is the ordinal index of\nthe Pod. This label allows you to route traffic to a particular pod index, filter logs/metrics\nusing the pod index label, and more. Note the feature gate `PodIndexLabel` must be enabled for this\nfeature, and it is enabled by default.\n", "relevant_passages": ["How do I identify a specific Pod in a StatefulSet using its index?"]}
{"query": "A user asked the following question:\nQuestion: Can I create Pods directly in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Using Pods\nRunbook Content: Using PodsThe following is an example of a Pod which consists of a container running the image `nginx:1.14.2`.  \n{{% code_sample file=\"pods/simple-pod.yaml\" %}}  \nTo create the Pod shown above, run the following command:\n```shell\nkubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml\n```  \nPods are generally not created directly and are created using workload resources.\nSee [Working with Pods](#working-with-pods) for more information on how Pods are used\nwith workload resources.\n", "relevant_passages": ["Can I create Pods directly in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What is a ReplicaSet and how does it relate to ReplicationController?\nThis is about the following runbook:\nRunbook Title: ReplicaSet\nRunbook Content: Alternatives to ReplicationControllerReplicaSet[`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is the next-generation ReplicationController that supports the new [set-based label selector](/docs/concepts/overview/working-with-objects/labels/#set-based-requirement).\nIt's mainly used by [Deployment](/docs/concepts/workloads/controllers/deployment/) as a mechanism to orchestrate pod creation, deletion and updates.\nNote that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don't require updates at all.\n", "relevant_passages": ["What is a ReplicaSet and how does it relate to ReplicationController?"]}
{"query": "A user asked the following question:\nQuestion: How do I set the starting ordinal for my Pods?\nThis is about the following runbook:\nRunbook Title: Start ordinal\nRunbook Content: Pod IdentityStart ordinal{{< feature-state feature_gate_name=\"StatefulSetStartOrdinal\" >}}  \n`.spec.ordinals` is an optional field that allows you to configure the integer\nordinals assigned to each Pod. It defaults to nil. Within the field, you can\nconfigure the following options:  \n* `.spec.ordinals.start`: If the `.spec.ordinals.start` field is set, Pods will\nbe assigned ordinals from `.spec.ordinals.start` up through\n`.spec.ordinals.start + .spec.replicas - 1`.\n", "relevant_passages": ["How do I set the starting ordinal for my Pods?"]}
{"query": "A user asked the following question:\nQuestion: What exactly is a Pod in Kubernetes?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- erictune\ntitle: Pods\napi_metadata:\n- apiVersion: \"v1\"\nkind: \"Pod\"\ncontent_type: concept\nweight: 10\nno_list: true\n---  \n<!-- overview -->  \n_Pods_ are the smallest deployable units of computing that you can create and manage in Kubernetes.  \nA _Pod_ (as in a pod of whales or pea pod) is a group of one or more\n{{< glossary_tooltip text=\"containers\" term_id=\"container\" >}}, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and\nco-scheduled, and run in a shared context. A Pod models an\napplication-specific \"logical host\": it contains one or more application\ncontainers which are relatively tightly coupled.\nIn non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.  \nAs well as application containers, a Pod can contain\n{{< glossary_tooltip text=\"init containers\" term_id=\"init-container\" >}} that run\nduring Pod startup. You can also inject\n{{< glossary_tooltip text=\"ephemeral containers\" term_id=\"ephemeral-container\" >}}\nfor debugging a running Pod.  \n<!-- body -->\n", "relevant_passages": ["What exactly is a Pod in Kubernetes?"]}
{"query": "A user asked the following question:\nQuestion: What command do I use to register a Pod with a remote server from the downward API?\nThis is about the following runbook:\nRunbook Title: Examples\nRunbook Content: Using init containersExamplesHere are some ideas for how to use init containers:  \n* Wait for a {{< glossary_tooltip text=\"Service\" term_id=\"service\">}} to\nbe created, using a shell one-line command like:\n```shell\nfor i in {1..100}; do sleep 1; if nslookup myservice; then exit 0; fi; done; exit 1\n```  \n* Register this Pod with a remote server from the downward API with a command like:\n```shell\ncurl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$(<POD_NAME>)&ip=$(<POD_IP>)'\n```  \n* Wait for some time before starting the app container with a command like\n```shell\nsleep 60\n```  \n* Clone a Git repository into a {{< glossary_tooltip text=\"Volume\" term_id=\"volume\" >}}  \n* Place values into a configuration file and run a template tool to dynamically\ngenerate a configuration file for the main app container. For example,\nplace the `POD_IP` value in a configuration and generate the main app\nconfiguration file using Jinja.  \n#### Init containers in use  \nThis example defines a simple Pod that has two init containers.\nThe first waits for `myservice`, and the second waits for `mydb`. Once both\ninit containers complete, the Pod runs the app container from its `spec` section.  \n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: myapp-pod\nlabels:\napp.kubernetes.io/name: MyApp\nspec:\ncontainers:\n- name: myapp-container\nimage: busybox:1.28\ncommand: ['sh', '-c', 'echo The app is running! && sleep 3600']\ninitContainers:\n- name: init-myservice\nimage: busybox:1.28\ncommand: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"]\n- name: init-mydb\nimage: busybox:1.28\ncommand: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"]\n```  \nYou can start this Pod by running:  \n```shell\nkubectl apply -f myapp.yaml\n```\nThe output is similar to this:\n```\npod/myapp-pod created\n```  \nAnd check on its status with:\n```shell\nkubectl get -f myapp.yaml\n```\nThe output is similar to this:\n```\nNAME        READY     STATUS     RESTARTS   AGE\nmyapp-pod   0/1       Init:0/2   0          6m\n```  \nor for more details:\n```shell\nkubectl describe -f myapp.yaml\n```\nThe output is similar to this:\n```\nName:          myapp-pod\nNamespace:     default\n[...]\nLabels:        app.kubernetes.io/name=MyApp\nStatus:        Pending\n[...]\nInit Containers:\ninit-myservice:\n[...]\nState:         Running\n[...]\ninit-mydb:\n[...]\nState:         Waiting\nReason:      PodInitializing\nReady:         False\n[...]\nContainers:\nmyapp-container:\n[...]\nState:         Waiting\nReason:      PodInitializing\nReady:         False\n[...]\nEvents:\nFirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message\n---------    --------    -----    ----                      -------------                           --------      ------        -------\n16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201\n16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image \"busybox\"\n13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image \"busybox\"\n13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container init-myservice\n13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container init-myservice\n```  \nTo see logs for the init containers in this Pod, run:\n```shell\nkubectl logs myapp-pod -c init-myservice # Inspect the first init container\nkubectl logs myapp-pod -c init-mydb      # Inspect the second init container\n```  \nAt this point, those init containers will be waiting to discover {{< glossary_tooltip text=\"Services\" term_id=\"service\" >}} named\n`mydb` and `myservice`.  \nHere's a configuration you can use to make those Services appear:  \n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: myservice\nspec:\nports:\n- protocol: TCP\nport: 80\ntargetPort: 9376\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: mydb\nspec:\nports:\n- protocol: TCP\nport: 80\ntargetPort: 9377\n```  \nTo create the `mydb` and `myservice` services:  \n```shell\nkubectl apply -f services.yaml\n```\nThe output is similar to this:\n```\nservice/myservice created\nservice/mydb created\n```  \nYou'll then see that those init containers complete, and that the `myapp-pod`\nPod moves into the Running state:  \n```shell\nkubectl get -f myapp.yaml\n```\nThe output is similar to this:\n```\nNAME        READY     STATUS    RESTARTS   AGE\nmyapp-pod   1/1       Running   0          9m\n```  \nThis simple example should provide some inspiration for you to create your own\ninit containers. [What's next](#what-s-next) contains a link to a more detailed example.\n", "relevant_passages": ["What command do I use to register a Pod with a remote server from the downward API?"]}
