[
  {
    "question": "When does the Job controller add terminal conditions in Kubernetes v1.31 and later?",
    "answer": "In Kubernetes v1.31 and later, the Job controller adds the terminal conditions `Failed` or `Complete` only after all of the Job Pods are terminated.",
    "uuid": "7c22487c-7361-4574-b313-371d70083d04",
    "question_with_context": "A user asked the following question:\nQuestion: When does the Job controller add terminal conditions in Kubernetes v1.31 and later?\nThis is about the following runbook:\nRunbook Title: Terminal Job conditions\nRunbook Content: Job termination and cleanupTerminal Job conditionsA Job has two possible terminal states, each of which has a corresponding Job\ncondition:\n* Succeeded:  Job condition `Complete`\n* Failed: Job condition `Failed`  \nJobs fail for the following reasons:\n- The number of Pod failures exceeded the specified `.spec.backoffLimit` in the Job\nspecification. For details, see [Pod backoff failure policy](#pod-backoff-failure-policy).\n- The Job runtime exceeded the specified `.spec.activeDeadlineSeconds`\n- An indexed Job that used `.spec.backoffLimitPerIndex` has failed indexes.\nFor details, see [Backoff limit per index](#backoff-limit-per-index).\n- The number of failed indexes in the Job exceeded the specified\n`spec.maxFailedIndexes`. For details, see [Backoff limit per index](#backoff-limit-per-index)\n- A failed Pod matches a rule in `.spec.podFailurePolicy` that has the `FailJob`\naction. For details about how Pod failure policy rules might affect failure\nevaluation, see [Pod failure policy](#pod-failure-policy).  \nJobs succeed for the following reasons:\n- The number of succeeded Pods reached the specified `.spec.completions`\n- The criteria specified in `.spec.successPolicy` are met. For details, see\n[Success policy](#success-policy).  \nIn Kubernetes v1.31 and later the Job controller delays the addition of the\nterminal conditions,`Failed` or `Complete`, until all of the Job Pods are terminated.  \nIn Kubernetes v1.30 and earlier, the Job controller added the `Complete` or the\n`Failed` Job terminal conditions as soon as the Job termination process was\ntriggered and all Pod finalizers were removed. However, some Pods would still\nbe running or terminating at the moment that the terminal condition was added.  \nIn Kubernetes v1.31 and later, the controller only adds the Job terminal conditions\n_after_ all of the Pods are terminated. You can enable this behavior by using the\n`JobManagedBy` or the `JobPodReplacementPolicy` (enabled by default)\n[feature gates](/docs/reference/command-line-tools-reference/feature-gates/).\n"
  },
  {
    "question": "How do I set the Pod Selector for a StatefulSet?",
    "answer": "You must set the `.spec.selector` field of a StatefulSet to match the labels of its `.spec.template.metadata.labels`.",
    "uuid": "5ad2f4f1-9448-402f-8c0d-7bbb9c95db82",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set the Pod Selector for a StatefulSet?\nThis is about the following runbook:\nRunbook Title: Pod Selector\nRunbook Content: ComponentsPod SelectorYou must set the `.spec.selector` field of a StatefulSet to match the labels of its\n`.spec.template.metadata.labels`. Failing to specify a matching Pod Selector will result in a\nvalidation error during StatefulSet creation.\n"
  },
  {
    "question": "Will my applications that need to run as root work with user namespaces activated?",
    "answer": "Yes, most applications that need to run as root but don't access other host namespaces or resources should continue to run fine without any changes needed if user namespaces is activated.",
    "uuid": "7598d4f3-b249-4018-b9c3-ee61832205b4",
    "question_with_context": "A user asked the following question:\nQuestion: Will my applications that need to run as root work with user namespaces activated?\nThis is about the following runbook:\nRunbook Title: Introduction\nRunbook Content: IntroductionUser namespaces is a Linux feature that allows to map users in the container to\ndifferent users in the host. Furthermore, the capabilities granted to a pod in\na user namespace are valid only in the namespace and void outside of it.  \nA pod can opt-in to use user namespaces by setting the `pod.spec.hostUsers` field\nto `false`.  \nThe kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way\nto guarantee that no two pods on the same node use the same mapping.  \nThe `runAsUser`, `runAsGroup`, `fsGroup`, etc. fields in the `pod.spec` always\nrefer to the user inside the container.  \nThe valid UIDs/GIDs when this feature is enabled is the range 0-65535. This\napplies to files and processes (`runAsUser`, `runAsGroup`, etc.).  \nFiles using a UID/GID outside this range will be seen as belonging to the\noverflow ID, usually 65534 (configured in `/proc/sys/kernel/overflowuid` and\n`/proc/sys/kernel/overflowgid`). However, it is not possible to modify those\nfiles, even by running as the 65534 user/group.  \nMost applications that need to run as root but don't access other host\nnamespaces or resources, should continue to run fine without any changes needed\nif user namespaces is activated.\n"
  },
  {
    "question": "Can I set the TTL for already finished Jobs?",
    "answer": "Yes, you can manually set the `.spec.ttlSecondsAfterFinished` field for existing, already finished Jobs to make them eligible for cleanup.",
    "uuid": "dacd1f0c-3d62-41d8-b508-7033d1771c37",
    "question_with_context": "A user asked the following question:\nQuestion: Can I set the TTL for already finished Jobs?\nThis is about the following runbook:\nRunbook Title: Cleanup for finished Jobs\nRunbook Content: Cleanup for finished JobsThe TTL-after-finished controller is only supported for Jobs. You can use this mechanism to clean\nup finished Jobs (either `Complete` or `Failed`) automatically by specifying the\n`.spec.ttlSecondsAfterFinished` field of a Job, as in this\n[example](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically).  \nThe TTL-after-finished controller assumes that a Job is eligible to be cleaned up\nTTL seconds after the Job has finished. The timer starts once the\nstatus condition of the Job changes to show that the Job is either `Complete` or `Failed`; once the TTL has\nexpired, that Job becomes eligible for\n[cascading](/docs/concepts/architecture/garbage-collection/#cascading-deletion) removal. When the\nTTL-after-finished controller cleans up a job, it will delete it cascadingly, that is to say it will delete\nits dependent objects together with it.  \nKubernetes honors object lifecycle guarantees on the Job, such as waiting for\n[finalizers](/docs/concepts/overview/working-with-objects/finalizers/).  \nYou can set the TTL seconds at any time. Here are some examples for setting the\n`.spec.ttlSecondsAfterFinished` field of a Job:  \n* Specify this field in the Job manifest, so that a Job can be cleaned up\nautomatically some time after it finishes.\n* Manually set this field of existing, already finished Jobs, so that they become eligible\nfor cleanup.\n* Use a\n[mutating admission webhook](/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\nto set this field dynamically at Job creation time. Cluster administrators can\nuse this to enforce a TTL policy for finished jobs.\n* Use a\n[mutating admission webhook](/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\nto set this field dynamically after the Job has finished, and choose\ndifferent TTL values based on job status, labels. For this case, the webhook needs\nto detect changes to the `.status` of the Job and only set a TTL when the Job\nis being marked as completed.\n* Write your own controller to manage the cleanup TTL for Jobs that match a particular\n{{< glossary_tooltip term_id=\"selector\" text=\"selector\" >}}.\n"
  },
  {
    "question": "How can I learn about Pods in Kubernetes?",
    "answer": "You can learn about Pods by visiting the [Pods documentation](/docs/concepts/workloads/pods).",
    "uuid": "d6335d1a-f4b1-4ed2-8c7e-6e8bd67b8dd2",
    "question_with_context": "A user asked the following question:\nQuestion: How can I learn about Pods in Kubernetes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Learn about [Deployment](/docs/concepts/workloads/controllers/deployment/), the replacement\nfor ReplicationController.\n* `ReplicationController` is part of the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/replication-controller-v1\" >}}\nobject definition to understand the API for replication controllers.\n"
  },
  {
    "question": "How does the ReplicationController ensure the number of pods is correct?",
    "answer": "The ReplicationController ensures that the desired number of pods matches its label selector and are operational, excluding only terminated pods from its count.",
    "uuid": "dab3cd51-b68c-4b3b-b0a9-3a50f33d293d",
    "question_with_context": "A user asked the following question:\nQuestion: How does the ReplicationController ensure the number of pods is correct?\nThis is about the following runbook:\nRunbook Title: Responsibilities of the ReplicationController\nRunbook Content: Responsibilities of the ReplicationControllerThe ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, [readiness](https://issue.k8s.io/620) and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.  \nThe ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in [#492](https://issue.k8s.io/492)), which would change its `replicas` field. We will not add scheduling policies (for example, [spreading](https://issue.k8s.io/367#issuecomment-48428019)) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation ([#170](https://issue.k8s.io/170)).  \nThe ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The \"macro\" operations currently supported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like [Asgard](https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1) managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.\n"
  },
  {
    "question": "How should I handle labels in the Pod Template?",
    "answer": "When specifying labels in the Pod Template, make sure they do not overlap with labels used by other controllers.",
    "uuid": "79d0fde0-3a05-4c97-b7bc-4789e1110272",
    "question_with_context": "A user asked the following question:\nQuestion: How should I handle labels in the Pod Template?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a Deployment SpecPod TemplateThe `.spec.template` and `.spec.selector` are the only required fields of the `.spec`.  \nThe `.spec.template` is a [Pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}, except it is nested and does not have an `apiVersion` or `kind`.  \nIn addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate\nlabels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [selector](#selector).  \nOnly a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is\nallowed, which is the default if not specified.\n"
  },
  {
    "question": "Can I prevent involuntary disruptions with a PodDisruptionBudget?",
    "answer": "No, involuntary disruptions cannot be prevented by PodDisruptionBudgets; however, they do count against the budget.",
    "uuid": "406c8f76-ccbd-4a8a-a0a6-ebce8ce90ef2",
    "question_with_context": "A user asked the following question:\nQuestion: Can I prevent involuntary disruptions with a PodDisruptionBudget?\nThis is about the following runbook:\nRunbook Title: Pod disruption budgets\nRunbook Content: Pod disruption budgets{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}  \nKubernetes offers features to help you run highly available applications even when you\nintroduce frequent voluntary disruptions.  \nAs an application owner, you can create a PodDisruptionBudget (PDB) for each application.\nA PDB limits the number of Pods of a replicated application that are down simultaneously from\nvoluntary disruptions. For example, a quorum-based application would\nlike to ensure that the number of replicas running is never brought below the\nnumber needed for a quorum. A web front end might want to\nensure that the number of replicas serving load never falls below a certain\npercentage of the total.  \nCluster managers and hosting providers should use tools which\nrespect PodDisruptionBudgets by calling the [Eviction API](/docs/tasks/administer-cluster/safely-drain-node/#eviction-api)\ninstead of directly deleting pods or deployments.  \nFor example, the `kubectl drain` subcommand lets you mark a node as going out of\nservice. When you run `kubectl drain`, the tool tries to evict all of the Pods on\nthe Node you're taking out of service. The eviction request that `kubectl` submits on\nyour behalf may be temporarily rejected, so the tool periodically retries all failed\nrequests until all Pods on the target node are terminated, or until a configurable timeout\nis reached.  \nA PDB specifies the number of replicas that an application can tolerate having, relative to how\nmany it is intended to have.  For example, a Deployment which has a `.spec.replicas: 5` is\nsupposed to have 5 pods at any given time.  If its PDB allows for there to be 4 at a time,\nthen the Eviction API will allow voluntary disruption of one (but not two) pods at a time.  \nThe group of pods that comprise the application is specified using a label selector, the same\nas the one used by the application's controller (deployment, stateful-set, etc).  \nThe \"intended\" number of pods is computed from the `.spec.replicas` of the workload resource\nthat is managing those pods. The control plane discovers the owning workload resource by\nexamining the `.metadata.ownerReferences` of the Pod.  \n[Involuntary disruptions](#voluntary-and-involuntary-disruptions) cannot be prevented by PDBs; however they\ndo count against the budget.  \nPods which are deleted or unavailable due to a rolling upgrade to an application do count\nagainst the disruption budget, but workload resources (such as Deployment and StatefulSet)\nare not limited by PDBs when doing rolling upgrades. Instead, the handling of failures\nduring application updates is configured in the spec for the specific workload resource.  \nIt is recommended to set `AlwaysAllow` [Unhealthy Pod Eviction Policy](/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy)\nto your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain.\nThe default behavior is to wait for the application pods to become [healthy](/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod)\nbefore the drain can proceed.  \nWhen a pod is evicted using the eviction API, it is gracefully\n[terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination), honoring the\n`terminationGracePeriodSeconds` setting in its [PodSpec](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podspec-v1-core).\n"
  },
  {
    "question": "Why should I use a ReplicationController even for a single pod application?",
    "answer": "You should use a ReplicationController because it ensures that your pod is automatically re-created on a node after disruptions, such as maintenance.",
    "uuid": "c9f7a9ca-2485-4edb-848e-3b8a868e4a72",
    "question_with_context": "A user asked the following question:\nQuestion: Why should I use a ReplicationController even for a single pod application?\nThis is about the following runbook:\nRunbook Title: How a ReplicationController works\nRunbook Content: How a ReplicationController worksIf there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the\nReplicationController starts more pods. Unlike manually created pods, the pods maintained by a\nReplicationController are automatically replaced if they fail, are deleted, or are terminated.\nFor example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.\nFor this reason, you should use a ReplicationController even if your application requires\nonly a single pod. A ReplicationController is similar to a process supervisor,\nbut instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods\nacross multiple nodes.  \nReplicationController is often abbreviated to \"rc\" in discussion, and as a shortcut in\nkubectl commands.  \nA simple case is to create one ReplicationController object to reliably run one instance of\na Pod indefinitely.  A more complex use case is to run several identical replicas of a replicated\nservice, such as web servers.\n"
  },
  {
    "question": "If I want to update my pods to a new spec, what's the best way to do it?",
    "answer": "To update pods to a new spec in a controlled way, you should use a rolling update.",
    "uuid": "4dec0fe8-2314-4cc2-a958-3091f5cb06af",
    "question_with_context": "A user asked the following question:\nQuestion: If I want to update my pods to a new spec, what's the best way to do it?\nThis is about the following runbook:\nRunbook Title: Deleting only a ReplicationController\nRunbook Content: Working with ReplicationControllersDeleting only a ReplicationControllerYou can delete a ReplicationController without affecting any of its pods.  \nUsing kubectl, specify the `--cascade=orphan` option to [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete).  \nWhen using the REST API or [client library](/docs/reference/using-api/client-libraries), you can delete the ReplicationController object.  \nOnce the original is deleted, you can create a new ReplicationController to replace it.  As long\nas the old and new `.spec.selector` are the same, then the new one will adopt the old pods.\nHowever, it will not make any effort to make existing pods match a new, different pod template.\nTo update pods to a new spec in a controlled way, use a [rolling update](#rolling-updates).\n"
  },
  {
    "question": "How do I ensure my init container runs for the entire life of the Pod?",
    "answer": "Set the `restartPolicy` of the init container to `Always`.",
    "uuid": "368d57e7-c672-4efa-baef-63b4d894534b",
    "question_with_context": "A user asked the following question:\nQuestion: How do I ensure my init container runs for the entire life of the Pod?\nThis is about the following runbook:\nRunbook Title: Sidecar containers and Pod lifecycle\nRunbook Content: Sidecar containers and Pod lifecycleIf an init container is created with its `restartPolicy` set to `Always`, it will\nstart and remain running during the entire life of the Pod. This can be helpful for\nrunning supporting services separated from the main application containers.  \nIf a `readinessProbe` is specified for this init container, its result will be used\nto determine the `ready` state of the Pod.  \nSince these containers are defined as init containers, they benefit from the same\nordering and sequential guarantees as regular init containers, allowing you to mix\nsidecar containers with regular init containers for complex Pod initialization flows.  \nCompared to regular init containers, sidecars defined within `initContainers` continue to\nrun after they have started. This is important when there is more than one entry inside\n`.spec.initContainers` for a Pod. After a sidecar-style init container is running (the kubelet\nhas set the `started` status for that init container to true), the kubelet then starts the\nnext init container from the ordered `.spec.initContainers` list.\nThat status either becomes true because there is a process running in the\ncontainer and no startup probe defined, or as a result of its `startupProbe` succeeding.  \nUpon Pod [termination](/docs/concepts/workloads/pods/pod-lifecycle/#termination-with-sidecars),\nthe kubelet postpones terminating sidecar containers until the main application container has fully stopped.\nThe sidecar containers are then shut down in the opposite order of their appearance in the Pod specification.\nThis approach ensures that the sidecars remain operational, supporting other containers within the Pod,\nuntil their service is no longer required.\n"
  },
  {
    "question": "What if I don't specify a StorageClass for my PersistentVolume?",
    "answer": "If no StorageClass is specified, then the default StorageClass will be used.",
    "uuid": "a0fc18b0-a652-431d-a065-7110d845ad5f",
    "question_with_context": "A user asked the following question:\nQuestion: What if I don't specify a StorageClass for my PersistentVolume?\nThis is about the following runbook:\nRunbook Title: Stable Storage\nRunbook Content: Pod IdentityStable StorageFor each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one\nPersistentVolumeClaim. In the nginx example above, each Pod receives a single PersistentVolume\nwith a StorageClass of `my-storage-class` and 1 GiB of provisioned storage. If no StorageClass\nis specified, then the default StorageClass will be used. When a Pod is (re)scheduled\nonto a node, its `volumeMounts` mount the PersistentVolumes associated with its\nPersistentVolume Claims. Note that, the PersistentVolumes associated with the\nPods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.\nThis must be done manually.\n"
  },
  {
    "question": "What should I do if I need to discover Pods quickly after they are created?",
    "answer": "If you need to discover Pods quickly after they are created, you can decrease the caching time in your Kubernetes DNS provider's config map, or use a watch on the Kubernetes API.",
    "uuid": "c62daafb-5e74-4722-ad2c-6e5b31d438a0",
    "question_with_context": "A user asked the following question:\nQuestion: What should I do if I need to discover Pods quickly after they are created?\nThis is about the following runbook:\nRunbook Title: Stable Network ID\nRunbook Content: Pod IdentityStable Network IDEach Pod in a StatefulSet derives its hostname from the name of the StatefulSet\nand the ordinal of the Pod. The pattern for the constructed hostname\nis `$(statefulset name)-$(ordinal)`. The example above will create three Pods\nnamed `web-0,web-1,web-2`.\nA StatefulSet can use a [Headless Service](/docs/concepts/services-networking/service/#headless-services)\nto control the domain of its Pods. The domain managed by this Service takes the form:\n`$(service name).$(namespace).svc.cluster.local`, where \"cluster.local\" is the\ncluster domain.\nAs each Pod is created, it gets a matching DNS subdomain, taking the form:\n`$(podname).$(governing service domain)`, where the governing service is defined\nby the `serviceName` field on the StatefulSet.  \nDepending on how DNS is configured in your cluster, you may not be able to look up the DNS\nname for a newly-run Pod immediately. This behavior can occur when other clients in the\ncluster have already sent queries for the hostname of the Pod before it was created.\nNegative caching (normal in DNS) means that the results of previous failed lookups are\nremembered and reused, even after the Pod is running, for at least a few seconds.  \nIf you need to discover Pods promptly after they are created, you have a few options:  \n- Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.\n- Decrease the time of caching in your Kubernetes DNS provider (typically this means editing the\nconfig map for CoreDNS, which currently caches for 30 seconds).  \nAs mentioned in the [limitations](#limitations) section, you are responsible for\ncreating the [Headless Service](/docs/concepts/services-networking/service/#headless-services)\nresponsible for the network identity of the pods.  \nHere are some examples of choices for Cluster Domain, Service name,\nStatefulSet name, and how that affects the DNS names for the StatefulSet's Pods.  \nCluster Domain | Service (ns/name) | StatefulSet (ns/name)  | StatefulSet Domain  | Pod DNS | Pod Hostname |\n-------------- | ----------------- | ----------------- | -------------- | ------- | ------------ |\ncluster.local | default/nginx     | default/web       | nginx.default.svc.cluster.local | web-{0..N-1}.nginx.default.svc.cluster.local | web-{0..N-1} |\ncluster.local | foo/nginx         | foo/web           | nginx.foo.svc.cluster.local     | web-{0..N-1}.nginx.foo.svc.cluster.local     | web-{0..N-1} |\nkube.local    | foo/nginx         | foo/web           | nginx.foo.svc.kube.local        | web-{0..N-1}.nginx.foo.svc.kube.local        | web-{0..N-1} |  \n{{< note >}}\nCluster Domain will be set to `cluster.local` unless\n[otherwise configured](/docs/concepts/services-networking/dns-pod-service/).\n{{< /note >}}\n"
  },
  {
    "question": "What is the pattern for constructing the hostname of a Pod in a StatefulSet?",
    "answer": "The hostname of a Pod in a StatefulSet is constructed using the pattern `$(statefulset name)-$(ordinal)`, resulting in names like `web-0`, `web-1`, `web-2`.",
    "uuid": "c62daafb-5e74-4722-ad2c-6e5b31d438a0",
    "question_with_context": "A user asked the following question:\nQuestion: What is the pattern for constructing the hostname of a Pod in a StatefulSet?\nThis is about the following runbook:\nRunbook Title: Stable Network ID\nRunbook Content: Pod IdentityStable Network IDEach Pod in a StatefulSet derives its hostname from the name of the StatefulSet\nand the ordinal of the Pod. The pattern for the constructed hostname\nis `$(statefulset name)-$(ordinal)`. The example above will create three Pods\nnamed `web-0,web-1,web-2`.\nA StatefulSet can use a [Headless Service](/docs/concepts/services-networking/service/#headless-services)\nto control the domain of its Pods. The domain managed by this Service takes the form:\n`$(service name).$(namespace).svc.cluster.local`, where \"cluster.local\" is the\ncluster domain.\nAs each Pod is created, it gets a matching DNS subdomain, taking the form:\n`$(podname).$(governing service domain)`, where the governing service is defined\nby the `serviceName` field on the StatefulSet.  \nDepending on how DNS is configured in your cluster, you may not be able to look up the DNS\nname for a newly-run Pod immediately. This behavior can occur when other clients in the\ncluster have already sent queries for the hostname of the Pod before it was created.\nNegative caching (normal in DNS) means that the results of previous failed lookups are\nremembered and reused, even after the Pod is running, for at least a few seconds.  \nIf you need to discover Pods promptly after they are created, you have a few options:  \n- Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.\n- Decrease the time of caching in your Kubernetes DNS provider (typically this means editing the\nconfig map for CoreDNS, which currently caches for 30 seconds).  \nAs mentioned in the [limitations](#limitations) section, you are responsible for\ncreating the [Headless Service](/docs/concepts/services-networking/service/#headless-services)\nresponsible for the network identity of the pods.  \nHere are some examples of choices for Cluster Domain, Service name,\nStatefulSet name, and how that affects the DNS names for the StatefulSet's Pods.  \nCluster Domain | Service (ns/name) | StatefulSet (ns/name)  | StatefulSet Domain  | Pod DNS | Pod Hostname |\n-------------- | ----------------- | ----------------- | -------------- | ------- | ------------ |\ncluster.local | default/nginx     | default/web       | nginx.default.svc.cluster.local | web-{0..N-1}.nginx.default.svc.cluster.local | web-{0..N-1} |\ncluster.local | foo/nginx         | foo/web           | nginx.foo.svc.cluster.local     | web-{0..N-1}.nginx.foo.svc.cluster.local     | web-{0..N-1} |\nkube.local    | foo/nginx         | foo/web           | nginx.foo.svc.kube.local        | web-{0..N-1}.nginx.foo.svc.kube.local        | web-{0..N-1} |  \n{{< note >}}\nCluster Domain will be set to `cluster.local` unless\n[otherwise configured](/docs/concepts/services-networking/dns-pod-service/).\n{{< /note >}}\n"
  },
  {
    "question": "What happens when I set memory limits for a container in Kubernetes with Memory QoS?",
    "answer": "When you set memory limits for a container, Memory QoS uses `memory.high` to throttle the workload as it approaches its memory limit, preventing the system from being overwhelmed by instantaneous memory allocation.",
    "uuid": "1c557ec4-1ee2-4d85-b0ed-2c405ce09422",
    "question_with_context": "A user asked the following question:\nQuestion: What happens when I set memory limits for a container in Kubernetes with Memory QoS?\nThis is about the following runbook:\nRunbook Title: Memory QoS with cgroup v2\nRunbook Content: Memory QoS with cgroup v2{{< feature-state feature_gate_name=\"MemoryQoS\" >}}  \nMemory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes.\nMemory requests and limits of containers in pod are used to set specific interfaces `memory.min`\nand `memory.high` provided by the memory controller. When `memory.min` is set to memory requests,\nmemory resources are reserved and never reclaimed by the kernel; this is how Memory QoS ensures\nmemory availability for Kubernetes pods. And if memory limits are set in the container,\nthis means that the system needs to limit container memory usage; Memory QoS uses `memory.high`\nto throttle workload approaching its memory limit, ensuring that the system is not overwhelmed\nby instantaneous memory allocation.  \nMemory QoS relies on QoS class to determine which settings to apply; however, these are different\nmechanisms that both provide controls over quality of service.\n"
  },
  {
    "question": "Should I set `.spec.replicas` if I'm using a HorizontalPodAutoscaler?",
    "answer": "No, if a HorizontalPodAutoscaler is managing scaling for a StatefulSet, you should not set `.spec.replicas`. Instead, let the Kubernetes control plane manage it automatically.",
    "uuid": "75a7ed8b-506f-4f6e-be40-43fc636dd2c9",
    "question_with_context": "A user asked the following question:\nQuestion: Should I set `.spec.replicas` if I'm using a HorizontalPodAutoscaler?\nThis is about the following runbook:\nRunbook Title: Replicas\nRunbook Content: PersistentVolumeClaim retentionReplicas`.spec.replicas` is an optional field that specifies the number of desired Pods. It defaults to 1.  \nShould you manually scale a deployment, example via `kubectl scale\nstatefulset statefulset --replicas=X`, and then you update that StatefulSet\nbased on a manifest (for example: by running `kubectl apply -f\nstatefulset.yaml`), then applying that manifest overwrites the manual scaling\nthat you previously did.  \nIf a [HorizontalPodAutoscaler](/docs/tasks/run-application/horizontal-pod-autoscale/)\n(or any similar API for horizontal scaling) is managing scaling for a\nStatefulset, don't set `.spec.replicas`. Instead, allow the Kubernetes\n{{<glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} to manage\nthe `.spec.replicas` field automatically.\n"
  },
  {
    "question": "What do I need to include in the Pod Template for a Deployment?",
    "answer": "In the Pod Template for a Deployment, you need to include the `.spec.template` and `.spec.selector` fields, along with appropriate labels and a restart policy.",
    "uuid": "79d0fde0-3a05-4c97-b7bc-4789e1110272",
    "question_with_context": "A user asked the following question:\nQuestion: What do I need to include in the Pod Template for a Deployment?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a Deployment SpecPod TemplateThe `.spec.template` and `.spec.selector` are the only required fields of the `.spec`.  \nThe `.spec.template` is a [Pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}, except it is nested and does not have an `apiVersion` or `kind`.  \nIn addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate\nlabels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [selector](#selector).  \nOnly a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is\nallowed, which is the default if not specified.\n"
  },
  {
    "question": "Can Guaranteed Pods use exclusive CPUs?",
    "answer": "Yes, Guaranteed Pods can make use of exclusive CPUs using the static CPU management policy.",
    "uuid": "5fc3e6b9-e905-4fff-952b-2870c0a4b958",
    "question_with_context": "A user asked the following question:\nQuestion: Can Guaranteed Pods use exclusive CPUs?\nThis is about the following runbook:\nRunbook Title: Guaranteed\nRunbook Content: Quality of Service classesGuaranteedPods that are `Guaranteed` have the strictest resource limits and are least likely\nto face eviction. They are guaranteed not to be killed until they exceed their limits\nor there are no lower-priority Pods that can be preempted from the Node. They may\nnot acquire resources beyond their specified limits. These Pods can also make\nuse of exclusive CPUs using the\n[`static`](/docs/tasks/administer-cluster/cpu-management-policies/#static-policy) CPU management policy.  \n#### Criteria  \nFor a Pod to be given a QoS class of `Guaranteed`:  \n* Every Container in the Pod must have a memory limit and a memory request.\n* For every Container in the Pod, the memory limit must equal the memory request.\n* Every Container in the Pod must have a CPU limit and a CPU request.\n* For every Container in the Pod, the CPU limit must equal the CPU request.\n"
  },
  {
    "question": "What happens to Jobs that are already running when I suspend a CronJob?",
    "answer": "Suspending a CronJob does not affect Jobs that the CronJob has already started.",
    "uuid": "5f135532-14e0-47b1-80c2-114f6d01696e",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to Jobs that are already running when I suspend a CronJob?\nThis is about the following runbook:\nRunbook Title: Schedule suspension\nRunbook Content: Writing a CronJob specSchedule suspensionYou can suspend execution of Jobs for a CronJob, by setting the optional `.spec.suspend` field\nto true. The field defaults to false.  \nThis setting does _not_ affect Jobs that the CronJob has already started.  \nIf you do set that field to true, all subsequent executions are suspended (they remain\nscheduled, but the CronJob controller does not start the Jobs to run the tasks) until\nyou unsuspend the CronJob.  \n{{< caution >}}\nExecutions that are suspended during their scheduled time count as missed Jobs.\nWhen `.spec.suspend` changes from `true` to `false` on an existing CronJob without a\n[starting deadline](#starting-deadline), the missed Jobs are scheduled immediately.\n{{< /caution >}}\n"
  },
  {
    "question": "What happens if I modify the Pod template in a workload resource?",
    "answer": "Modifying the Pod template does not affect existing Pods directly. The workload resource must create new replacement Pods that use the updated template to reflect the changes.",
    "uuid": "7ddf978c-8e14-4ae2-9438-f37ca848cb9b",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I modify the Pod template in a workload resource?\nThis is about the following runbook:\nRunbook Title: Pod templates\nRunbook Content: Working with PodsPod templatesControllers for {{< glossary_tooltip text=\"workload\" term_id=\"workload\" >}} resources create Pods\nfrom a _pod template_ and manage those Pods on your behalf.  \nPodTemplates are specifications for creating Pods, and are included in workload resources such as\n[Deployments](/docs/concepts/workloads/controllers/deployment/),\n[Jobs](/docs/concepts/workloads/controllers/job/), and\n[DaemonSets](/docs/concepts/workloads/controllers/daemonset/).  \nEach controller for a workload resource uses the `PodTemplate` inside the workload\nobject to make actual Pods. The `PodTemplate` is part of the desired state of whatever\nworkload resource you used to run your app.  \nWhen you create a Pod, you can include\n[environment variables](/docs/tasks/inject-data-application/define-environment-variable-container/)\nin the Pod template for the containers that run in the Pod.  \nThe sample below is a manifest for a simple Job with a `template` that starts one\ncontainer. The container in that Pod prints a message then pauses.  \n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: hello\nspec:\ntemplate:\n# This is the pod template\nspec:\ncontainers:\n- name: hello\nimage: busybox:1.28\ncommand: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600']\nrestartPolicy: OnFailure\n# The pod template ends here\n```  \nModifying the pod template or switching to a new pod template has no direct effect\non the Pods that already exist. If you change the pod template for a workload\nresource, that resource needs to create replacement Pods that use the updated template.  \nFor example, the StatefulSet controller ensures that the running Pods match the current\npod template for each StatefulSet object. If you edit the StatefulSet to change its pod\ntemplate, the StatefulSet starts to create new Pods based on the updated template.\nEventually, all of the old Pods are replaced with new Pods, and the update is complete.  \nEach workload resource implements its own rules for handling changes to the Pod template.\nIf you want to read more about StatefulSet specifically, read\n[Update strategy](/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets) in the StatefulSet Basics tutorial.  \nOn Nodes, the {{< glossary_tooltip term_id=\"kubelet\" text=\"kubelet\" >}} does not\ndirectly observe or manage any of the details around pod templates and updates; those\ndetails are abstracted away. That abstraction and separation of concerns simplifies\nsystem semantics, and makes it feasible to extend the cluster's behavior without\nchanging existing code.\n"
  },
  {
    "question": "How is the pod-template-hash label generated for a ReplicaSet?",
    "answer": "It is generated by hashing the `PodTemplate` of the ReplicaSet and using the resulting hash as the label value.",
    "uuid": "dcfab6c2-ad36-498e-9cb0-3172bb40bb58",
    "question_with_context": "A user asked the following question:\nQuestion: How is the pod-template-hash label generated for a ReplicaSet?\nThis is about the following runbook:\nRunbook Title: Pod-template-hash label\nRunbook Content: Creating a DeploymentPod-template-hash label{{< caution >}}\nDo not change this label.\n{{< /caution >}}  \nThe `pod-template-hash` label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.  \nThis label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the `PodTemplate` of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,\nand in any existing Pods that the ReplicaSet might have.\n"
  },
  {
    "question": "What prefix should I use for job labels in Kubernetes?",
    "answer": "You should use the `batch.kubernetes.io/` prefix for job labels.",
    "uuid": "13c6c55a-b9e2-4882-b8ba-820637b8834f",
    "question_with_context": "A user asked the following question:\nQuestion: What prefix should I use for job labels in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Job Labels\nRunbook Content: Writing a Job specJob LabelsJob labels will have `batch.kubernetes.io/` prefix for `job-name` and `controller-uid`.\n"
  },
  {
    "question": "How do I check the status of my Pod after applying the configuration?",
    "answer": "You can check the status of your Pod by running: `kubectl get -f myapp.yaml`.",
    "uuid": "18b36a5d-8d0c-4b23-9da6-e4603fc9fa2f",
    "question_with_context": "A user asked the following question:\nQuestion: How do I check the status of my Pod after applying the configuration?\nThis is about the following runbook:\nRunbook Title: Examples\nRunbook Content: Using init containersExamplesHere are some ideas for how to use init containers:  \n* Wait for a {{< glossary_tooltip text=\"Service\" term_id=\"service\">}} to\nbe created, using a shell one-line command like:\n```shell\nfor i in {1..100}; do sleep 1; if nslookup myservice; then exit 0; fi; done; exit 1\n```  \n* Register this Pod with a remote server from the downward API with a command like:\n```shell\ncurl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$(<POD_NAME>)&ip=$(<POD_IP>)'\n```  \n* Wait for some time before starting the app container with a command like\n```shell\nsleep 60\n```  \n* Clone a Git repository into a {{< glossary_tooltip text=\"Volume\" term_id=\"volume\" >}}  \n* Place values into a configuration file and run a template tool to dynamically\ngenerate a configuration file for the main app container. For example,\nplace the `POD_IP` value in a configuration and generate the main app\nconfiguration file using Jinja.  \n#### Init containers in use  \nThis example defines a simple Pod that has two init containers.\nThe first waits for `myservice`, and the second waits for `mydb`. Once both\ninit containers complete, the Pod runs the app container from its `spec` section.  \n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: myapp-pod\nlabels:\napp.kubernetes.io/name: MyApp\nspec:\ncontainers:\n- name: myapp-container\nimage: busybox:1.28\ncommand: ['sh', '-c', 'echo The app is running! && sleep 3600']\ninitContainers:\n- name: init-myservice\nimage: busybox:1.28\ncommand: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"]\n- name: init-mydb\nimage: busybox:1.28\ncommand: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"]\n```  \nYou can start this Pod by running:  \n```shell\nkubectl apply -f myapp.yaml\n```\nThe output is similar to this:\n```\npod/myapp-pod created\n```  \nAnd check on its status with:\n```shell\nkubectl get -f myapp.yaml\n```\nThe output is similar to this:\n```\nNAME        READY     STATUS     RESTARTS   AGE\nmyapp-pod   0/1       Init:0/2   0          6m\n```  \nor for more details:\n```shell\nkubectl describe -f myapp.yaml\n```\nThe output is similar to this:\n```\nName:          myapp-pod\nNamespace:     default\n[...]\nLabels:        app.kubernetes.io/name=MyApp\nStatus:        Pending\n[...]\nInit Containers:\ninit-myservice:\n[...]\nState:         Running\n[...]\ninit-mydb:\n[...]\nState:         Waiting\nReason:      PodInitializing\nReady:         False\n[...]\nContainers:\nmyapp-container:\n[...]\nState:         Waiting\nReason:      PodInitializing\nReady:         False\n[...]\nEvents:\nFirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message\n---------    --------    -----    ----                      -------------                           --------      ------        -------\n16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201\n16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image \"busybox\"\n13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image \"busybox\"\n13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container init-myservice\n13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container init-myservice\n```  \nTo see logs for the init containers in this Pod, run:\n```shell\nkubectl logs myapp-pod -c init-myservice # Inspect the first init container\nkubectl logs myapp-pod -c init-mydb      # Inspect the second init container\n```  \nAt this point, those init containers will be waiting to discover {{< glossary_tooltip text=\"Services\" term_id=\"service\" >}} named\n`mydb` and `myservice`.  \nHere's a configuration you can use to make those Services appear:  \n```yaml\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: myservice\nspec:\nports:\n- protocol: TCP\nport: 80\ntargetPort: 9376\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: mydb\nspec:\nports:\n- protocol: TCP\nport: 80\ntargetPort: 9377\n```  \nTo create the `mydb` and `myservice` services:  \n```shell\nkubectl apply -f services.yaml\n```\nThe output is similar to this:\n```\nservice/myservice created\nservice/mydb created\n```  \nYou'll then see that those init containers complete, and that the `myapp-pod`\nPod moves into the Running state:  \n```shell\nkubectl get -f myapp.yaml\n```\nThe output is similar to this:\n```\nNAME        READY     STATUS    RESTARTS   AGE\nmyapp-pod   1/1       Running   0          9m\n```  \nThis simple example should provide some inspiration for you to create your own\ninit containers. [What's next](#what-s-next) contains a link to a more detailed example.\n"
  },
  {
    "question": "Why should we separate Cluster Owner and Application Owner roles?",
    "answer": "Separating these roles is useful when there are many application teams sharing a Kubernetes cluster, allowing for natural specialization of roles, or when third-party tools or services are used to automate cluster management.",
    "uuid": "09bf4d01-d0fc-43ec-bbf5-1007b6d6c2e5",
    "question_with_context": "A user asked the following question:\nQuestion: Why should we separate Cluster Owner and Application Owner roles?\nThis is about the following runbook:\nRunbook Title: Separating Cluster Owner and Application Owner Roles\nRunbook Content: Separating Cluster Owner and Application Owner RolesOften, it is useful to think of the Cluster Manager\nand Application Owner as separate roles with limited knowledge\nof each other.   This separation of responsibilities\nmay make sense in these scenarios:  \n- when there are many application teams sharing a Kubernetes cluster, and\nthere is natural specialization of roles\n- when third-party tools or services are used to automate cluster management  \nPod Disruption Budgets support this separation of roles by providing an\ninterface between the roles.  \nIf you do not have such a separation of responsibilities in your organization,\nyou may not need to use Pod Disruption Budgets.\n"
  },
  {
    "question": "What components make up the unique identity of StatefulSet Pods?",
    "answer": "A unique identity consists of an ordinal, a stable network identity, and stable storage.",
    "uuid": "4e7ec3e4-16f8-4a73-be68-962c085e83d2",
    "question_with_context": "A user asked the following question:\nQuestion: What components make up the unique identity of StatefulSet Pods?\nThis is about the following runbook:\nRunbook Title: Pod Identity\nRunbook Content: Pod IdentityStatefulSet Pods have a unique identity that consists of an ordinal, a\nstable network identity, and stable storage. The identity sticks to the Pod,\nregardless of which node it's (re)scheduled on.\n"
  },
  {
    "question": "How can I manage a stateless application workload in Kubernetes?",
    "answer": "You can manage a stateless application workload in Kubernetes using a Deployment, which is the most common way to run an application on your cluster. Deployments allow any Pod in the Deployment to be interchangeable and can be replaced if needed.",
    "uuid": "07f756fa-8145-40e5-a51d-263eebd4db85",
    "question_with_context": "A user asked the following question:\nQuestion: How can I manage a stateless application workload in Kubernetes?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: \"Workload Management\"\nweight: 20\nsimple_list: true\n---  \nKubernetes provides several built-in APIs for declarative management of your\n{{< glossary_tooltip text=\"workloads\" term_id=\"workload\" >}}\nand the components of those workloads.  \nUltimately, your applications run as containers inside\n{{< glossary_tooltip term_id=\"Pod\" text=\"Pods\" >}}; however, managing individual\nPods would be a lot of effort. For example, if a Pod fails, you probably want to\nrun a new Pod to replace it. Kubernetes can do that for you.  \nYou use the Kubernetes API to create a workload\n{{< glossary_tooltip text=\"object\" term_id=\"object\" >}} that represents a higher abstraction level\nthan a Pod, and then the Kubernetes\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} automatically manages\nPod objects on your behalf, based on the specification for the workload object you defined.  \nThe built-in APIs for managing workloads are:  \n[Deployment](/docs/concepts/workloads/controllers/deployment/) (and, indirectly, [ReplicaSet](/docs/concepts/workloads/controllers/replicaset/)),\nthe most common way to run an application on your cluster.\nDeployment is a good fit for managing a stateless application workload on your cluster, where\nany Pod in the Deployment is interchangeable and can be replaced if needed.\n(Deployments are a replacement for the legacy\n{{< glossary_tooltip text=\"ReplicationController\" term_id=\"replication-controller\" >}} API).  \nA [StatefulSet](/docs/concepts/workloads/controllers/statefulset/) lets you\nmanage one or more Pods \u2013 all running the same application code \u2013 where the Pods rely\non having a distinct identity. This is different from a Deployment where the Pods are\nexpected to be interchangeable.\nThe most common use for a StatefulSet is to be able to make a link between its Pods and\ntheir persistent storage. For example, you can run a StatefulSet that associates each Pod\nwith a [PersistentVolume](/docs/concepts/storage/persistent-volumes/). If one of the Pods\nin the StatefulSet fails, Kubernetes makes a replacement Pod that is connected to the\nsame PersistentVolume.  \nA [DaemonSet](/docs/concepts/workloads/controllers/daemonset/) defines Pods that provide\nfacilities that are local to a specific {{< glossary_tooltip text=\"node\" term_id=\"node\" >}};\nfor example, a driver that lets containers on that node access a storage system. You use a DaemonSet\nwhen the driver, or other node-level service, has to run on the node where it's useful.\nEach Pod in a DaemonSet performs a role similar to a system daemon on a classic Unix / POSIX\nserver.\nA DaemonSet might be fundamental to the operation of your cluster,\nsuch as a plugin to let that node access\n[cluster networking](/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-network-model),\nit might help you to manage the node,\nor it could provide less essential facilities that enhance the container platform you are running.\nYou can run DaemonSets (and their pods) across every node in your cluster, or across just a subset (for example,\nonly install the GPU accelerator driver on nodes that have a GPU installed).  \nYou can use a [Job](/docs/concepts/workloads/controllers/job/) and / or\na [CronJob](/docs/concepts/workloads/controllers/cron-jobs/) to\ndefine tasks that run to completion and then stop. A Job represents a one-off task,\nwhereas each CronJob repeats according to a schedule.  \nOther topics in this section:\n<!-- relies on simple_list: true in the front matter -->\n"
  },
  {
    "question": "What does the CronJob controller do if there are more than 100 missed schedules?",
    "answer": "If there are more than 100 missed schedules, the CronJob controller does not start the Job and logs an error.",
    "uuid": "7d4703a0-9f08-4c44-a075-a81eca9a01dc",
    "question_with_context": "A user asked the following question:\nQuestion: What does the CronJob controller do if there are more than 100 missed schedules?\nThis is about the following runbook:\nRunbook Title: Job creation\nRunbook Content: CronJob limitations {#cron-job-limitations}Job creationA CronJob creates a Job object approximately once per execution time of its schedule.\nThe scheduling is approximate because there\nare certain circumstances where two Jobs might be created, or no Job might be created.\nKubernetes tries to avoid those situations, but does not completely prevent them. Therefore,\nthe Jobs that you define should be _idempotent_.  \nIf `startingDeadlineSeconds` is set to a large value or left unset (the default)\nand if `concurrencyPolicy` is set to `Allow`, the Jobs will always run\nat least once.  \n{{< caution >}}\nIf `startingDeadlineSeconds` is set to a value less than 10 seconds, the CronJob may not be scheduled. This is because the CronJob controller checks things every 10 seconds.\n{{< /caution >}}  \nFor every CronJob, the CronJob {{< glossary_tooltip term_id=\"controller\" >}} checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the Job and logs the error.  \n```\nCannot determine if job needs to be started. Too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.\n```  \nIt is important to note that if the `startingDeadlineSeconds` field is set (not `nil`), the controller counts how many missed Jobs occurred from the value of `startingDeadlineSeconds` until now rather than from the last scheduled time until now. For example, if `startingDeadlineSeconds` is `200`, the controller counts how many missed Jobs occurred in the last 200 seconds.  \nA CronJob is counted as missed if it has failed to be created at its scheduled time. For example, if `concurrencyPolicy` is set to `Forbid` and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed.  \nFor example, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its\n`startingDeadlineSeconds` field is not set. If the CronJob controller happens to\nbe down from `08:29:00` to `10:21:00`, the Job will not start as the number of missed Jobs which missed their schedule is greater than 100.  \nTo illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its\n`startingDeadlineSeconds` is set to 200 seconds. If the CronJob controller happens to\nbe down for the same period as the previous example (`08:29:00` to `10:21:00`,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (i.e., 3 missed schedules), rather than from the last scheduled time until now.  \nThe CronJob is only responsible for creating Jobs that match its schedule, and\nthe Job in turn is responsible for the management of the Pods it represents.\n"
  },
  {
    "question": "How do I manage multiple Pods effectively?",
    "answer": "You can use workload resources like Deployments, StatefulSets, or DaemonSets to create and manage multiple Pods.",
    "uuid": "28a94df8-423a-40eb-98e2-4682e6e57c45",
    "question_with_context": "A user asked the following question:\nQuestion: How do I manage multiple Pods effectively?\nThis is about the following runbook:\nRunbook Title: Pods and controllers\nRunbook Content: Working with PodsPods and controllersYou can use workload resources to create and manage multiple Pods for you. A controller\nfor the resource handles replication and rollout and automatic healing in case of\nPod failure. For example, if a Node fails, a controller notices that Pods on that\nNode have stopped working and creates a replacement Pod. The scheduler places the\nreplacement Pod onto a healthy Node.  \nHere are some examples of workload resources that manage one or more Pods:  \n* {{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}\n* {{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}}\n* {{< glossary_tooltip text=\"DaemonSet\" term_id=\"daemonset\" >}}\n"
  },
  {
    "question": "Can I delay the scheduling of a Pod? How?",
    "answer": "Yes, you can use Pod Scheduling Readiness to delay scheduling for a Pod until all its scheduling gates are removed.",
    "uuid": "11910d67-4cf2-447c-8dc9-b8cfee385745",
    "question_with_context": "A user asked the following question:\nQuestion: Can I delay the scheduling of a Pod? How?\nThis is about the following runbook:\nRunbook Title: Pod lifetime\nRunbook Content: Pod lifetimeWhilst a Pod is running, the kubelet is able to restart containers to handle some\nkind of faults. Within a Pod, Kubernetes tracks different container\n[states](#container-states) and determines what action to take to make the Pod\nhealthy again.  \nIn the Kubernetes API, Pods have both a specification and an actual status. The\nstatus for a Pod object consists of a set of [Pod conditions](#pod-conditions).\nYou can also inject [custom readiness information](#pod-readiness-gate) into the\ncondition data for a Pod, if that is useful to your application.  \nPods are only [scheduled](/docs/concepts/scheduling-eviction/) once in their lifetime;\nassigning a Pod to a specific node is called _binding_, and the process of selecting\nwhich node to use is called _scheduling_.\nOnce a Pod has been scheduled and is bound to a node, Kubernetes tries\nto run that Pod on the node. The Pod runs on that node until it stops, or until the Pod\nis [terminated](#pod-termination); if Kubernetes isn't able to start the Pod on the selected\nnode (for example, if the node crashes before the Pod starts), then that particular Pod\nnever starts.  \nYou can use [Pod Scheduling Readiness](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/)\nto delay scheduling for a Pod until all its _scheduling gates_ are removed. For example,\nyou might want to define a set of Pods but only trigger scheduling once all the Pods\nhave been created.\n"
  },
  {
    "question": "How does a ReplicationController manage local container restarts?",
    "answer": "A ReplicationController delegates local container restarts to an agent on the node, such as the kubelet.",
    "uuid": "344dfacf-03ab-45ee-969f-d95a70718df7",
    "question_with_context": "A user asked the following question:\nQuestion: How does a ReplicationController manage local container restarts?\nThis is about the following runbook:\nRunbook Title: Bare Pods\nRunbook Content: Alternatives to ReplicationControllerBare PodsUnlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node.  A ReplicationController delegates local container restarts to some agent on the node, such as the kubelet.\n"
  },
  {
    "question": "Can BestEffort Pods use CPU resources that are not assigned to other Pods?",
    "answer": "Yes, `BestEffort` Pods can use node resources that aren't specifically assigned to Pods in other QoS classes. For example, if a node has available CPU cores after assigning some to a `Guaranteed` Pod, a `BestEffort` Pod can use the remaining cores.",
    "uuid": "e78046a9-5089-4963-892b-d25b03565051",
    "question_with_context": "A user asked the following question:\nQuestion: Can BestEffort Pods use CPU resources that are not assigned to other Pods?\nThis is about the following runbook:\nRunbook Title: BestEffort\nRunbook Content: Quality of Service classesBestEffortPods in the `BestEffort` QoS class can use node resources that aren't specifically assigned\nto Pods in other QoS classes. For example, if you have a node with 16 CPU cores available to the\nkubelet, and you assign 4 CPU cores to a `Guaranteed` Pod, then a Pod in the `BestEffort`\nQoS class can try to use any amount of the remaining 12 CPU cores.  \nThe kubelet prefers to evict `BestEffort` Pods if the node comes under resource pressure.  \n#### Criteria  \nA Pod has a QoS class of `BestEffort` if it doesn't meet the criteria for either `Guaranteed`\nor `Burstable`. In other words, a Pod is `BestEffort` only if none of the Containers in the Pod have a\nmemory limit or a memory request, and none of the Containers in the Pod have a\nCPU limit or a CPU request.\nContainers in a Pod can request other resources (not CPU or memory) and still be classified as\n`BestEffort`.\n"
  },
  {
    "question": "How do I set the PVC retention policy for a StatefulSet?",
    "answer": "You can set the PVC retention policy for a StatefulSet by configuring the optional `.spec.persistentVolumeClaimRetentionPolicy` field. You need to enable the `StatefulSetAutoDeletePVC` feature gate on the API server and the controller manager. Then, you can specify the `whenDeleted` and `whenScaled` policies, setting their values to either `Delete` or `Retain`.",
    "uuid": "587393e3-236c-4943-ba0f-3f3e589d8e1d",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set the PVC retention policy for a StatefulSet?\nThis is about the following runbook:\nRunbook Title: PersistentVolumeClaim retention\nRunbook Content: PersistentVolumeClaim retention{{< feature-state for_k8s_version=\"v1.27\" state=\"beta\" >}}  \nThe optional `.spec.persistentVolumeClaimRetentionPolicy` field controls if\nand how PVCs are deleted during the lifecycle of a StatefulSet. You must enable the\n`StatefulSetAutoDeletePVC` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\non the API server and the controller manager to use this field.\nOnce enabled, there are two policies you can configure for each StatefulSet:  \n`whenDeleted`\n: configures the volume retention behavior that applies when the StatefulSet is deleted  \n`whenScaled`\n: configures the volume retention behavior that applies when the replica count of\nthe StatefulSet   is reduced; for example, when scaling down the set.  \nFor each policy that you can configure, you can set the value to either `Delete` or `Retain`.  \n`Delete`\n: The PVCs created from the StatefulSet `volumeClaimTemplate` are deleted for each Pod\naffected by the policy. With the `whenDeleted` policy all PVCs from the\n`volumeClaimTemplate` are deleted after their Pods have been deleted. With the\n`whenScaled` policy, only PVCs corresponding to Pod replicas being scaled down are\ndeleted, after their Pods have been deleted.  \n`Retain` (default)\n: PVCs from the `volumeClaimTemplate` are not affected when their Pod is\ndeleted. This is the behavior before this new feature.  \nBear in mind that these policies **only** apply when Pods are being removed due to the\nStatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSet\nfails due to node failure, and the control plane creates a replacement Pod, the StatefulSet\nretains the existing PVC.  The existing volume is unaffected, and the cluster will attach it to\nthe node where the new Pod is about to launch.  \nThe default for policies is `Retain`, matching the StatefulSet behavior before this new feature.  \nHere is an example policy.  \n```yaml\napiVersion: apps/v1\nkind: StatefulSet\n...\nspec:\npersistentVolumeClaimRetentionPolicy:\nwhenDeleted: Retain\nwhenScaled: Delete\n...\n```  \nThe StatefulSet {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} adds\n[owner references](/docs/concepts/overview/working-with-objects/owners-dependents/#owner-references-in-object-specifications)\nto its PVCs, which are then deleted by the {{<glossary_tooltip text=\"garbage collector\"\nterm_id=\"garbage-collection\">}} after the Pod is terminated. This enables the Pod to\ncleanly unmount all volumes before the PVCs are deleted (and before the backing PV and\nvolume are deleted, depending on the retain policy).  When you set the `whenDeleted`\npolicy to `Delete`, an owner reference to the StatefulSet instance is placed on all PVCs\nassociated with that StatefulSet.  \nThe `whenScaled` policy must delete PVCs only when a Pod is scaled down, and not when a\nPod is deleted for another reason. When reconciling, the StatefulSet controller compares\nits desired replica count to the actual Pods present on the cluster. Any StatefulSet Pod\nwhose id greater than the replica count is condemned and marked for deletion. If the\n`whenScaled` policy is `Delete`, the condemned Pods are first set as owners to the\nassociated StatefulSet template PVCs, before the Pod is deleted. This causes the PVCs\nto be garbage collected after only the condemned Pods have terminated.  \nThis means that if the controller crashes and restarts, no Pod will be deleted before its\nowner reference has been updated appropriate to the policy. If a condemned Pod is\nforce-deleted while the controller is down, the owner reference may or may not have been\nset up, depending on when the controller crashed. It may take several reconcile loops to\nupdate the owner references, so some condemned Pods may have set up owner references and\nothers may not. For this reason we recommend waiting for the controller to come back up,\nwhich will verify owner references before terminating Pods. If that is not possible, the\noperator should verify the owner references on PVCs to ensure the expected objects are\ndeleted when Pods are force-deleted.\n"
  },
  {
    "question": "How do I schedule a CronJob to run every Monday at 3 AM?",
    "answer": "You can schedule a CronJob to run every Monday at 3 AM by setting the `.spec.schedule` field to `0 3 * * 1`.",
    "uuid": "33b12a5c-c753-485e-ba50-0af0af61aee2",
    "question_with_context": "A user asked the following question:\nQuestion: How do I schedule a CronJob to run every Monday at 3 AM?\nThis is about the following runbook:\nRunbook Title: Schedule syntax\nRunbook Content: Writing a CronJob specSchedule syntaxThe `.spec.schedule` field is required. The value of that field follows the [Cron](https://en.wikipedia.org/wiki/Cron) syntax:  \n```\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n# \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n# \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31)\n# \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12)\n# \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6) (Sunday to Saturday)\n# \u2502 \u2502 \u2502 \u2502 \u2502                                   OR sun, mon, tue, wed, thu, fri, sat\n# \u2502 \u2502 \u2502 \u2502 \u2502\n# \u2502 \u2502 \u2502 \u2502 \u2502\n# * * * * *\n```  \nFor example, `0 3 * * 1` means this task is scheduled to run weekly on a Monday at 3 AM.  \nThe format also includes extended \"Vixie cron\" step values. As explained in the\n[FreeBSD manual](https://www.freebsd.org/cgi/man.cgi?crontab%285%29):  \n> Step values can be used in conjunction with ranges. Following a range\n> with `/<number>` specifies skips of the number's value through the\n> range. For example, `0-23/2` can be used in the hours field to specify\n> command execution every other hour (the alternative in the V7 standard is\n> `0,2,4,6,8,10,12,14,16,18,20,22`). Steps are also permitted after an\n> asterisk, so if you want to say \"every two hours\", just use `*/2`.  \n{{< note >}}\nA question mark (`?`) in the schedule has the same meaning as an asterisk `*`, that is,\nit stands for any of available value for a given field.\n{{< /note >}}  \nOther than the standard syntax, some macros like `@monthly` can also be used:  \n| Entry | Description| Equivalent to |\n| ------------- | ------------- |-------------  |\n| @yearly (or @annually)| Run once a year at midnight of 1 January| 0 0 1 1 * |\n| @monthly | Run once a month at midnight of the first day of the month| 0 0 1 * * |\n| @weekly | Run once a week at midnight on Sunday morning| 0 0 * * 0 |\n| @daily (or @midnight)| Run once a day at midnight| 0 0 * * * |\n| @hourly | Run once an hour at the beginning of the hour| 0 * * * * |  \nTo generate CronJob schedule expressions, you can also use web tools like [crontab.guru](https://crontab.guru/).\n"
  },
  {
    "question": "How do I specify the operating system for my Pod in Kubernetes?",
    "answer": "You should set the `.spec.os.name` field to either `windows` or `linux` to indicate the OS on which you want the pod to run.",
    "uuid": "c3e62092-1862-4043-892e-175903eec42c",
    "question_with_context": "A user asked the following question:\nQuestion: How do I specify the operating system for my Pod in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Pod OS\nRunbook Content: Working with PodsPod OS{{< feature-state state=\"stable\" for_k8s_version=\"v1.25\" >}}  \nYou should set the `.spec.os.name` field to either `windows` or `linux` to indicate the OS on\nwhich you want the pod to run. These two are the only operating systems supported for now by\nKubernetes. In the future, this list may be expanded.  \nIn Kubernetes v{{< skew currentVersion >}}, the value of `.spec.os.name` does not affect\nhow the {{< glossary_tooltip text=\"kube-scheduler\" term_id=\"kube-scheduler\" >}}\npicks a node for the Pod to run on. In any cluster where there is more than one operating system for\nrunning nodes, you should set the\n[kubernetes.io/os](/docs/reference/labels-annotations-taints/#kubernetes-io-os)\nlabel correctly on each node, and define pods with a `nodeSelector` based on the operating system\nlabel. The kube-scheduler assigns your pod to a node based on other criteria and may or may not\nsucceed in picking a suitable node placement where the node OS is right for the containers in that Pod.\nThe [Pod security standards](/docs/concepts/security/pod-security-standards/) also use this\nfield to avoid enforcing policies that aren't relevant to the operating system.\n"
  },
  {
    "question": "How can I set up multiple release tracks for my application?",
    "answer": "You can set up multiple release tracks by using labels to differentiate them. For example, you can create a ReplicationController with `replicas` set to 9 for the stable track and another ReplicationController with `replicas` set to 1 for the canary track, both targeting the same service.",
    "uuid": "9d164dd7-2202-43ad-8c69-d4bfabe6d624",
    "question_with_context": "A user asked the following question:\nQuestion: How can I set up multiple release tracks for my application?\nThis is about the following runbook:\nRunbook Title: Multiple release tracks\nRunbook Content: Common usage patternsMultiple release tracksIn addition to running multiple releases of an application while a rolling update is in progress, it's common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.  \nFor instance, a service might target all pods with `tier in (frontend), environment in (prod)`.  Now say you have 10 replicated pods that make up this tier.  But you want to be able to 'canary' a new version of this component.  You could set up a ReplicationController with `replicas` set to 9 for the bulk of the replicas, with labels `tier=frontend, environment=prod, track=stable`, and another ReplicationController with `replicas` set to 1 for the canary, with labels `tier=frontend, environment=prod, track=canary`.  Now the service is covering both the canary and non-canary pods.  But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.\n"
  },
  {
    "question": "Can I change scheduling directives for a job that has already started?",
    "answer": "No, you can only update a Job's scheduling directives if it is suspended and has never been unsuspended before.",
    "uuid": "acacbd25-d702-425a-b0fe-33bc46cd8447",
    "question_with_context": "A user asked the following question:\nQuestion: Can I change scheduling directives for a job that has already started?\nThis is about the following runbook:\nRunbook Title: Mutable Scheduling Directives\nRunbook Content: Advanced usageMutable Scheduling Directives{{< feature-state for_k8s_version=\"v1.27\" state=\"stable\" >}}  \nIn most cases, a parallel job will want the pods to run with constraints,\nlike all in the same zone, or all either on GPU model x or y but not a mix of both.  \nThe [suspend](#suspending-a-job) field is the first step towards achieving those semantics. Suspend allows a\ncustom queue controller to decide when a job should start; However, once a job is unsuspended,\na custom queue controller has no influence on where the pods of a job will actually land.  \nThis feature allows updating a Job's scheduling directives before it starts, which gives custom queue\ncontrollers the ability to influence pod placement while at the same time offloading actual\npod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that have never\nbeen unsuspended before.  \nThe fields in a Job's pod template that can be updated are node affinity, node selector,\ntolerations, labels, annotations and [scheduling gates](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/).\n"
  },
  {
    "question": "What will the output look like after scaling my deployment?",
    "answer": "After scaling your deployment, the output will be similar to: `deployment.apps/nginx-deployment scaled`.",
    "uuid": "b0b9f323-c01a-447a-b28b-21c481252eff",
    "question_with_context": "A user asked the following question:\nQuestion: What will the output look like after scaling my deployment?\nThis is about the following runbook:\nRunbook Title: Scaling a Deployment\nRunbook Content: Scaling a DeploymentYou can scale a Deployment by using the following command:  \n```shell\nkubectl scale deployment/nginx-deployment --replicas=10\n```\nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment scaled\n```  \nAssuming [horizontal Pod autoscaling](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) is enabled\nin your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of\nPods you want to run based on the CPU utilization of your existing Pods.  \n```shell\nkubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80\n```\nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment scaled\n```\n"
  },
  {
    "question": "How do I set the backoff limit for a Job in Kubernetes?",
    "answer": "You can set the backoff limit for a Job by specifying `.spec.backoffLimit` to indicate the number of retries before considering the Job as failed. The default backoff limit is set to 6.",
    "uuid": "98fce0f9-e205-4916-91d2-9ca203918f6a",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set the backoff limit for a Job in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Pod backoff failure policy\nRunbook Content: Handling Pod and container failuresPod backoff failure policyThere are situations where you want to fail a Job after some amount of retries\ndue to a logical error in configuration etc.\nTo do so, set `.spec.backoffLimit` to specify the number of retries before\nconsidering a Job as failed. The back-off limit is set by default to 6. Failed\nPods associated with the Job are recreated by the Job controller with an\nexponential back-off delay (10s, 20s, 40s ...) capped at six minutes.  \nThe number of retries is calculated in two ways:  \n- The number of Pods with `.status.phase = \"Failed\"`.\n- When using `restartPolicy = \"OnFailure\"`, the number of retries in all the\ncontainers of Pods with `.status.phase` equal to `Pending` or `Running`.  \nIf either of the calculations reaches the `.spec.backoffLimit`, the Job is\nconsidered failed.  \n{{< note >}}\nIf your job has `restartPolicy = \"OnFailure\"`, keep in mind that your Pod running the Job\nwill be terminated once the job backoff limit has been reached. This can make debugging\nthe Job's executable more difficult. We suggest setting\n`restartPolicy = \"Never\"` when debugging the Job or using a logging system to ensure output\nfrom failed Jobs is not lost inadvertently.\n{{< /note >}}\n"
  },
  {
    "question": "What happens to the volumes when I delete or scale down a StatefulSet?",
    "answer": "Deleting and/or scaling a StatefulSet down will not delete the volumes associated with the StatefulSet to ensure data safety.",
    "uuid": "0b1518b9-4bb8-4a95-bd2e-eadb677d4593",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to the volumes when I delete or scale down a StatefulSet?\nThis is about the following runbook:\nRunbook Title: Limitations\nRunbook Content: Limitations* The storage for a given Pod must either be provisioned by a\n[PersistentVolume Provisioner](/docs/concepts/storage/dynamic-provisioning/) ([examples here](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md))\nbased on the requested _storage class_, or pre-provisioned by an admin.\n* Deleting and/or scaling a StatefulSet down will *not* delete the volumes associated with the\nStatefulSet. This is done to ensure data safety, which is generally more valuable than an\nautomatic purge of all related StatefulSet resources.\n* StatefulSets currently require a [Headless Service](/docs/concepts/services-networking/service/#headless-services)\nto be responsible for the network identity of the Pods. You are responsible for creating this\nService.\n* StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is\ndeleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is\npossible to scale the StatefulSet down to 0 prior to deletion.\n* When using [Rolling Updates](#rolling-updates) with the default\n[Pod Management Policy](#pod-management-policies) (`OrderedReady`),\nit's possible to get into a broken state that requires\n[manual intervention to repair](#forced-rollback).\n"
  },
  {
    "question": "How do I run an example ReplicationController for nginx?",
    "answer": "You can run the example ReplicationController by downloading the example file and executing the command: `kubectl apply -f https://k8s.io/examples/controllers/replication.yaml`.",
    "uuid": "7671cba2-ca80-4b20-b0b4-d892ffce2be9",
    "question_with_context": "A user asked the following question:\nQuestion: How do I run an example ReplicationController for nginx?\nThis is about the following runbook:\nRunbook Title: Running an example ReplicationController\nRunbook Content: Running an example ReplicationControllerThis example ReplicationController config runs three copies of the nginx web server.  \n{{% code_sample file=\"controllers/replication.yaml\" %}}  \nRun the example job by downloading the example file and then running this command:  \n```shell\nkubectl apply -f https://k8s.io/examples/controllers/replication.yaml\n```  \nThe output is similar to this:  \n```\nreplicationcontroller/nginx created\n```  \nCheck on the status of the ReplicationController using this command:  \n```shell\nkubectl describe replicationcontrollers/nginx\n```  \nThe output is similar to this:  \n```\nName:        nginx\nNamespace:   default\nSelector:    app=nginx\nLabels:      app=nginx\nAnnotations:    <none>\nReplicas:    3 current / 3 desired\nPods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed\nPod Template:\nLabels:       app=nginx\nContainers:\nnginx:\nImage:              nginx\nPort:               80/TCP\nEnvironment:        <none>\nMounts:             <none>\nVolumes:              <none>\nEvents:\nFirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message\n---------       --------     -----    ----                        -------------    ----      ------              -------\n20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m\n20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0\n20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v\n```  \nHere, three pods are created, but none is running yet, perhaps because the image is being pulled.\nA little later, the same command may show:  \n```shell\nPods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed\n```  \nTo list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:  \n```shell\npods=$(kubectl get pods --selector=app=nginx --output=jsonpath={.items..metadata.name})\necho $pods\n```  \nThe output is similar to this:  \n```\nnginx-3ntk0 nginx-4ok8v nginx-qrm3m\n```  \nHere, the selector is the same as the selector for the ReplicationController (seen in the\n`kubectl describe` output), and in a different form in `replication.yaml`.  The `--output=jsonpath` option\nspecifies an expression with the name from each pod in the returned list.\n"
  },
  {
    "question": "How do I enable user namespaces for a pod?",
    "answer": "You can enable user namespaces for a pod by setting the `pod.spec.hostUsers` field to `false'.",
    "uuid": "7598d4f3-b249-4018-b9c3-ee61832205b4",
    "question_with_context": "A user asked the following question:\nQuestion: How do I enable user namespaces for a pod?\nThis is about the following runbook:\nRunbook Title: Introduction\nRunbook Content: IntroductionUser namespaces is a Linux feature that allows to map users in the container to\ndifferent users in the host. Furthermore, the capabilities granted to a pod in\na user namespace are valid only in the namespace and void outside of it.  \nA pod can opt-in to use user namespaces by setting the `pod.spec.hostUsers` field\nto `false`.  \nThe kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way\nto guarantee that no two pods on the same node use the same mapping.  \nThe `runAsUser`, `runAsGroup`, `fsGroup`, etc. fields in the `pod.spec` always\nrefer to the user inside the container.  \nThe valid UIDs/GIDs when this feature is enabled is the range 0-65535. This\napplies to files and processes (`runAsUser`, `runAsGroup`, etc.).  \nFiles using a UID/GID outside this range will be seen as belonging to the\noverflow ID, usually 65534 (configured in `/proc/sys/kernel/overflowuid` and\n`/proc/sys/kernel/overflowgid`). However, it is not possible to modify those\nfiles, even by running as the 65534 user/group.  \nMost applications that need to run as root but don't access other host\nnamespaces or resources, should continue to run fine without any changes needed\nif user namespaces is activated.\n"
  },
  {
    "question": "How can I check the rollout history of my nginx deployment?",
    "answer": "You can check the rollout history of your nginx deployment by running the command: `kubectl rollout history deployment/nginx-deployment`.",
    "uuid": "c8336b44-ba83-495d-a40f-70bec028eb5e",
    "question_with_context": "A user asked the following question:\nQuestion: How can I check the rollout history of my nginx deployment?\nThis is about the following runbook:\nRunbook Title: Checking Rollout History of a Deployment\nRunbook Content: Rolling Back a DeploymentChecking Rollout History of a DeploymentFollow the steps given below to check the rollout history:  \n1. First, check the revisions of this Deployment:\n```shell\nkubectl rollout history deployment/nginx-deployment\n```\nThe output is similar to this:\n```\ndeployments \"nginx-deployment\"\nREVISION    CHANGE-CAUSE\n1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml\n2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n3           kubectl set image deployment/nginx-deployment nginx=nginx:1.161\n```  \n`CHANGE-CAUSE` is copied from the Deployment annotation `kubernetes.io/change-cause` to its revisions upon creation. You can specify the`CHANGE-CAUSE` message by:  \n* Annotating the Deployment with `kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"image updated to 1.16.1\"`\n* Manually editing the manifest of the resource.  \n2. To see the details of each revision, run:\n```shell\nkubectl rollout history deployment/nginx-deployment --revision=2\n```  \nThe output is similar to this:\n```\ndeployments \"nginx-deployment\" revision 2\nLabels:       app=nginx\npod-template-hash=1159050644\nAnnotations:  kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\nContainers:\nnginx:\nImage:      nginx:1.16.1\nPort:       80/TCP\nQoS Tier:\ncpu:      BestEffort\nmemory:   BestEffort\nEnvironment Variables:      <none>\nNo volumes.\n```\n"
  },
  {
    "question": "How does the `.spec.minReadySeconds` affect the rollout of a Pod?",
    "answer": "This field is used to check the progression of a rollout when using a Rolling Update strategy.",
    "uuid": "5ddfa00b-cd6a-4590-98c0-7b3ff039761a",
    "question_with_context": "A user asked the following question:\nQuestion: How does the `.spec.minReadySeconds` affect the rollout of a Pod?\nThis is about the following runbook:\nRunbook Title: Minimum ready seconds\nRunbook Content: ComponentsMinimum ready seconds{{< feature-state for_k8s_version=\"v1.25\" state=\"stable\" >}}  \n`.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly\ncreated Pod should be running and ready without any of its containers crashing, for it to be considered available.\nThis is used to check progression of a rollout when using a [Rolling Update](#rolling-updates) strategy.\nThis field defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when\na Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).\n"
  },
  {
    "question": "What happens if my Deployment exceeds the progress deadline?",
    "answer": "If your Deployment exceeds the progress deadline, the system will report that the Deployment has failed progressing, indicated by a condition with `type: Progressing`, `status: \"False\"`, and `reason: ProgressDeadlineExceeded`.",
    "uuid": "1378b07a-4204-46a6-975f-2038e6cf7a93",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if my Deployment exceeds the progress deadline?\nThis is about the following runbook:\nRunbook Title: Progress Deadline Seconds\nRunbook Content: Writing a Deployment SpecProgress Deadline Seconds`.spec.progressDeadlineSeconds` is an optional field that specifies the number of seconds you want\nto wait for your Deployment to progress before the system reports back that the Deployment has\n[failed progressing](#failed-deployment) - surfaced as a condition with `type: Progressing`, `status: \"False\"`.\nand `reason: ProgressDeadlineExceeded` in the status of the resource. The Deployment controller will keep\nretrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment\ncontroller will roll back a Deployment as soon as it observes such a condition.  \nIf specified, this field needs to be greater than `.spec.minReadySeconds`.\n"
  },
  {
    "question": "Why would I want to keep a finished Job in the API instead of deleting it immediately?",
    "answer": "Keeping a finished Job in the API allows you to determine whether the Job succeeded or failed.",
    "uuid": "3f4edcf6-c8f1-4a3e-a1ea-9c7e025f363e",
    "question_with_context": "A user asked the following question:\nQuestion: Why would I want to keep a finished Job in the API instead of deleting it immediately?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- janetkuo\ntitle: Automatic Cleanup for Finished Jobs\ncontent_type: concept\nweight: 70\ndescription: >-\nA time-to-live mechanism to clean up old Jobs that have finished execution.\n---  \n<!-- overview -->  \n{{< feature-state for_k8s_version=\"v1.23\" state=\"stable\" >}}  \nWhen your Job has finished, it's useful to keep that Job in the API (and not immediately delete the Job)\nso that you can tell whether the Job succeeded or failed.  \nKubernetes' TTL-after-finished {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} provides a\nTTL (time to live) mechanism to limit the lifetime of Job objects that\nhave finished execution.  \n<!-- body -->\n"
  },
  {
    "question": "Do I need to worry about ReplicationControllers when using Services?",
    "answer": "No, both services and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.",
    "uuid": "b84ed1dd-6880-464a-a74f-f23595a7c269",
    "question_with_context": "A user asked the following question:\nQuestion: Do I need to worry about ReplicationControllers when using Services?\nThis is about the following runbook:\nRunbook Title: Using ReplicationControllers with Services\nRunbook Content: Common usage patternsUsing ReplicationControllers with ServicesMultiple ReplicationControllers can sit behind a single service, so that, for example, some traffic\ngoes to the old version, and some goes to the new version.  \nA ReplicationController will never terminate on its own, but it isn't expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.\n"
  },
  {
    "question": "What happens if I use a non-unique selector for my Job's Pods?",
    "answer": "If you use a non-unique selector for your Job's Pods, it may lead to unintended consequences such as unrelated Pods being deleted, your Job counting other Pods as completing it, or conflicts with other controllers like ReplicationController.",
    "uuid": "93cc2a2e-39d7-49b3-84cd-80379b5a33b5",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I use a non-unique selector for my Job's Pods?\nThis is about the following runbook:\nRunbook Title: Specifying your own Pod selector\nRunbook Content: Advanced usageSpecifying your own Pod selectorNormally, when you create a Job object, you do not specify `.spec.selector`.\nThe system defaulting logic adds this field when the Job is created.\nIt picks a selector value that will not overlap with any other jobs.  \nHowever, in some cases, you might need to override this automatically set selector.\nTo do this, you can specify the `.spec.selector` of the Job.  \nBe very careful when doing this. If you specify a label selector which is not\nunique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelated\njob may be deleted, or this Job may count other Pods as completing it, or one or both\nJobs may refuse to create Pods or run to completion. If a non-unique selector is\nchosen, then other controllers (e.g. ReplicationController) and their Pods may behave\nin unpredictable ways too. Kubernetes will not stop you from making a mistake when\nspecifying `.spec.selector`.  \nHere is an example of a case when you might want to use this feature.  \nSay Job `old` is already running. You want existing Pods\nto keep running, but you want the rest of the Pods it creates\nto use a different pod template and for the Job to have a new name.\nYou cannot update the Job because these fields are not updatable.\nTherefore, you delete Job `old` but _leave its pods\nrunning_, using `kubectl delete jobs/old --cascade=orphan`.\nBefore deleting it, you make a note of what selector it uses:  \n```shell\nkubectl get job old -o yaml\n```  \nThe output is similar to this:  \n```yaml\nkind: Job\nmetadata:\nname: old\n...\nspec:\nselector:\nmatchLabels:\nbatch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002\n...\n```  \nThen you create a new Job with name `new` and you explicitly specify the same selector.\nSince the existing Pods have label `batch.kubernetes.io/controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002`,\nthey are controlled by Job `new` as well.  \nYou need to specify `manualSelector: true` in the new Job since you are not using\nthe selector that the system normally generates for you automatically.  \n```yaml\nkind: Job\nmetadata:\nname: new\n...\nspec:\nmanualSelector: true\nselector:\nmatchLabels:\nbatch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002\n...\n```  \nThe new Job itself will have a different uid from `a8f3d00d-c6d2-11e5-9f87-42010af00002`. Setting\n`manualSelector: true` tells the system that you know what you are doing and to allow this\nmismatch.\n"
  },
  {
    "question": "Can I limit the number of failed indexes for my Job?",
    "answer": "Yes, you can limit the maximal number of indexes marked as failed by setting the `.spec.maxFailedIndexes` field. If the number of failed indexes exceeds this limit, the Job controller will terminate all remaining running Pods for that Job.",
    "uuid": "e6262089-39e5-4a12-8e62-a9e5224e693e",
    "question_with_context": "A user asked the following question:\nQuestion: Can I limit the number of failed indexes for my Job?\nThis is about the following runbook:\nRunbook Title: Backoff limit per index {#backoff-limit-per-index}\nRunbook Content: Handling Pod and container failuresBackoff limit per index {#backoff-limit-per-index}{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \n{{< note >}}\nYou can only configure the backoff limit per index for an [Indexed](#completion-mode) Job, if you\nhave the `JobBackoffLimitPerIndex` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nenabled in your cluster.\n{{< /note >}}  \nWhen you run an [indexed](#completion-mode) Job, you can choose to handle retries\nfor pod failures independently for each index. To do so, set the\n`.spec.backoffLimitPerIndex` to specify the maximal number of pod failures\nper index.  \nWhen the per-index backoff limit is exceeded for an index, Kubernetes considers the index as failed and adds it to the\n`.status.failedIndexes` field. The succeeded indexes, those with a successfully\nexecuted pods, are recorded in the `.status.completedIndexes` field, regardless of whether you set\nthe `backoffLimitPerIndex` field.  \nNote that a failing index does not interrupt execution of other indexes.\nOnce all indexes finish for a Job where you specified a backoff limit per index,\nif at least one of those indexes did fail, the Job controller marks the overall\nJob as failed, by setting the Failed condition in the status. The Job gets\nmarked as failed even if some, potentially nearly all, of the indexes were\nprocessed successfully.  \nYou can additionally limit the maximal number of indexes marked failed by\nsetting the `.spec.maxFailedIndexes` field.\nWhen the number of failed indexes exceeds the `maxFailedIndexes` field, the\nJob controller triggers termination of all remaining running Pods for that Job.\nOnce all pods are terminated, the entire Job is marked failed by the Job\ncontroller, by setting the Failed condition in the Job status.  \nHere is an example manifest for a Job that defines a `backoffLimitPerIndex`:  \n{{< code_sample file=\"/controllers/job-backoff-limit-per-index-example.yaml\" >}}  \nIn the example above, the Job controller allows for one restart for each\nof the indexes. When the total number of failed indexes exceeds 5, then\nthe entire Job is terminated.  \nOnce the job is finished, the Job status looks as follows:  \n```sh\nkubectl get -o yaml job job-backoff-limit-per-index-example\n```  \n```yaml\nstatus:\ncompletedIndexes: 1,3,5,7,9\nfailedIndexes: 0,2,4,6,8\nsucceeded: 5          # 1 succeeded pod for each of 5 succeeded indexes\nfailed: 10            # 2 failed pods (1 retry) for each of 5 failed indexes\nconditions:\n- message: Job has failed indexes\nreason: FailedIndexes\nstatus: \"True\"\ntype: FailureTarget\n- message: Job has failed indexes\nreason: FailedIndexes\nstatus: \"True\"\ntype: Failed\n```  \nThe Job controller adds the `FailureTarget` Job condition to trigger\n[Job termination and cleanup](#job-termination-and-cleanup). When all of the\nJob Pods are terminated, the Job controller adds the `Failed` condition\nwith the same values for `reason` and `message` as the `FailureTarget` Job\ncondition. For details, see [Termination of Job Pods](#termination-of-job-pods).  \nAdditionally, you may want to use the per-index backoff along with a\n[pod failure policy](#pod-failure-policy). When using\nper-index backoff, there is a new `FailIndex` action available which allows you to\navoid unnecessary retries within an index.\n"
  },
  {
    "question": "Can I manually adjust the number of replicas for my application?",
    "answer": "Yes, you can manually scale the number of replicas up or down using the ReplicationController.",
    "uuid": "bdd251a4-292a-412e-8468-18c072a7d29f",
    "question_with_context": "A user asked the following question:\nQuestion: Can I manually adjust the number of replicas for my application?\nThis is about the following runbook:\nRunbook Title: Scaling\nRunbook Content: Common usage patternsScalingThe ReplicationController enables scaling the number of replicas up or down, either manually or by an auto-scaling control agent, by updating the `replicas` field.\n"
  },
  {
    "question": "Can I use a ReplicaSet for batch jobs?",
    "answer": "No, you should use a Job instead of a ReplicaSet for batch jobs.",
    "uuid": "413c0909-0246-46a5-ade6-19ebdbf884ff",
    "question_with_context": "A user asked the following question:\nQuestion: Can I use a ReplicaSet for batch jobs?\nThis is about the following runbook:\nRunbook Title: Job\nRunbook Content: Alternatives to ReplicaSetJobUse a [`Job`](/docs/concepts/workloads/controllers/job/) instead of a ReplicaSet for Pods that are\nexpected to terminate on their own (that is, batch jobs).\n"
  },
  {
    "question": "What command can I use to check the rollout status of my Deployment?",
    "answer": "To check the rollout status of your Deployment, you can run: `kubectl rollout status deployment/nginx-deployment`. This will show you the progress of the rollout.",
    "uuid": "b4a16ae1-aef1-489a-ac8d-f28e9c5cea4f",
    "question_with_context": "A user asked the following question:\nQuestion: What command can I use to check the rollout status of my Deployment?\nThis is about the following runbook:\nRunbook Title: Updating a Deployment\nRunbook Content: Updating a Deployment{{< note >}}\nA Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, `.spec.template`)\nis changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.\n{{< /note >}}  \nFollow the steps given below to update your Deployment:  \n1. Let's update the nginx Pods to use the `nginx:1.16.1` image instead of the `nginx:1.14.2` image.  \n```shell\nkubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1\n```  \nor use the following command:  \n```shell\nkubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n```\nwhere `deployment/nginx-deployment` indicates the Deployment,\n`nginx` indicates the Container the update will take place and\n`nginx:1.16.1` indicates the new image and its tag.  \nThe output is similar to:  \n```\ndeployment.apps/nginx-deployment image updated\n```  \nAlternatively, you can `edit` the Deployment and change `.spec.template.spec.containers[0].image` from `nginx:1.14.2` to `nginx:1.16.1`:  \n```shell\nkubectl edit deployment/nginx-deployment\n```  \nThe output is similar to:  \n```\ndeployment.apps/nginx-deployment edited\n```  \n2. To see the rollout status, run:  \n```shell\nkubectl rollout status deployment/nginx-deployment\n```  \nThe output is similar to this:  \n```\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\n```  \nor  \n```\ndeployment \"nginx-deployment\" successfully rolled out\n```  \nGet more details on your updated Deployment:  \n* After the rollout succeeds, you can view the Deployment by running `kubectl get deployments`.\nThe output is similar to this:  \n```ini\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           36s\n```  \n* Run `kubectl get rs` to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it\nup to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.  \n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY   AGE\nnginx-deployment-1564180365   3         3         3       6s\nnginx-deployment-2035384211   0         0         0       36s\n```  \n* Running `get pods` should now show only the new Pods:  \n```shell\nkubectl get pods\n```  \nThe output is similar to this:\n```\nNAME                                READY     STATUS    RESTARTS   AGE\nnginx-deployment-1564180365-khku8   1/1       Running   0          14s\nnginx-deployment-1564180365-nacti   1/1       Running   0          14s\nnginx-deployment-1564180365-z9gth   1/1       Running   0          14s\n```  \nNext time you want to update these Pods, you only need to update the Deployment's Pod template again.  \nDeployment ensures that only a certain number of Pods are down while they are being updated. By default,\nit ensures that at least 75% of the desired number of Pods are up (25% max unavailable).  \nDeployment also ensures that only a certain number of Pods are created above the desired number of Pods.\nBy default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).  \nFor example, if you look at the above Deployment closely, you will see that it first creates a new Pod,\nthen deletes an old Pod, and creates another new one. It does not kill old Pods until a sufficient number of\nnew Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.\nIt makes sure that at least 3 Pods are available and that at max 4 Pods in total are available. In case of\na Deployment with 4 replicas, the number of Pods would be between 3 and 5.  \n* Get details of your Deployment:\n```shell\nkubectl describe deployments\n```\nThe output is similar to this:\n```\nName:                   nginx-deployment\nNamespace:              default\nCreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000\nLabels:                 app=nginx\nAnnotations:            deployment.kubernetes.io/revision=2\nSelector:               app=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx\nContainers:\nnginx:\nImage:        nginx:1.16.1\nPort:         80/TCP\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    NewReplicaSetAvailable\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)\nEvents:\nType    Reason             Age   From                   Message\n----    ------             ----  ----                   -------\nNormal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3\nNormal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1\nNormal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2\nNormal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2\nNormal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1\nNormal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3\nNormal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0\n```\nHere you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)\nand scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet\n(nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet\nto 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times.\nIt then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy.\nFinally, you'll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.  \n{{< note >}}\nKubernetes doesn't count terminating Pods when calculating the number of `availableReplicas`, which must be between\n`replicas - maxUnavailable` and `replicas + maxSurge`. As a result, you might notice that there are more Pods than\nexpected during a rollout, and that the total resources consumed by the Deployment is more than `replicas + maxSurge`\nuntil the `terminationGracePeriodSeconds` of the terminating Pods expires.\n{{< /note >}}\n"
  },
  {
    "question": "What should I do to investigate a `CrashLoopBackOff` issue with my container?",
    "answer": "You can check logs using `kubectl logs <name-of-pod>`, inspect events with `kubectl describe pod <name-of-pod>`, review the Pod configuration, check resource limits, and debug the application.",
    "uuid": "1c1fb0a0-20b3-42b2-a6ae-6e80cddd5a2b",
    "question_with_context": "A user asked the following question:\nQuestion: What should I do to investigate a `CrashLoopBackOff` issue with my container?\nThis is about the following runbook:\nRunbook Title: How Pods handle problems with containers {#container-restarts}\nRunbook Content: How Pods handle problems with containers {#container-restarts}Kubernetes manages container failures within Pods using a [`restartPolicy`](#restart-policy) defined in the Pod `spec`. This policy determines how Kubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:  \n1. **Initial crash**: Kubernetes attempts an immediate restart based on the Pod `restartPolicy`.\n1. **Repeated crashes**: After the initial crash Kubernetes applies an exponential\nbackoff delay for subsequent restarts, described in [`restartPolicy`](#restart-policy).\nThis prevents rapid, repeated restart attempts from overloading the system.\n1. **CrashLoopBackOff state**: This indicates that the backoff delay mechanism is currently\nin effect for a given container that is in a crash loop, failing and restarting repeatedly.\n1. **Backoff reset**: If a container runs successfully for a certain duration\n(e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash\nas the first one.  \nIn practice, a `CrashLoopBackOff` is a condition or event that might be seen as output\nfrom the `kubectl` command, while describing or listing Pods, when a container in the Pod\nfails to start properly and then continually tries and fails in a loop.  \nIn other words, when a container enters the crash loop, Kubernetes applies the\nexponential backoff delay mentioned in the [Container restart policy](#restart-policy).\nThis mechanism prevents a faulty container from overwhelming the system with continuous\nfailed start attempts.  \nThe `CrashLoopBackOff` can be caused by issues like the following:  \n* Application errors that cause the container to exit.\n* Configuration errors, such as incorrect environment variables or missing\nconfiguration files.\n* Resource constraints, where the container might not have enough memory or CPU\nto start properly.\n* Health checks failing if the application doesn't start serving within the\nexpected time.\n* Container liveness probes or startup probes returning a `Failure` result\nas mentioned in the [probes section](#container-probes).  \nTo investigate the root cause of a `CrashLoopBackOff` issue, a user can:  \n1. **Check logs**: Use `kubectl logs <name-of-pod>` to check the logs of the container.\nThis is often the most direct way to diagnose the issue causing the crashes.\n1. **Inspect events**: Use `kubectl describe pod <name-of-pod>` to see events\nfor the Pod, which can provide hints about configuration or resource issues.\n1. **Review configuration**: Ensure that the Pod configuration, including\nenvironment variables and mounted volumes, is correct and that all required\nexternal resources are available.\n1. **Check resource limits**: Make sure that the container has enough CPU\nand memory allocated. Sometimes, increasing the resources in the Pod definition\ncan resolve the issue.\n1. **Debug application**: There might exist bugs or misconfigurations in the\napplication code. Running this container image locally or in a development\nenvironment can help diagnose application specific issues.\n"
  },
  {
    "question": "What happens if I don't specify the number of replicas in my ReplicaSet?",
    "answer": "If you do not specify `.spec.replicas`, it defaults to 1.",
    "uuid": "a7cf9eaf-4e3f-4d6b-b903-9a80491a9349",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I don't specify the number of replicas in my ReplicaSet?\nThis is about the following runbook:\nRunbook Title: Replicas\nRunbook Content: Writing a ReplicaSet manifestReplicasYou can specify how many Pods should run concurrently by setting `.spec.replicas`. The ReplicaSet will create/delete\nits Pods to match this number.  \nIf you do not specify `.spec.replicas`, then it defaults to 1.\n"
  },
  {
    "question": "Can I add a new container to a Pod after it has been created?",
    "answer": "No, you cannot add a container to a Pod once it has been created; instead, you usually delete and replace Pods using deployments.",
    "uuid": "05c3808b-c5eb-418a-b293-6b59dc3624d6",
    "question_with_context": "A user asked the following question:\nQuestion: Can I add a new container to a Pod after it has been created?\nThis is about the following runbook:\nRunbook Title: Understanding ephemeral containers\nRunbook Content: Understanding ephemeral containers{{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} are the fundamental building\nblock of Kubernetes applications. Since Pods are intended to be disposable and\nreplaceable, you cannot add a container to a Pod once it has been created.\nInstead, you usually delete and replace Pods in a controlled fashion using\n{{< glossary_tooltip text=\"deployments\" term_id=\"deployment\" >}}.  \nSometimes it's necessary to inspect the state of an existing Pod, however, for\nexample to troubleshoot a hard-to-reproduce bug. In these cases you can run\nan ephemeral container in an existing Pod to inspect its state and run\narbitrary commands.\n"
  },
  {
    "question": "How can I expose Pod fields to my running container?",
    "answer": "You can expose Pod fields to a running container using environment variables or as files populated by a special volume type.",
    "uuid": "63cd54b8-226c-45cd-9bcf-f57e428245f0",
    "question_with_context": "A user asked the following question:\nQuestion: How can I expose Pod fields to my running container?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: Downward API\ncontent_type: concept\nweight: 170\ndescription: >\nThere are two ways to expose Pod and container fields to a running container:\nenvironment variables, and as files that are populated by a special volume type.\nTogether, these two ways of exposing Pod and container fields are called the downward API.\n---  \n<!-- overview -->  \nIt is sometimes useful for a container to have information about itself, without\nbeing overly coupled to Kubernetes. The _downward API_ allows containers to consume\ninformation about themselves or the cluster without using the Kubernetes client\nor API server.  \nAn example is an existing application that assumes a particular well-known\nenvironment variable holds a unique identifier. One possibility is to wrap the\napplication, but that is tedious and error-prone, and it violates the goal of low\ncoupling. A better option would be to use the Pod's name as an identifier, and\ninject the Pod's name into the well-known environment variable.  \nIn Kubernetes, there are two ways to expose Pod and container fields to a running container:  \n* as [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)\n* as [files in a `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)  \nTogether, these two ways of exposing Pod and container fields are called the\n_downward API_.  \n<!-- body -->\n"
  },
  {
    "question": "What are the possible states a container can be in Kubernetes?",
    "answer": "In Kubernetes, a container can be in one of three possible states: `Waiting`, `Running`, or `Terminated`.",
    "uuid": "10c9061b-43f7-4c3b-aa32-f69d5803e81b",
    "question_with_context": "A user asked the following question:\nQuestion: What are the possible states a container can be in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Container states\nRunbook Content: Container statesAs well as the [phase](#pod-phase) of the Pod overall, Kubernetes tracks the state of\neach container inside a Pod. You can use\n[container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/) to\ntrigger events to run at certain points in a container's lifecycle.  \nOnce the {{< glossary_tooltip text=\"scheduler\" term_id=\"kube-scheduler\" >}}\nassigns a Pod to a Node, the kubelet starts creating containers for that Pod\nusing a {{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}.\nThere are three possible container states: `Waiting`, `Running`, and `Terminated`.  \nTo check the state of a Pod's containers, you can use\n`kubectl describe pod <name-of-pod>`. The output shows the state for each container\nwithin that Pod.  \nEach state has a specific meaning:\n"
  },
  {
    "question": "How does the restartPolicy affect init containers?",
    "answer": "The restartPolicy for a Pod applies to init containers, and the kubelet restarts an init container if it exits with an error when the Pod level restartPolicy is either OnFailure or Always.",
    "uuid": "01c4b24e-ec3e-4f66-9816-b778bc3c67fc",
    "question_with_context": "A user asked the following question:\nQuestion: How does the restartPolicy affect init containers?\nThis is about the following runbook:\nRunbook Title: Container restart policy {#restart-policy}\nRunbook Content: How Pods handle problems with containers {#container-restarts}Container restart policy {#restart-policy}The `spec` of a Pod has a `restartPolicy` field with possible values Always, OnFailure,\nand Never. The default value is Always.  \nThe `restartPolicy` for a Pod applies to {{< glossary_tooltip text=\"app containers\" term_id=\"app-container\" >}}\nin the Pod and to regular [init containers](/docs/concepts/workloads/pods/init-containers/).\n[Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\nignore the Pod-level `restartPolicy` field: in Kubernetes, a sidecar is defined as an\nentry inside `initContainers` that has its container-level `restartPolicy` set to `Always`.\nFor init containers that exit with an error, the kubelet restarts the init container if\nthe Pod level `restartPolicy` is either `OnFailure` or `Always`:  \n* `Always`: Automatically restarts the container after any termination.\n* `OnFailure`: Only restarts the container if it exits with an error (non-zero exit status).\n* `Never`: Does not automatically restart the terminated container.  \nWhen the kubelet is handling container restarts according to the configured restart\npolicy, that only applies to restarts that make replacement containers inside the\nsame Pod and running on the same node. After containers in a Pod exit, the kubelet\nrestarts them with an exponential backoff delay (10s, 20s, 40s, \u2026), that is capped at\n300 seconds (5 minutes). Once a container has executed for 10 minutes without any\nproblems, the kubelet resets the restart backoff timer for that container.\n[Sidecar containers and Pod lifecycle](/docs/concepts/workloads/pods/sidecar-containers/#sidecar-containers-and-pod-lifecycle)\nexplains the behaviour of `init containers` when specify `restartpolicy` field on it.\n"
  },
  {
    "question": "What is the Replication Controller in Kubernetes?",
    "answer": "The Replication Controller is a top-level resource in the Kubernetes REST API.",
    "uuid": "8b3b13a3-93f7-494b-9af4-9c468e018a0a",
    "question_with_context": "A user asked the following question:\nQuestion: What is the Replication Controller in Kubernetes?\nThis is about the following runbook:\nRunbook Title: API Object\nRunbook Content: API ObjectReplication controller is a top-level resource in the Kubernetes REST API. More details about the\nAPI object can be found at:\n[ReplicationController API object](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#replicationcontroller-v1-core).\n"
  },
  {
    "question": "Can I control PVC deletion when scaling down a StatefulSet?",
    "answer": "Yes, you can control PVC deletion when scaling down a StatefulSet using the `whenScaled` policy. If set to `Delete`, only the PVCs corresponding to the Pod replicas being scaled down will be deleted after their Pods have been deleted. If set to `Retain`, the PVCs will not be deleted.",
    "uuid": "587393e3-236c-4943-ba0f-3f3e589d8e1d",
    "question_with_context": "A user asked the following question:\nQuestion: Can I control PVC deletion when scaling down a StatefulSet?\nThis is about the following runbook:\nRunbook Title: PersistentVolumeClaim retention\nRunbook Content: PersistentVolumeClaim retention{{< feature-state for_k8s_version=\"v1.27\" state=\"beta\" >}}  \nThe optional `.spec.persistentVolumeClaimRetentionPolicy` field controls if\nand how PVCs are deleted during the lifecycle of a StatefulSet. You must enable the\n`StatefulSetAutoDeletePVC` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\non the API server and the controller manager to use this field.\nOnce enabled, there are two policies you can configure for each StatefulSet:  \n`whenDeleted`\n: configures the volume retention behavior that applies when the StatefulSet is deleted  \n`whenScaled`\n: configures the volume retention behavior that applies when the replica count of\nthe StatefulSet   is reduced; for example, when scaling down the set.  \nFor each policy that you can configure, you can set the value to either `Delete` or `Retain`.  \n`Delete`\n: The PVCs created from the StatefulSet `volumeClaimTemplate` are deleted for each Pod\naffected by the policy. With the `whenDeleted` policy all PVCs from the\n`volumeClaimTemplate` are deleted after their Pods have been deleted. With the\n`whenScaled` policy, only PVCs corresponding to Pod replicas being scaled down are\ndeleted, after their Pods have been deleted.  \n`Retain` (default)\n: PVCs from the `volumeClaimTemplate` are not affected when their Pod is\ndeleted. This is the behavior before this new feature.  \nBear in mind that these policies **only** apply when Pods are being removed due to the\nStatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSet\nfails due to node failure, and the control plane creates a replacement Pod, the StatefulSet\nretains the existing PVC.  The existing volume is unaffected, and the cluster will attach it to\nthe node where the new Pod is about to launch.  \nThe default for policies is `Retain`, matching the StatefulSet behavior before this new feature.  \nHere is an example policy.  \n```yaml\napiVersion: apps/v1\nkind: StatefulSet\n...\nspec:\npersistentVolumeClaimRetentionPolicy:\nwhenDeleted: Retain\nwhenScaled: Delete\n...\n```  \nThe StatefulSet {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} adds\n[owner references](/docs/concepts/overview/working-with-objects/owners-dependents/#owner-references-in-object-specifications)\nto its PVCs, which are then deleted by the {{<glossary_tooltip text=\"garbage collector\"\nterm_id=\"garbage-collection\">}} after the Pod is terminated. This enables the Pod to\ncleanly unmount all volumes before the PVCs are deleted (and before the backing PV and\nvolume are deleted, depending on the retain policy).  When you set the `whenDeleted`\npolicy to `Delete`, an owner reference to the StatefulSet instance is placed on all PVCs\nassociated with that StatefulSet.  \nThe `whenScaled` policy must delete PVCs only when a Pod is scaled down, and not when a\nPod is deleted for another reason. When reconciling, the StatefulSet controller compares\nits desired replica count to the actual Pods present on the cluster. Any StatefulSet Pod\nwhose id greater than the replica count is condemned and marked for deletion. If the\n`whenScaled` policy is `Delete`, the condemned Pods are first set as owners to the\nassociated StatefulSet template PVCs, before the Pod is deleted. This causes the PVCs\nto be garbage collected after only the condemned Pods have terminated.  \nThis means that if the controller crashes and restarts, no Pod will be deleted before its\nowner reference has been updated appropriate to the policy. If a condemned Pod is\nforce-deleted while the controller is down, the owner reference may or may not have been\nset up, depending on when the controller crashed. It may take several reconcile loops to\nupdate the owner references, so some condemned Pods may have set up owner references and\nothers may not. For this reason we recommend waiting for the controller to come back up,\nwhich will verify owner references before terminating Pods. If that is not possible, the\noperator should verify the owner references on PVCs to ensure the expected objects are\ndeleted when Pods are force-deleted.\n"
  },
  {
    "question": "How do init containers help with app container startup timing?",
    "answer": "Init containers run to completion before any app containers start, which allows them to block or delay app container startup until certain preconditions are met.",
    "uuid": "b91fb861-090f-47da-af13-3ee58314590a",
    "question_with_context": "A user asked the following question:\nQuestion: How do init containers help with app container startup timing?\nThis is about the following runbook:\nRunbook Title: Using init containers\nRunbook Content: Using init containersBecause init containers have separate images from app containers, they\nhave some advantages for start-up related code:  \n* Init containers can contain utilities or custom code for setup that are not present in an app\nimage. For example, there is no need to make an image `FROM` another image just to use a tool like\n`sed`, `awk`, `python`, or `dig` during setup.\n* The application image builder and deployer roles can work independently without\nthe need to jointly build a single app image.\n* Init containers can run with a different view of the filesystem than app containers in the\nsame Pod. Consequently, they can be given access to\n{{< glossary_tooltip text=\"Secrets\" term_id=\"secret\" >}} that app containers cannot access.\n* Because init containers run to completion before any app containers start, init containers offer\na mechanism to block or delay app container startup until a set of preconditions are met. Once\npreconditions are met, all of the app containers in a Pod can start in parallel.\n* Init containers can securely run utilities or custom code that would otherwise make an app\ncontainer image less secure. By keeping unnecessary tools separate you can limit the attack\nsurface of your app container image.\n"
  },
  {
    "question": "What happens if an init container fails in my Pod?",
    "answer": "If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds. However, if the Pod has a `restartPolicy` of Never, and an init container fails during startup, Kubernetes treats the overall Pod as failed.",
    "uuid": "7f5730c2-3588-4ef1-a0c8-7361ca75747b",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if an init container fails in my Pod?\nThis is about the following runbook:\nRunbook Title: Understanding init containers\nRunbook Content: Understanding init containersA {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} can have multiple containers\nrunning apps within it, but it can also have one or more init containers, which are run\nbefore the app containers are started.  \nInit containers are exactly like regular containers, except:  \n* Init containers always run to completion.\n* Each init container must complete successfully before the next one starts.  \nIf a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.\nHowever, if the Pod has a `restartPolicy` of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.  \nTo specify an init container for a Pod, add the `initContainers` field into\nthe [Pod specification](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec),\nas an array of `container` items (similar to the app `containers` field and its contents).\nSee [Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container) in the\nAPI reference for more details.  \nThe status of the init containers is returned in `.status.initContainerStatuses`\nfield as an array of the container statuses (similar to the `.status.containerStatuses`\nfield).\n"
  },
  {
    "question": "Can I use host namespaces if I set hostUsers to false?",
    "answer": "No, if you set hostUsers to false, you are not allowed to use any host namespaces.",
    "uuid": "50091ea9-1175-4468-80fa-c72c9f0b30ea",
    "question_with_context": "A user asked the following question:\nQuestion: Can I use host namespaces if I set hostUsers to false?\nThis is about the following runbook:\nRunbook Title: Limitations\nRunbook Content: LimitationsWhen using a user namespace for the pod, it is disallowed to use other host\nnamespaces. In particular, if you set `hostUsers: false` then you are not\nallowed to set any of:  \n* `hostNetwork: true`\n* `hostIPC: true`\n* `hostPID: true`\n"
  },
  {
    "question": "Do init containers support lifecycle and readiness probes?",
    "answer": "Init containers do not support lifecycle, livenessProbe, readinessProbe, or startupProbe, whereas sidecar containers support all these probes.",
    "uuid": "54fadf58-20c3-4458-a642-1125c157ce38",
    "question_with_context": "A user asked the following question:\nQuestion: Do init containers support lifecycle and readiness probes?\nThis is about the following runbook:\nRunbook Title: Differences from sidecar containers\nRunbook Content: Understanding init containersDifferences from sidecar containersInit containers run and complete their tasks before the main application container starts.\nUnlike [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers),\ninit containers are not continuously running alongside the main containers.  \nInit containers run to completion sequentially, and the main container does not start\nuntil all the init containers have successfully completed.  \ninit containers do not support `lifecycle`, `livenessProbe`, `readinessProbe`, or\n`startupProbe` whereas sidecar containers support all these [probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe) to control their lifecycle.  \nInit containers share the same resources (CPU, memory, network) with the main application\ncontainers but do not interact directly with them. They can, however, use shared volumes\nfor data exchange.\n"
  },
  {
    "question": "What happens to a process running as root in a container with user namespaces enabled?",
    "answer": "With user namespaces enabled, a process running as root in a container has full privileges for operations inside the user namespace but is unprivileged for operations outside the namespace.",
    "uuid": "6bf2c3e3-0d16-4007-b2e8-9b01b01b6b3a",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to a process running as root in a container with user namespaces enabled?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: User Namespaces\nreviewers:\ncontent_type: concept\nweight: 160\nmin-kubernetes-server-version: v1.25\n---  \n<!-- overview -->\n{{< feature-state for_k8s_version=\"v1.30\" state=\"beta\" >}}  \nThis page explains how user namespaces are used in Kubernetes pods. A user\nnamespace isolates the user running inside the container from the one\nin the host.  \nA process running as root in a container can run as a different (non-root) user\nin the host; in other words, the process has full privileges for operations\ninside the user namespace, but is unprivileged for operations outside the\nnamespace.  \nYou can use this feature to reduce the damage a compromised container can do to\nthe host or other pods in the same node. There are [several security\nvulnerabilities][KEP-vulns] rated either **HIGH** or **CRITICAL** that were not\nexploitable when user namespaces is active. It is expected user namespace will\nmitigate some future vulnerabilities too.  \n[KEP-vulns]: https://github.com/kubernetes/enhancements/tree/217d790720c5aef09b8bd4d6ca96284a0affe6c2/keps/sig-node/127-user-namespaces#motivation  \n<!-- body -->\n"
  },
  {
    "question": "What happens if I don't specify CPU and memory limits for my container when using the downward API?",
    "answer": "If CPU and memory limits are not specified, the kubelet defaults to exposing the maximum allocatable value for CPU and memory based on the node allocatable calculation.",
    "uuid": "3680aa14-032e-40b8-95f6-e57cfc5b282b",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I don't specify CPU and memory limits for my container when using the downward API?\nThis is about the following runbook:\nRunbook Title: Information available via `resourceFieldRef` {#downwardapi-resourceFieldRef}\nRunbook Content: Available fieldsInformation available via `resourceFieldRef` {#downwardapi-resourceFieldRef}These container-level fields allow you to provide information about\n[requests and limits](/docs/concepts/configuration/manage-resources-containers/#requests-and-limits)\nfor resources such as CPU and memory.  \n`resource: limits.cpu`\n: A container's CPU limit  \n`resource: requests.cpu`\n: A container's CPU request  \n`resource: limits.memory`\n: A container's memory limit  \n`resource: requests.memory`\n: A container's memory request  \n`resource: limits.hugepages-*`\n: A container's hugepages limit  \n`resource: requests.hugepages-*`\n: A container's hugepages request  \n`resource: limits.ephemeral-storage`\n: A container's ephemeral-storage limit  \n`resource: requests.ephemeral-storage`\n: A container's ephemeral-storage request  \n#### Fallback information for resource limits  \nIf CPU and memory limits are not specified for a container, and you use the\ndownward API to try to expose that information, then the\nkubelet defaults to exposing the maximum allocatable value for CPU and memory\nbased on the [node allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)\ncalculation.\n"
  },
  {
    "question": "How can I isolate a pod from a ReplicationController for debugging?",
    "answer": "You can isolate a pod from a ReplicationController by changing its labels.",
    "uuid": "91779434-b52a-40d9-a29d-643d87afe40d",
    "question_with_context": "A user asked the following question:\nQuestion: How can I isolate a pod from a ReplicationController for debugging?\nThis is about the following runbook:\nRunbook Title: Isolating pods from a ReplicationController\nRunbook Content: Working with ReplicationControllersIsolating pods from a ReplicationControllerPods may be removed from a ReplicationController's target set by changing their labels. This technique may be used to remove pods from service for debugging and data recovery. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).\n"
  },
  {
    "question": "How do I set up a partitioned rolling update for my StatefulSet?",
    "answer": "You can set up a partitioned rolling update by specifying `.spec.updateStrategy.rollingUpdate.partition` in your StatefulSet configuration.",
    "uuid": "a692f847-44bb-49bd-aedd-77599d423f11",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set up a partitioned rolling update for my StatefulSet?\nThis is about the following runbook:\nRunbook Title: Partitioned rolling updates {#partitions}\nRunbook Content: Rolling UpdatesPartitioned rolling updates {#partitions}The `RollingUpdate` update strategy can be partitioned, by specifying a\n`.spec.updateStrategy.rollingUpdate.partition`. If a partition is specified, all Pods with an\nordinal that is greater than or equal to the partition will be updated when the StatefulSet's\n`.spec.template` is updated. All Pods with an ordinal that is less than the partition will not\nbe updated, and, even if they are deleted, they will be recreated at the previous version. If a\nStatefulSet's `.spec.updateStrategy.rollingUpdate.partition` is greater than its `.spec.replicas`,\nupdates to its `.spec.template` will not be propagated to its Pods.\nIn most cases you will not need to use a partition, but they are useful if you want to stage an\nupdate, roll out a canary, or perform a phased roll out.\n"
  },
  {
    "question": "Are there any limitations when using the patch or replace operations on Pods?",
    "answer": "Yes, there are limitations. You cannot change immutable metadata fields like `namespace`, `name`, `uid`, or `creationTimestamp`. Additionally, if `metadata.deletionTimestamp` is set, you cannot add new entries to `metadata.finalizers`.",
    "uuid": "0992d191-58e7-4970-91a0-890b5b2cb360",
    "question_with_context": "A user asked the following question:\nQuestion: Are there any limitations when using the patch or replace operations on Pods?\nThis is about the following runbook:\nRunbook Title: Pod update and replacement\nRunbook Content: Pod update and replacementAs mentioned in the previous section, when the Pod template for a workload\nresource is changed, the controller creates new Pods based on the updated\ntemplate instead of updating or patching the existing Pods.  \nKubernetes doesn't prevent you from managing Pods directly. It is possible to\nupdate some fields of a running Pod, in place. However, Pod update operations\nlike\n[`patch`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#patch-pod-v1-core), and\n[`replace`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#replace-pod-v1-core)\nhave some limitations:  \n- Most of the metadata about a Pod is immutable. For example, you cannot\nchange the `namespace`, `name`, `uid`, or `creationTimestamp` fields;\nthe `generation` field is unique. It only accepts updates that increment the\nfield's current value.\n- If the `metadata.deletionTimestamp` is set, no new entry can be added to the\n`metadata.finalizers` list.\n- Pod updates may not change fields other than `spec.containers[*].image`,\n`spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or\n`spec.tolerations`. For `spec.tolerations`, you can only add new entries.\n- When updating the `spec.activeDeadlineSeconds` field, two types of updates\nare allowed:  \n1. setting the unassigned field to a positive number;\n1. updating the field from a positive number to a smaller, non-negative\nnumber.\n"
  },
  {
    "question": "Can you give an example of when to use a DaemonSet?",
    "answer": "An example of using a DaemonSet is for network plugins, which often include a component that runs as a DaemonSet to ensure that the node has working cluster networking.",
    "uuid": "688caeb4-c255-463b-92f5-f68bfe6bb305",
    "question_with_context": "A user asked the following question:\nQuestion: Can you give an example of when to use a DaemonSet?\nThis is about the following runbook:\nRunbook Title: Deployments\nRunbook Content: Alternatives to DaemonSetDeploymentsDaemonSets are similar to [Deployments](/docs/concepts/workloads/controllers/deployment/) in that\nthey both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,\nstorage servers).  \nUse a Deployment for stateless services, like frontends, where scaling up and down the\nnumber of replicas and rolling out updates are more important than controlling exactly which host\nthe Pod runs on.  Use a DaemonSet when it is important that a copy of a Pod always run on\nall or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that particular node.  \nFor example, [network plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/) often include a component that runs as a DaemonSet. The DaemonSet component makes sure that the node where it's running has working cluster networking.\n"
  },
  {
    "question": "What does it mean if my deployment has failed to progress?",
    "answer": "If your deployment has failed to progress, it indicates that it is not moving forward as expected.",
    "uuid": "8a84aa43-65af-4e4e-84fb-c7dc462327a9",
    "question_with_context": "A user asked the following question:\nQuestion: What does it mean if my deployment has failed to progress?\nThis is about the following runbook:\nRunbook Title: Deployment status\nRunbook Content: Deployment statusA Deployment enters various states during its lifecycle. It can be [progressing](#progressing-deployment) while\nrolling out a new ReplicaSet, it can be [complete](#complete-deployment), or it can [fail to progress](#failed-deployment).\n"
  },
  {
    "question": "Where can I find an example of a DaemonSet YAML file?",
    "answer": "You can find an example of a DaemonSet YAML file at the URL: https://k8s.io/examples/controllers/daemonset.yaml.",
    "uuid": "12883777-a181-4e6f-8df0-c02fc7d6b296",
    "question_with_context": "A user asked the following question:\nQuestion: Where can I find an example of a DaemonSet YAML file?\nThis is about the following runbook:\nRunbook Title: Create a DaemonSet\nRunbook Content: Writing a DaemonSet SpecCreate a DaemonSetYou can describe a DaemonSet in a YAML file. For example, the `daemonset.yaml` file below\ndescribes a DaemonSet that runs the fluentd-elasticsearch Docker image:  \n{{% code_sample file=\"controllers/daemonset.yaml\" %}}  \nCreate a DaemonSet based on the YAML file:  \n```\nkubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml\n```\n"
  },
  {
    "question": "Can the number of running pods change after I've set the replicas?",
    "answer": "Yes, the number of running pods may be higher or lower than the specified replicas due to increases or decreases in replicas, or if a pod is gracefully shutdown and a replacement starts early.",
    "uuid": "3bf22c2a-7ed8-4ab0-9407-21cf9e46f3a7",
    "question_with_context": "A user asked the following question:\nQuestion: Can the number of running pods change after I've set the replicas?\nThis is about the following runbook:\nRunbook Title: Multiple Replicas\nRunbook Content: Writing a ReplicationController ManifestMultiple ReplicasYou can specify how many pods should run concurrently by setting `.spec.replicas` to the number\nof pods you would like to have running concurrently.  The number running at any time may be higher\nor lower, such as if the replicas were just increased or decreased, or if a pod is gracefully\nshutdown, and a replacement starts early.  \nIf you do not specify `.spec.replicas`, then it defaults to 1.\n"
  },
  {
    "question": "What should I consider when setting up Init containers regarding resource limits?",
    "answer": "When setting up Init containers, you should consider the effective Pod request and limit for resource allocations, as these will dictate the behavior of the cgroups.",
    "uuid": "0f4f5e09-e821-48f7-8fd6-1a167a74805e",
    "question_with_context": "A user asked the following question:\nQuestion: What should I consider when setting up Init containers regarding resource limits?\nThis is about the following runbook:\nRunbook Title: Init containers and Linux cgroups {#cgroups}\nRunbook Content: Detailed behaviorInit containers and Linux cgroups {#cgroups}On Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod\nrequest and limit, the same as the scheduler.  \n{{< comment >}}\nThis section also present under [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/) page.\nIf you're editing this section, change both places.\n{{< /comment >}}\n"
  },
  {
    "question": "What should I do next if I want to configure user namespaces in my pod?",
    "answer": "You should refer to the documentation on [Use a User Namespace With a Pod](/docs/tasks/configure-pod-container/user-namespaces/).",
    "uuid": "10bdcefa-ab0e-4b4e-86cc-1330024de596",
    "question_with_context": "A user asked the following question:\nQuestion: What should I do next if I want to configure user namespaces in my pod?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Take a look at [Use a User Namespace With a Pod](/docs/tasks/configure-pod-container/user-namespaces/)\n"
  },
  {
    "question": "How can I change the number of replicas in a ReplicationController?",
    "answer": "You can change the number of replicas by updating the `replicas` field in the ReplicationController.",
    "uuid": "bdd251a4-292a-412e-8468-18c072a7d29f",
    "question_with_context": "A user asked the following question:\nQuestion: How can I change the number of replicas in a ReplicationController?\nThis is about the following runbook:\nRunbook Title: Scaling\nRunbook Content: Common usage patternsScalingThe ReplicationController enables scaling the number of replicas up or down, either manually or by an auto-scaling control agent, by updating the `replicas` field.\n"
  },
  {
    "question": "What happens when I force delete a Pod?",
    "answer": "When you force delete a Pod, the API server removes the Pod immediately without waiting for confirmation from the kubelet that the Pod has been terminated. The Pod may continue to run on the cluster indefinitely.",
    "uuid": "ef9926ff-1785-4fce-ad48-0e54d5164076",
    "question_with_context": "A user asked the following question:\nQuestion: What happens when I force delete a Pod?\nThis is about the following runbook:\nRunbook Title: Forced Pod termination {#pod-termination-forced}\nRunbook Content: Termination of Pods {#pod-termination}Forced Pod termination {#pod-termination-forced}{{< caution >}}\nForced deletions can be potentially disruptive for some workloads and their Pods.\n{{< /caution >}}  \nBy default, all deletes are graceful within 30 seconds. The `kubectl delete` command supports\nthe `--grace-period=<seconds>` option which allows you to override the default and specify your\nown value.  \nSetting the grace period to `0` forcibly and immediately deletes the Pod from the API\nserver. If the Pod was still running on a node, that forcible deletion triggers the kubelet to\nbegin immediate cleanup.  \nUsing kubectl, You must specify an additional flag `--force` along with `--grace-period=0`\nin order to perform force deletions.  \nWhen a force deletion is performed, the API server does not wait for confirmation\nfrom the kubelet that the Pod has been terminated on the node it was running on. It\nremoves the Pod in the API immediately so a new Pod can be created with the same\nname. On the node, Pods that are set to terminate immediately will still be given\na small grace period before being force killed.  \n{{< caution >}}\nImmediate deletion does not wait for confirmation that the running resource has been terminated.\nThe resource may continue to run on the cluster indefinitely.\n{{< /caution >}}  \nIf you need to force-delete Pods that are part of a StatefulSet, refer to the task\ndocumentation for\n[deleting Pods from a StatefulSet](/docs/tasks/run-application/force-delete-stateful-set-pod/).\n"
  },
  {
    "question": "What should I use instead of ReplicationController for managing workloads?",
    "answer": "You should use a Deployment that configures a ReplicaSet for managing workloads instead of a ReplicationController.",
    "uuid": "7e24bd6f-2b68-4c2a-9c56-0786f9738fc0",
    "question_with_context": "A user asked the following question:\nQuestion: What should I use instead of ReplicationController for managing workloads?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- bprashanth\n- janetkuo\ntitle: ReplicationController\napi_metadata:\n- apiVersion: \"v1\"\nkind: \"ReplicationController\"\ncontent_type: concept\nweight: 90\ndescription: >-\nLegacy API for managing workloads that can scale horizontally.\nSuperseded by the Deployment and ReplicaSet APIs.\n---  \n<!-- overview -->  \n{{< note >}}\nA [`Deployment`](/docs/concepts/workloads/controllers/deployment/) that configures a [`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is now the recommended way to set up replication.\n{{< /note >}}  \nA _ReplicationController_ ensures that a specified number of pod replicas are running at any one\ntime. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is\nalways up and available.  \n<!-- body -->\n"
  },
  {
    "question": "What type of workload is a Job suitable for?",
    "answer": "A Job is suitable for batch jobs that are expected to terminate on their own.",
    "uuid": "413c0909-0246-46a5-ade6-19ebdbf884ff",
    "question_with_context": "A user asked the following question:\nQuestion: What type of workload is a Job suitable for?\nThis is about the following runbook:\nRunbook Title: Job\nRunbook Content: Alternatives to ReplicaSetJobUse a [`Job`](/docs/concepts/workloads/controllers/job/) instead of a ReplicaSet for Pods that are\nexpected to terminate on their own (that is, batch jobs).\n"
  },
  {
    "question": "What format do the names of the added Pod conditions need to follow?",
    "answer": "The names of the added Pod conditions must meet the Kubernetes label key format.",
    "uuid": "80aa7fcf-1153-4512-888e-5ad9a9ee7e49",
    "question_with_context": "A user asked the following question:\nQuestion: What format do the names of the added Pod conditions need to follow?\nThis is about the following runbook:\nRunbook Title: Pod readiness {#pod-readiness-gate}\nRunbook Content: Pod conditionsPod readiness {#pod-readiness-gate}{{< feature-state for_k8s_version=\"v1.14\" state=\"stable\" >}}  \nYour application can inject extra feedback or signals into PodStatus:\n_Pod readiness_. To use this, set `readinessGates` in the Pod's `spec` to\nspecify a list of additional conditions that the kubelet evaluates for Pod readiness.  \nReadiness gates are determined by the current state of `status.condition`\nfields for the Pod. If Kubernetes cannot find such a condition in the\n`status.conditions` field of a Pod, the status of the condition\nis defaulted to \"`False`\".  \nHere is an example:  \n```yaml\nkind: Pod\n...\nspec:\nreadinessGates:\n- conditionType: \"www.example.com/feature-1\"\nstatus:\nconditions:\n- type: Ready                              # a built in PodCondition\nstatus: \"False\"\nlastProbeTime: null\nlastTransitionTime: 2018-01-01T00:00:00Z\n- type: \"www.example.com/feature-1\"        # an extra PodCondition\nstatus: \"False\"\nlastProbeTime: null\nlastTransitionTime: 2018-01-01T00:00:00Z\ncontainerStatuses:\n- containerID: docker://abcd...\nready: true\n...\n```  \nThe Pod conditions you add must have names that meet the Kubernetes\n[label key format](/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set).\n"
  },
  {
    "question": "What happens to BestEffort Pods when the node is under resource pressure?",
    "answer": "The kubelet prefers to evict `BestEffort` Pods if the node comes under resource pressure.",
    "uuid": "e78046a9-5089-4963-892b-d25b03565051",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to BestEffort Pods when the node is under resource pressure?\nThis is about the following runbook:\nRunbook Title: BestEffort\nRunbook Content: Quality of Service classesBestEffortPods in the `BestEffort` QoS class can use node resources that aren't specifically assigned\nto Pods in other QoS classes. For example, if you have a node with 16 CPU cores available to the\nkubelet, and you assign 4 CPU cores to a `Guaranteed` Pod, then a Pod in the `BestEffort`\nQoS class can try to use any amount of the remaining 12 CPU cores.  \nThe kubelet prefers to evict `BestEffort` Pods if the node comes under resource pressure.  \n#### Criteria  \nA Pod has a QoS class of `BestEffort` if it doesn't meet the criteria for either `Guaranteed`\nor `Burstable`. In other words, a Pod is `BestEffort` only if none of the Containers in the Pod have a\nmemory limit or a memory request, and none of the Containers in the Pod have a\nCPU limit or a CPU request.\nContainers in a Pod can request other resources (not CPU or memory) and still be classified as\n`BestEffort`.\n"
  },
  {
    "question": "How does the Pod garbage collector handle non-terminal Pods during disruption?",
    "answer": "The Pod garbage collector will mark non-terminal Pods as failed if they are being cleaned up, in addition to cleaning them up.",
    "uuid": "062a9a04-fb58-4ece-aaad-e52b5b1638e4",
    "question_with_context": "A user asked the following question:\nQuestion: How does the Pod garbage collector handle non-terminal Pods during disruption?\nThis is about the following runbook:\nRunbook Title: Pod disruption conditions {#pod-disruption-conditions}\nRunbook Content: Pod disruption conditions {#pod-disruption-conditions}{{< feature-state feature_gate_name=\"PodDisruptionConditions\" >}}  \nA dedicated Pod `DisruptionTarget` [condition](/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions)\nis added to indicate\nthat the Pod is about to be deleted due to a {{<glossary_tooltip term_id=\"disruption\" text=\"disruption\">}}.\nThe `reason` field of the condition additionally\nindicates one of the following reasons for the Pod termination:  \n`PreemptionByScheduler`\n: Pod is due to be {{<glossary_tooltip term_id=\"preemption\" text=\"preempted\">}} by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see [Pod priority preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).  \n`DeletionByTaintManager`\n: Pod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within `kube-controller-manager`) due to a `NoExecute` taint that the Pod does not tolerate; see {{<glossary_tooltip term_id=\"taint\" text=\"taint\">}}-based evictions.  \n`EvictionByEvictionAPI`\n: Pod has been marked for {{<glossary_tooltip term_id=\"api-eviction\" text=\"eviction using the Kubernetes API\">}} .  \n`DeletionByPodGC`\n: Pod, that is bound to a no longer existing Node, is due to be deleted by [Pod garbage collection](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection).  \n`TerminationByKubelet`\n: Pod has been terminated by the kubelet, because of either {{<glossary_tooltip term_id=\"node-pressure-eviction\" text=\"node pressure eviction\">}},\nthe [graceful node shutdown](/docs/concepts/architecture/nodes/#graceful-node-shutdown),\nor preemption for [system critical pods](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).  \nIn all other disruption scenarios, like eviction due to exceeding\n[Pod container limits](/docs/concepts/configuration/manage-resources-containers/),\nPods don't receive the `DisruptionTarget` condition because the disruptions were\nprobably caused by the Pod and would reoccur on retry.  \n{{< note >}}\nA Pod disruption might be interrupted. The control plane might re-attempt to\ncontinue the disruption of the same Pod, but it is not guaranteed. As a result,\nthe `DisruptionTarget` condition might be added to a Pod, but that Pod might then not actually be\ndeleted. In such a situation, after some time, the\nPod disruption condition will be cleared.\n{{< /note >}}  \nAlong with cleaning up the pods, the Pod garbage collector (PodGC) will also mark them as failed if they are in a non-terminal\nphase (see also [Pod garbage collection](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)).  \nWhen using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job's\n[Pod failure policy](/docs/concepts/workloads/controllers/job#pod-failure-policy).\n"
  },
  {
    "question": "What is the relationship between the lifetime of a Pod in a DaemonSet and the machine?",
    "answer": "The Pod's lifetime is tied to the machine's lifetime: it needs to be running on the machine before other Pods start and can be safely terminated when the machine is ready to be rebooted or shutdown.",
    "uuid": "b8c7d5a8-b648-448e-9077-d2f5c2cd0a42",
    "question_with_context": "A user asked the following question:\nQuestion: What is the relationship between the lifetime of a Pod in a DaemonSet and the machine?\nThis is about the following runbook:\nRunbook Title: DaemonSet\nRunbook Content: Alternatives to ReplicaSetDaemonSetUse a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicaSet for Pods that provide a\nmachine-level function, such as machine monitoring or machine logging. These Pods have a lifetime that is tied\nto a machine lifetime: the Pod needs to be running on the machine before other Pods start, and are\nsafe to terminate when the machine is otherwise ready to be rebooted/shutdown.\n"
  },
  {
    "question": "Can I use CRON_TZ in my CronJob schedule?",
    "answer": "No, specifying a timezone using CRON_TZ or TZ variables inside .spec.schedule is not officially supported.",
    "uuid": "e75b4463-88f4-44fe-8263-02749a8f8b27",
    "question_with_context": "A user asked the following question:\nQuestion: Can I use CRON_TZ in my CronJob schedule?\nThis is about the following runbook:\nRunbook Title: Unsupported TimeZone specification\nRunbook Content: CronJob limitations {#cron-job-limitations}Unsupported TimeZone specificationSpecifying a timezone using `CRON_TZ` or `TZ` variables inside `.spec.schedule`\nis **not officially supported** (and never has been).  \nStarting with Kubernetes 1.29 if you try to set a schedule that includes `TZ` or `CRON_TZ`\ntimezone specification, Kubernetes will fail to create the resource with a validation\nerror.\nUpdates to CronJobs already using `TZ` or `CRON_TZ` will continue to report a\n[warning](/blog/2020/09/03/warnings/) to the client.\n"
  },
  {
    "question": "How can I check the number of terminating Pods for a Job?",
    "answer": "You can check the number of terminating Pods for a Job by inspecting the `.status.terminating` field. Use the command `kubectl get jobs/myjob -o yaml` to see the status, which will show the number of Pods currently terminating.",
    "uuid": "c52c1311-e9a1-48eb-80a2-53a64fef65d7",
    "question_with_context": "A user asked the following question:\nQuestion: How can I check the number of terminating Pods for a Job?\nThis is about the following runbook:\nRunbook Title: Delayed creation of replacement pods {#pod-replacement-policy}\nRunbook Content: Advanced usageDelayed creation of replacement pods {#pod-replacement-policy}{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \n{{< note >}}\nYou can only set `podReplacementPolicy` on Jobs if you enable the `JobPodReplacementPolicy`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n(enabled by default).\n{{< /note >}}  \nBy default, the Job controller recreates Pods as soon they either fail or are terminating (have a deletion timestamp).\nThis means that, at a given time, when some of the Pods are terminating, the number of running Pods for a Job\ncan be greater than `parallelism` or greater than one Pod per index (if you are using an Indexed Job).  \nYou may choose to create replacement Pods only when the terminating Pod is fully terminal (has `status.phase: Failed`).\nTo do this, set the `.spec.podReplacementPolicy: Failed`.\nThe default replacement policy depends on whether the Job has a `podFailurePolicy` set.\nWith no Pod failure policy defined for a Job, omitting the `podReplacementPolicy` field selects the\n`TerminatingOrFailed` replacement policy:\nthe control plane creates replacement Pods immediately upon Pod deletion\n(as soon as the control plane sees that a Pod for this Job has `deletionTimestamp` set).\nFor Jobs with a Pod failure policy set, the default  `podReplacementPolicy` is `Failed`, and no other\nvalue is permitted.\nSee [Pod failure policy](#pod-failure-policy) to learn more about Pod failure policies for Jobs.  \n```yaml\nkind: Job\nmetadata:\nname: new\n...\nspec:\npodReplacementPolicy: Failed\n...\n```  \nProvided your cluster has the feature gate enabled, you can inspect the `.status.terminating` field of a Job.\nThe value of the field is the number of Pods owned by the Job that are currently terminating.  \n```shell\nkubectl get jobs/myjob -o yaml\n```  \n```yaml\napiVersion: batch/v1\nkind: Job\n# .metadata and .spec omitted\nstatus:\nterminating: 3 # three Pods are terminating and have not yet reached the Failed phase\n```\n"
  },
  {
    "question": "Is it possible to roll back a failed deployment?",
    "answer": "Yes, you can roll back to a previous revision if the deployment has failed.",
    "uuid": "73d546f1-e7c9-4a97-824d-70bea90a3839",
    "question_with_context": "A user asked the following question:\nQuestion: Is it possible to roll back a failed deployment?\nThis is about the following runbook:\nRunbook Title: Operating on a failed deployment\nRunbook Content: Deployment statusOperating on a failed deploymentAll actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back\nto a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.\n"
  },
  {
    "question": "Can I directly manipulate ReplicaSet objects?",
    "answer": "You may never need to manipulate ReplicaSet objects; it's recommended to use a Deployment instead.",
    "uuid": "fc0d9609-0ccb-4237-9c10-570ce16e53c1",
    "question_with_context": "A user asked the following question:\nQuestion: Can I directly manipulate ReplicaSet objects?\nThis is about the following runbook:\nRunbook Title: When to use a ReplicaSet\nRunbook Content: When to use a ReplicaSetA ReplicaSet ensures that a specified number of pod replicas are running at any given\ntime. However, a Deployment is a higher-level concept that manages ReplicaSets and\nprovides declarative updates to Pods along with a lot of other useful features.\nTherefore, we recommend using Deployments instead of directly using ReplicaSets, unless\nyou require custom update orchestration or don't require updates at all.  \nThis actually means that you may never need to manipulate ReplicaSet objects:\nuse a Deployment instead, and define your application in the spec section.\n"
  },
  {
    "question": "How can I ensure stable storage for my StatefulSet Pods?",
    "answer": "You can ensure stable storage for your StatefulSet Pods by using `volumeClaimTemplates` in the StatefulSet spec, which provisions PersistentVolumes for stable storage.",
    "uuid": "c1b09876-14af-4bea-8766-05785da9c28d",
    "question_with_context": "A user asked the following question:\nQuestion: How can I ensure stable storage for my StatefulSet Pods?\nThis is about the following runbook:\nRunbook Title: Components\nRunbook Content: ComponentsThe example below demonstrates the components of a StatefulSet.  \n```yaml\napiVersion: v1\nkind: Service\nmetadata:\nname: nginx\nlabels:\napp: nginx\nspec:\nports:\n- port: 80\nname: web\nclusterIP: None\nselector:\napp: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nname: web\nspec:\nselector:\nmatchLabels:\napp: nginx # has to match .spec.template.metadata.labels\nserviceName: \"nginx\"\nreplicas: 3 # by default is 1\nminReadySeconds: 10 # by default is 0\ntemplate:\nmetadata:\nlabels:\napp: nginx # has to match .spec.selector.matchLabels\nspec:\nterminationGracePeriodSeconds: 10\ncontainers:\n- name: nginx\nimage: registry.k8s.io/nginx-slim:0.24\nports:\n- containerPort: 80\nname: web\nvolumeMounts:\n- name: www\nmountPath: /usr/share/nginx/html\nvolumeClaimTemplates:\n- metadata:\nname: www\nspec:\naccessModes: [ \"ReadWriteOnce\" ]\nstorageClassName: \"my-storage-class\"\nresources:\nrequests:\nstorage: 1Gi\n```  \n{{< note >}}\nThis example uses the `ReadWriteOnce` access mode, for simplicity. For\nproduction use, the Kubernetes project recommends using the `ReadWriteOncePod`\naccess mode instead.\n{{< /note >}}  \nIn the above example:  \n* A Headless Service, named `nginx`, is used to control the network domain.\n* The StatefulSet, named `web`, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.\n* The `volumeClaimTemplates` will provide stable storage using\n[PersistentVolumes](/docs/concepts/storage/persistent-volumes/) provisioned by a\nPersistentVolume Provisioner.  \nThe name of a StatefulSet object must be a valid\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\n"
  },
  {
    "question": "What types of Pods does the Pod garbage collector clean up?",
    "answer": "PodGC cleans up terminated Pods with a phase of `Succeeded` or `Failed`, orphan Pods bound to a non-existent node, unscheduled terminating Pods, and terminating Pods bound to a non-ready node tainted with `node.kubernetes.io/out-of-service` when the `NodeOutOfServiceVolumeDetach` feature gate is enabled.",
    "uuid": "7d1a7883-7821-4838-bee9-2a05a08eb3e7",
    "question_with_context": "A user asked the following question:\nQuestion: What types of Pods does the Pod garbage collector clean up?\nThis is about the following runbook:\nRunbook Title: Garbage collection of Pods {#pod-garbage-collection}\nRunbook Content: Termination of Pods {#pod-termination}Garbage collection of Pods {#pod-garbage-collection}For failed Pods, the API objects remain in the cluster's API until a human or\n{{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}} process\nexplicitly removes them.  \nThe Pod garbage collector (PodGC), which is a controller in the control plane, cleans up\nterminated Pods (with a phase of `Succeeded` or `Failed`), when the number of Pods exceeds the\nconfigured threshold (determined by `terminated-pod-gc-threshold` in the kube-controller-manager).\nThis avoids a resource leak as Pods are created and terminated over time.  \nAdditionally, PodGC cleans up any Pods which satisfy any of the following conditions:  \n1. are orphan Pods - bound to a node which no longer exists,\n1. are unscheduled terminating Pods,\n1. are terminating Pods, bound to a non-ready node tainted with\n[`node.kubernetes.io/out-of-service`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-out-of-service),\nwhen the `NodeOutOfServiceVolumeDetach` feature gate is enabled.  \nAlong with cleaning up the Pods, PodGC will also mark them as failed if they are in a non-terminal\nphase. Also, PodGC adds a Pod disruption condition when cleaning up an orphan Pod.\nSee [Pod disruption conditions](/docs/concepts/workloads/pods/disruptions#pod-disruption-conditions)\nfor more details.\n"
  },
  {
    "question": "What should I be cautious about when setting labels in a Pod Template?",
    "answer": "Be careful not to overlap with the selectors of other controllers, as they might try to adopt this Pod.",
    "uuid": "f5fef35e-c8c9-4faf-91b0-30b8c16870df",
    "question_with_context": "A user asked the following question:\nQuestion: What should I be cautious about when setting labels in a Pod Template?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a ReplicaSet manifestPod TemplateThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates) which is also\nrequired to have labels in place. In our `frontend.yaml` example we had one label: `tier: frontend`.\nBe careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.  \nFor the template's [restart policy](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) field,\n`.spec.template.spec.restartPolicy`, the only allowed value is `Always`, which is the default.\n"
  },
  {
    "question": "What happens to Pods when there are not enough resources on a Node?",
    "answer": "When there are not enough resources on a Node, Kubernetes uses the QoS class to determine which Pods to evict.",
    "uuid": "84d61461-8884-4f57-83d6-7cdfc1348080",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to Pods when there are not enough resources on a Node?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: Pod Quality of Service Classes\ncontent_type: concept\nweight: 85\n---  \n<!-- overview -->  \nThis page introduces _Quality of Service (QoS) classes_ in Kubernetes, and explains\nhow Kubernetes assigns a QoS class to each Pod as a consequence of the resource\nconstraints that you specify for the containers in that Pod. Kubernetes relies on this\nclassification to make decisions about which Pods to evict when there are not enough\navailable resources on a Node.  \n<!-- body -->\n"
  },
  {
    "question": "How can I fix a broken StatefulSet rollout after reverting the Pod template?",
    "answer": "After reverting the template, you must delete the Pods that were created with the bad configuration, and then StatefulSet will begin to recreate the Pods using the reverted template.",
    "uuid": "10973aa5-42f5-4429-b4da-5de9cc6ae0c6",
    "question_with_context": "A user asked the following question:\nQuestion: How can I fix a broken StatefulSet rollout after reverting the Pod template?\nThis is about the following runbook:\nRunbook Title: Forced rollback\nRunbook Content: Rolling UpdatesForced rollbackWhen using [Rolling Updates](#rolling-updates) with the default\n[Pod Management Policy](#pod-management-policies) (`OrderedReady`),\nit's possible to get into a broken state that requires manual intervention to repair.  \nIf you update the Pod template to a configuration that never becomes Running and\nReady (for example, due to a bad binary or application-level configuration error),\nStatefulSet will stop the rollout and wait.  \nIn this state, it's not enough to revert the Pod template to a good configuration.\nDue to a [known issue](https://github.com/kubernetes/kubernetes/issues/67250),\nStatefulSet will continue to wait for the broken Pod to become Ready\n(which never happens) before it will attempt to revert it back to the working\nconfiguration.  \nAfter reverting the template, you must also delete any Pods that StatefulSet had\nalready attempted to run with the bad configuration.\nStatefulSet will then begin to recreate the Pods using the reverted template.\n"
  },
  {
    "question": "When should I use a Deployment instead of a DaemonSet?",
    "answer": "Use a Deployment for stateless services, like frontends, where scaling up and down the number of replicas and rolling out updates are more important than controlling exactly which host the Pod runs on.",
    "uuid": "688caeb4-c255-463b-92f5-f68bfe6bb305",
    "question_with_context": "A user asked the following question:\nQuestion: When should I use a Deployment instead of a DaemonSet?\nThis is about the following runbook:\nRunbook Title: Deployments\nRunbook Content: Alternatives to DaemonSetDeploymentsDaemonSets are similar to [Deployments](/docs/concepts/workloads/controllers/deployment/) in that\nthey both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,\nstorage servers).  \nUse a Deployment for stateless services, like frontends, where scaling up and down the\nnumber of replicas and rolling out updates are more important than controlling exactly which host\nthe Pod runs on.  Use a DaemonSet when it is important that a copy of a Pod always run on\nall or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that particular node.  \nFor example, [network plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/) often include a component that runs as a DaemonSet. The DaemonSet component makes sure that the node where it's running has working cluster networking.\n"
  },
  {
    "question": "Is the ordinal index for each Pod in a StatefulSet unique?",
    "answer": "Yes, each Pod in a StatefulSet is assigned a unique ordinal index.",
    "uuid": "23e34f25-6f4e-4d1a-8bc5-f73c23995c0b",
    "question_with_context": "A user asked the following question:\nQuestion: Is the ordinal index for each Pod in a StatefulSet unique?\nThis is about the following runbook:\nRunbook Title: Ordinal Index\nRunbook Content: Pod IdentityOrdinal IndexFor a StatefulSet with N [replicas](#replicas), each Pod in the StatefulSet\nwill be assigned an integer ordinal, that is unique over the Set. By default,\npods will be assigned ordinals from 0 up through N-1. The StatefulSet controller\nwill also add a pod label with this index: `apps.kubernetes.io/pod-index`.\n"
  },
  {
    "question": "What is the main focus of the guide?",
    "answer": "The main focus of the guide is to help application owners build highly available applications and understand disruptions to Pods.",
    "uuid": "763c9c29-c8fc-4d23-b2aa-b81f5556eb17",
    "question_with_context": "A user asked the following question:\nQuestion: What is the main focus of the guide?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- erictune\n- foxish\n- davidopp\ntitle: Disruptions\ncontent_type: concept\nweight: 70\n---  \n<!-- overview -->\nThis guide is for application owners who want to build\nhighly available applications, and thus need to understand\nwhat types of disruptions can happen to Pods.  \nIt is also for cluster administrators who want to perform automated\ncluster actions, like upgrading and autoscaling clusters.  \n<!-- body -->\n"
  },
  {
    "question": "What happens if I don't set minReadySeconds for my Pod?",
    "answer": "If you don't set `minReadySeconds`, it defaults to 0, meaning the Pod will be considered available as soon as it is ready.",
    "uuid": "0793bfd6-938a-433f-9da3-b0f4eb6550f0",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I don't set minReadySeconds for my Pod?\nThis is about the following runbook:\nRunbook Title: Min Ready Seconds\nRunbook Content: Writing a Deployment SpecMin Ready Seconds`.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly\ncreated Pod should be ready without any of its containers crashing, for it to be considered available.\nThis defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when\na Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).\n"
  },
  {
    "question": "What does the example CronJob manifest do?",
    "answer": "The example CronJob manifest prints the current time and a hello message every minute.",
    "uuid": "3c0bb110-45e9-405e-b115-8f727808b91e",
    "question_with_context": "A user asked the following question:\nQuestion: What does the example CronJob manifest do?\nThis is about the following runbook:\nRunbook Title: Example\nRunbook Content: ExampleThis example CronJob manifest prints the current time and a hello message every minute:  \n{{% code_sample file=\"application/job/cronjob.yaml\" %}}  \n([Running Automated Tasks with a CronJob](/docs/tasks/job/automated-tasks-with-cron-jobs/)\ntakes you through this example in more detail).\n"
  },
  {
    "question": "How do I set up a sidecar container in a Kubernetes Deployment?",
    "answer": "You can set up a sidecar container in a Kubernetes Deployment by defining two containers in the Deployment configuration, where one of them acts as the sidecar.",
    "uuid": "1c8053ad-eb0a-4c2e-b800-cd13e4d87aeb",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set up a sidecar container in a Kubernetes Deployment?\nThis is about the following runbook:\nRunbook Title: Example application {#sidecar-example}\nRunbook Content: Sidecar containers in Kubernetes {#pod-sidecar-containers}Example application {#sidecar-example}Here's an example of a Deployment with two containers, one of which is a sidecar:  \n{{% code_sample language=\"yaml\" file=\"application/deployment-sidecar.yaml\" %}}\n"
  },
  {
    "question": "What do I need to know about scaling a StatefulSet?",
    "answer": "To learn how to scale a StatefulSet, refer to the guide on [scaling a StatefulSet](/docs/tasks/run-application/scale-stateful-set/).",
    "uuid": "7cc8b8c0-c5e0-4035-a800-fb017cb6f40c",
    "question_with_context": "A user asked the following question:\nQuestion: What do I need to know about scaling a StatefulSet?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Find out how to use StatefulSets\n* Follow an example of [deploying a stateful application](/docs/tutorials/stateful-application/basic-stateful-set/).\n* Follow an example of [deploying Cassandra with Stateful Sets](/docs/tutorials/stateful-application/cassandra/).\n* Follow an example of [running a replicated stateful application](/docs/tasks/run-application/run-replicated-stateful-application/).\n* Learn how to [scale a StatefulSet](/docs/tasks/run-application/scale-stateful-set/).\n* Learn what's involved when you [delete a StatefulSet](/docs/tasks/run-application/delete-stateful-set/).\n* Learn how to [configure a Pod to use a volume for storage](/docs/tasks/configure-pod-container/configure-volume-storage/).\n* Learn how to [configure a Pod to use a PersistentVolume for storage](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).\n* `StatefulSet` is a top-level resource in the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/stateful-set-v1\" >}}\nobject definition to understand the API for stateful sets.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how\nyou can use it to manage application availability during disruptions.\n"
  },
  {
    "question": "What's the main purpose of using a Deployment instead of a ReplicaSet directly?",
    "answer": "The main purpose of using a Deployment is to orchestrate Pod creation, deletion, and updates through declarative, server-side rolling updates.",
    "uuid": "ca016763-c204-4ab9-af89-09d3611e05e7",
    "question_with_context": "A user asked the following question:\nQuestion: What's the main purpose of using a Deployment instead of a ReplicaSet directly?\nThis is about the following runbook:\nRunbook Title: Deployment (recommended)\nRunbook Content: Alternatives to ReplicaSetDeployment (recommended)[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is an object which can own ReplicaSets and update\nthem and their Pods via declarative, server-side rolling updates.\nWhile ReplicaSets can be used independently, today they're mainly used by Deployments as a mechanism to orchestrate Pod\ncreation, deletion and updates. When you use Deployments you don't have to worry about managing the ReplicaSets that\nthey create. Deployments own and manage their ReplicaSets.\nAs such, it is recommended to use Deployments when you want ReplicaSets.\n"
  },
  {
    "question": "What command do I use to set up autoscaling for my deployment?",
    "answer": "To set up autoscaling for your deployment, use the command: `kubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80`. This will configure the autoscaler based on CPU utilization.",
    "uuid": "b0b9f323-c01a-447a-b28b-21c481252eff",
    "question_with_context": "A user asked the following question:\nQuestion: What command do I use to set up autoscaling for my deployment?\nThis is about the following runbook:\nRunbook Title: Scaling a Deployment\nRunbook Content: Scaling a DeploymentYou can scale a Deployment by using the following command:  \n```shell\nkubectl scale deployment/nginx-deployment --replicas=10\n```\nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment scaled\n```  \nAssuming [horizontal Pod autoscaling](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) is enabled\nin your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of\nPods you want to run based on the CPU utilization of your existing Pods.  \n```shell\nkubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80\n```\nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment scaled\n```\n"
  },
  {
    "question": "How can I inspect the state of an existing Pod in Kubernetes?",
    "answer": "You can run an ephemeral container in an existing Pod to inspect its state and run arbitrary commands.",
    "uuid": "05c3808b-c5eb-418a-b293-6b59dc3624d6",
    "question_with_context": "A user asked the following question:\nQuestion: How can I inspect the state of an existing Pod in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Understanding ephemeral containers\nRunbook Content: Understanding ephemeral containers{{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} are the fundamental building\nblock of Kubernetes applications. Since Pods are intended to be disposable and\nreplaceable, you cannot add a container to a Pod once it has been created.\nInstead, you usually delete and replace Pods in a controlled fashion using\n{{< glossary_tooltip text=\"deployments\" term_id=\"deployment\" >}}.  \nSometimes it's necessary to inspect the state of an existing Pod, however, for\nexample to troubleshoot a hard-to-reproduce bug. In these cases you can run\nan ephemeral container in an existing Pod to inspect its state and run\narbitrary commands.\n"
  },
  {
    "question": "How do I set the completion mode for a Job in Kubernetes?",
    "answer": "You can set the completion mode for a Job by specifying it in the `.spec.completionMode` field. You can choose between `NonIndexed` (default) or `Indexed`.",
    "uuid": "9bbbccce-658e-4332-b59d-6dd1d95dad1c",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set the completion mode for a Job in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Completion mode\nRunbook Content: Writing a Job specCompletion mode{{< feature-state for_k8s_version=\"v1.24\" state=\"stable\" >}}  \nJobs with _fixed completion count_ - that is, jobs that have non null\n`.spec.completions` - can have a completion mode that is specified in `.spec.completionMode`:  \n- `NonIndexed` (default): the Job is considered complete when there have been\n`.spec.completions` successfully completed Pods. In other words, each Pod\ncompletion is homologous to each other. Note that Jobs that have null\n`.spec.completions` are implicitly `NonIndexed`.\n- `Indexed`: the Pods of a Job get an associated completion index from 0 to\n`.spec.completions-1`. The index is available through four mechanisms:\n- The Pod annotation `batch.kubernetes.io/job-completion-index`.\n- The Pod label `batch.kubernetes.io/job-completion-index` (for v1.28 and later). Note\nthe feature gate `PodIndexLabel` must be enabled to use this label, and it is enabled\nby default.\n- As part of the Pod hostname, following the pattern `$(job-name)-$(index)`.\nWhen you use an Indexed Job in combination with a\n{{< glossary_tooltip term_id=\"Service\" >}}, Pods within the Job can use\nthe deterministic hostnames to address each other via DNS. For more information about\nhow to configure this, see [Job with Pod-to-Pod Communication](/docs/tasks/job/job-with-pod-to-pod-communication/).\n- From the containerized task, in the environment variable `JOB_COMPLETION_INDEX`.  \nThe Job is considered complete when there is one successfully completed Pod\nfor each index. For more information about how to use this mode, see\n[Indexed Job for Parallel Processing with Static Work Assignment](/docs/tasks/job/indexed-parallel-processing-static/).  \n{{< note >}}\nAlthough rare, more than one Pod could be started for the same index (due to various reasons such as node failures,\nkubelet restarts, or Pod evictions). In this case, only the first Pod that completes successfully will\ncount towards the completion count and update the status of the Job. The other Pods that are running\nor completed for the same index will be deleted by the Job controller once they are detected.\n{{< /note >}}\n"
  },
  {
    "question": "How do I set up a Parallel Job with a fixed completion count?",
    "answer": "To set up a Parallel Job with a fixed completion count, you need to specify a non-zero positive value for `.spec.completions`. The Job will be complete when there are `.spec.completions` successful Pods. If you use `.spec.completionMode=\"Indexed\"`, each Pod will receive a different index from 0 to `.spec.completions-1`.",
    "uuid": "5f7bb8dd-10a3-435d-8622-46f6870b83f2",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set up a Parallel Job with a fixed completion count?\nThis is about the following runbook:\nRunbook Title: Parallel execution for Jobs {#parallel-jobs}\nRunbook Content: Writing a Job specParallel execution for Jobs {#parallel-jobs}There are three main types of task suitable to run as a Job:  \n1. Non-parallel Jobs\n- normally, only one Pod is started, unless the Pod fails.\n- the Job is complete as soon as its Pod terminates successfully.\n1. Parallel Jobs with a *fixed completion count*:\n- specify a non-zero positive value for `.spec.completions`.\n- the Job represents the overall task, and is complete when there are `.spec.completions` successful Pods.\n- when using `.spec.completionMode=\"Indexed\"`, each Pod gets a different index in the range 0 to `.spec.completions-1`.\n1. Parallel Jobs with a *work queue*:\n- do not specify `.spec.completions`, default to `.spec.parallelism`.\n- the Pods must coordinate amongst themselves or an external service to determine\nwhat each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.\n- each Pod is independently capable of determining whether or not all its peers are done,\nand thus that the entire Job is done.\n- when _any_ Pod from the Job terminates with success, no new Pods are created.\n- once at least one Pod has terminated with success and all Pods are terminated,\nthen the Job is completed with success.\n- once any Pod has exited with success, no other Pod should still be doing any work\nfor this task or writing any output. They should all be in the process of exiting.  \nFor a _non-parallel_ Job, you can leave both `.spec.completions` and `.spec.parallelism` unset.\nWhen both are unset, both are defaulted to 1.  \nFor a _fixed completion count_ Job, you should set `.spec.completions` to the number of completions needed.\nYou can set `.spec.parallelism`, or leave it unset and it will default to 1.  \nFor a _work queue_ Job, you must leave `.spec.completions` unset, and set `.spec.parallelism` to\na non-negative integer.  \nFor more information about how to make use of the different types of job,\nsee the [job patterns](#job-patterns) section.  \n#### Controlling parallelism  \nThe requested parallelism (`.spec.parallelism`) can be set to any non-negative value.\nIf it is unspecified, it defaults to 1.\nIf it is specified as 0, then the Job is effectively paused until it is increased.  \nActual parallelism (number of pods running at any instant) may be more or less than requested\nparallelism, for a variety of reasons:  \n- For _fixed completion count_ Jobs, the actual number of pods running in parallel will not exceed the number of\nremaining completions. Higher values of `.spec.parallelism` are effectively ignored.\n- For _work queue_ Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete, however.\n- If the Job {{< glossary_tooltip term_id=\"controller\" >}} has not had time to react.\n- If the Job controller failed to create Pods for any reason (lack of `ResourceQuota`, lack of permission, etc.),\nthen there may be fewer pods than requested.\n- The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.\n- When a Pod is gracefully shut down, it takes time to stop.\n"
  },
  {
    "question": "What happens to a Bare Pod if the node it runs on fails?",
    "answer": "When the node that a Pod is running on reboots or fails, the pod is terminated and will not be restarted.",
    "uuid": "8ebb15f2-65aa-4082-89f1-8e231b275549",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to a Bare Pod if the node it runs on fails?\nThis is about the following runbook:\nRunbook Title: Bare Pods\nRunbook Content: AlternativesBare PodsWhen the node that a Pod is running on reboots or fails, the pod is terminated\nand will not be restarted. However, a Job will create new Pods to replace terminated ones.\nFor this reason, we recommend that you use a Job rather than a bare Pod, even if your application\nrequires only a single Pod.\n"
  },
  {
    "question": "Can I include environment variables in my Pod template?",
    "answer": "Yes, you can include environment variables in the Pod template for the containers that run in the Pod.",
    "uuid": "7ddf978c-8e14-4ae2-9438-f37ca848cb9b",
    "question_with_context": "A user asked the following question:\nQuestion: Can I include environment variables in my Pod template?\nThis is about the following runbook:\nRunbook Title: Pod templates\nRunbook Content: Working with PodsPod templatesControllers for {{< glossary_tooltip text=\"workload\" term_id=\"workload\" >}} resources create Pods\nfrom a _pod template_ and manage those Pods on your behalf.  \nPodTemplates are specifications for creating Pods, and are included in workload resources such as\n[Deployments](/docs/concepts/workloads/controllers/deployment/),\n[Jobs](/docs/concepts/workloads/controllers/job/), and\n[DaemonSets](/docs/concepts/workloads/controllers/daemonset/).  \nEach controller for a workload resource uses the `PodTemplate` inside the workload\nobject to make actual Pods. The `PodTemplate` is part of the desired state of whatever\nworkload resource you used to run your app.  \nWhen you create a Pod, you can include\n[environment variables](/docs/tasks/inject-data-application/define-environment-variable-container/)\nin the Pod template for the containers that run in the Pod.  \nThe sample below is a manifest for a simple Job with a `template` that starts one\ncontainer. The container in that Pod prints a message then pauses.  \n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: hello\nspec:\ntemplate:\n# This is the pod template\nspec:\ncontainers:\n- name: hello\nimage: busybox:1.28\ncommand: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600']\nrestartPolicy: OnFailure\n# The pod template ends here\n```  \nModifying the pod template or switching to a new pod template has no direct effect\non the Pods that already exist. If you change the pod template for a workload\nresource, that resource needs to create replacement Pods that use the updated template.  \nFor example, the StatefulSet controller ensures that the running Pods match the current\npod template for each StatefulSet object. If you edit the StatefulSet to change its pod\ntemplate, the StatefulSet starts to create new Pods based on the updated template.\nEventually, all of the old Pods are replaced with new Pods, and the update is complete.  \nEach workload resource implements its own rules for handling changes to the Pod template.\nIf you want to read more about StatefulSet specifically, read\n[Update strategy](/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets) in the StatefulSet Basics tutorial.  \nOn Nodes, the {{< glossary_tooltip term_id=\"kubelet\" text=\"kubelet\" >}} does not\ndirectly observe or manage any of the details around pod templates and updates; those\ndetails are abstracted away. That abstraction and separation of concerns simplifies\nsystem semantics, and makes it feasible to extend the cluster's behavior without\nchanging existing code.\n"
  },
  {
    "question": "When should I use a Job instead of a Replication Controller?",
    "answer": "You should use a Job for managing Pods that are expected to terminate, like batch tasks.",
    "uuid": "8e8361be-2ec5-43d2-991d-11726e6ad39e",
    "question_with_context": "A user asked the following question:\nQuestion: When should I use a Job instead of a Replication Controller?\nThis is about the following runbook:\nRunbook Title: Replication Controller\nRunbook Content: AlternativesReplication ControllerJobs are complementary to [Replication Controllers](/docs/concepts/workloads/controllers/replicationcontroller/).\nA Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Job\nmanages Pods that are expected to terminate (e.g. batch tasks).  \nAs discussed in [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/), `Job` is *only* appropriate\nfor pods with `RestartPolicy` equal to `OnFailure` or `Never`.\n(Note: If `RestartPolicy` is not set, the default value is `Always`.)\n"
  },
  {
    "question": "Can I create other Pods with labels that match my Deployment's selector?",
    "answer": "You should not create other Pods whose labels match this selector, either directly or by creating another Deployment or controller, as it can lead to conflicts and unexpected behavior.",
    "uuid": "72dcff4e-b7af-403e-b9c5-65f77814e35b",
    "question_with_context": "A user asked the following question:\nQuestion: Can I create other Pods with labels that match my Deployment's selector?\nThis is about the following runbook:\nRunbook Title: Selector\nRunbook Content: Writing a Deployment SpecSelector`.spec.selector` is a required field that specifies a [label selector](/docs/concepts/overview/working-with-objects/labels/)\nfor the Pods targeted by this Deployment.  \n`.spec.selector` must match `.spec.template.metadata.labels`, or it will be rejected by the API.  \nIn API version `apps/v1`, `.spec.selector` and `.metadata.labels` do not default to `.spec.template.metadata.labels` if not set. So they must be set explicitly. Also note that `.spec.selector` is immutable after creation of the Deployment in `apps/v1`.  \nA Deployment may terminate Pods whose labels match the selector if their template is different\nfrom `.spec.template` or if the total number of such Pods exceeds `.spec.replicas`. It brings up new\nPods with `.spec.template` if the number of Pods is less than the desired number.  \n{{< note >}}\nYou should not create other Pods whose labels match this selector, either directly, by creating\nanother Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you\ndo so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.\n{{< /note >}}  \nIf you have multiple controllers that have overlapping selectors, the controllers will fight with each\nother and won't behave correctly.\n"
  },
  {
    "question": "What happens if the `preStop` hook is still running after the grace period?",
    "answer": "If the `preStop` hook is still running after the grace period expires, the kubelet requests a small, one-off grace period extension of 2 seconds. If more time is needed, you must modify the `terminationGracePeriodSeconds` to allow for longer completion.",
    "uuid": "1afa0f13-8e39-4fd1-9cb2-1109248c56b3",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if the `preStop` hook is still running after the grace period?\nThis is about the following runbook:\nRunbook Title: Termination of Pods {#pod-termination}\nRunbook Content: Termination of Pods {#pod-termination}Because Pods represent processes running on nodes in the cluster, it is important to\nallow those processes to gracefully terminate when they are no longer needed (rather\nthan being abruptly stopped with a `KILL` signal and having no chance to clean up).  \nThe design aim is for you to be able to request deletion and know when processes\nterminate, but also be able to ensure that deletes eventually complete.\nWhen you request deletion of a Pod, the cluster records and tracks the intended grace period\nbefore the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in\nplace, the {{< glossary_tooltip text=\"kubelet\" term_id=\"kubelet\" >}} attempts graceful\nshutdown.  \nTypically, with this graceful termination of the pod, kubelet makes requests to the container runtime to attempt to stop the containers in the pod by first sending a TERM (aka. SIGTERM) signal, with a grace period timeout, to the main process in each container. The requests to stop the containers are processed by the container runtime asynchronously. There is no guarantee to the order of processing for these requests. Many container runtimes respect the `STOPSIGNAL` value defined in the container image and, if different, send the container image configured STOPSIGNAL instead of TERM.\nOnce the grace period has expired, the KILL signal is sent to any remaining\nprocesses, and the Pod is then deleted from the\n{{< glossary_tooltip text=\"API Server\" term_id=\"kube-apiserver\" >}}. If the kubelet or the\ncontainer runtime's management service is restarted while waiting for processes to terminate, the\ncluster retries from the start including the full original grace period.  \nPod termination flow, illustrated with an example:  \n1. You use the `kubectl` tool to manually delete a specific Pod, with the default grace period\n(30 seconds).  \n1. The Pod in the API server is updated with the time beyond which the Pod is considered \"dead\"\nalong with the grace period.\nIf you use `kubectl describe` to check the Pod you're deleting, that Pod shows up as \"Terminating\".\nOn the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked\nas terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod\nshutdown process.  \n1. If one of the Pod's containers has defined a `preStop`\n[hook](/docs/concepts/containers/container-lifecycle-hooks) and the `terminationGracePeriodSeconds`\nin the Pod spec is not set to 0, the kubelet runs that hook inside of the container.\nThe default `terminationGracePeriodSeconds` setting is 30 seconds.  \nIf the `preStop` hook is still running after the grace period expires, the kubelet requests\na small, one-off grace period extension of 2 seconds.\n{{% note %}}\nIf the `preStop` hook needs longer to complete than the default grace period allows,\nyou must modify `terminationGracePeriodSeconds` to suit this.\n{{% /note %}}  \n1. The kubelet triggers the container runtime to send a TERM signal to process 1 inside each\ncontainer.  \nThere is [special ordering](#termination-with-sidecars) if the Pod has any\n{{< glossary_tooltip text=\"sidecar containers\" term_id=\"sidecar-container\" >}} defined.\nOtherwise, the containers in the Pod receive the TERM signal at different times and in\nan arbitrary order. If the order of shutdowns matters, consider using a `preStop` hook\nto synchronize (or switch to using sidecar containers).  \n1. At the same time as the kubelet is starting graceful shutdown of the Pod, the control plane\nevaluates whether to remove that shutting-down Pod from EndpointSlice (and Endpoints) objects,\nwhere those objects represent a {{< glossary_tooltip term_id=\"service\" text=\"Service\" >}}\nwith a configured {{< glossary_tooltip text=\"selector\" term_id=\"selector\" >}}.\n{{< glossary_tooltip text=\"ReplicaSets\" term_id=\"replica-set\" >}} and other workload resources\nno longer treat the shutting-down Pod as a valid, in-service replica.  \nPods that shut down slowly should not continue to serve regular traffic and should start\nterminating and finish processing open connections.  Some applications need to go beyond\nfinishing open connections and need more graceful termination, for example, session draining\nand completion.  \nAny endpoints that represent the terminating Pods are not immediately removed from\nEndpointSlices, and a status indicating [terminating state](/docs/concepts/services-networking/endpoint-slices/#conditions)\nis exposed from the EndpointSlice API (and the legacy Endpoints API).\nTerminating endpoints always have their `ready` status as `false` (for backward compatibility\nwith versions before 1.26), so load balancers will not use it for regular traffic.  \nIf traffic draining on terminating Pod is needed, the actual readiness can be checked as a\ncondition `serving`.  You can find more details on how to implement connections draining in the\ntutorial [Pods And Endpoints Termination Flow](/docs/tutorials/services/pods-and-endpoint-termination-flow/)  \n<a id=\"pod-termination-beyond-grace-period\" />  \n1. The kubelet ensures the Pod is shut down and terminated\n1. When the grace period expires, if there is still any container running in the Pod, the\nkubelet triggers forcible shutdown.\nThe container runtime sends `SIGKILL` to any processes still running in any container in the Pod.\nThe kubelet also cleans up a hidden `pause` container if that container runtime uses one.\n1. The kubelet transitions the Pod into a terminal phase (`Failed` or `Succeeded` depending on\nthe end state of its containers).\n1. The kubelet triggers forcible removal of the Pod object from the API server, by setting grace period\nto 0 (immediate deletion).\n1. The API server deletes the Pod's API object, which is then no longer visible from any client.\n"
  },
  {
    "question": "Can I control static Pods from the Kubernetes API server?",
    "answer": "No, static Pods are visible on the API server through mirror Pods, but they cannot be controlled from there.",
    "uuid": "f6d2ef40-f07d-4122-976c-ff9d8d9f4bd3",
    "question_with_context": "A user asked the following question:\nQuestion: Can I control static Pods from the Kubernetes API server?\nThis is about the following runbook:\nRunbook Title: Static Pods\nRunbook Content: Static Pods_Static Pods_ are managed directly by the kubelet daemon on a specific node,\nwithout the {{< glossary_tooltip text=\"API server\" term_id=\"kube-apiserver\" >}}\nobserving them.\nWhereas most Pods are managed by the control plane (for example, a\n{{< glossary_tooltip text=\"Deployment\" term_id=\"deployment\" >}}), for static\nPods, the kubelet directly supervises each static Pod (and restarts it if it fails).  \nStatic Pods are always bound to one {{< glossary_tooltip term_id=\"kubelet\" >}} on a specific node.\nThe main use for static Pods is to run a self-hosted control plane: in other words,\nusing the kubelet to supervise the individual [control plane components](/docs/concepts/architecture/#control-plane-components).  \nThe kubelet automatically tries to create a {{< glossary_tooltip text=\"mirror Pod\" term_id=\"mirror-pod\" >}}\non the Kubernetes API server for each static Pod.\nThis means that the Pods running on a node are visible on the API server,\nbut cannot be controlled from there. See the guide [Create static Pods](/docs/tasks/configure-pod-container/static-pod) for more information.  \n{{< note >}}\nThe `spec` of a static Pod cannot refer to other API objects\n(e.g., {{< glossary_tooltip text=\"ServiceAccount\" term_id=\"service-account\" >}},\n{{< glossary_tooltip text=\"ConfigMap\" term_id=\"configmap\" >}},\n{{< glossary_tooltip text=\"Secret\" term_id=\"secret\" >}}, etc).\n{{< /note >}}\n"
  },
  {
    "question": "Can I use macros in my CronJob schedule?",
    "answer": "Yes, you can use macros like `@monthly`, `@weekly`, and `@daily` in your CronJob schedule. For example, `@monthly` runs once a month at midnight on the first day of the month.",
    "uuid": "33b12a5c-c753-485e-ba50-0af0af61aee2",
    "question_with_context": "A user asked the following question:\nQuestion: Can I use macros in my CronJob schedule?\nThis is about the following runbook:\nRunbook Title: Schedule syntax\nRunbook Content: Writing a CronJob specSchedule syntaxThe `.spec.schedule` field is required. The value of that field follows the [Cron](https://en.wikipedia.org/wiki/Cron) syntax:  \n```\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n# \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n# \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31)\n# \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12)\n# \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6) (Sunday to Saturday)\n# \u2502 \u2502 \u2502 \u2502 \u2502                                   OR sun, mon, tue, wed, thu, fri, sat\n# \u2502 \u2502 \u2502 \u2502 \u2502\n# \u2502 \u2502 \u2502 \u2502 \u2502\n# * * * * *\n```  \nFor example, `0 3 * * 1` means this task is scheduled to run weekly on a Monday at 3 AM.  \nThe format also includes extended \"Vixie cron\" step values. As explained in the\n[FreeBSD manual](https://www.freebsd.org/cgi/man.cgi?crontab%285%29):  \n> Step values can be used in conjunction with ranges. Following a range\n> with `/<number>` specifies skips of the number's value through the\n> range. For example, `0-23/2` can be used in the hours field to specify\n> command execution every other hour (the alternative in the V7 standard is\n> `0,2,4,6,8,10,12,14,16,18,20,22`). Steps are also permitted after an\n> asterisk, so if you want to say \"every two hours\", just use `*/2`.  \n{{< note >}}\nA question mark (`?`) in the schedule has the same meaning as an asterisk `*`, that is,\nit stands for any of available value for a given field.\n{{< /note >}}  \nOther than the standard syntax, some macros like `@monthly` can also be used:  \n| Entry | Description| Equivalent to |\n| ------------- | ------------- |-------------  |\n| @yearly (or @annually)| Run once a year at midnight of 1 January| 0 0 1 1 * |\n| @monthly | Run once a month at midnight of the first day of the month| 0 0 1 * * |\n| @weekly | Run once a week at midnight on Sunday morning| 0 0 * * 0 |\n| @daily (or @midnight)| Run once a day at midnight| 0 0 * * * |\n| @hourly | Run once an hour at the beginning of the hour| 0 * * * * |  \nTo generate CronJob schedule expressions, you can also use web tools like [crontab.guru](https://crontab.guru/).\n"
  },
  {
    "question": "How do I set up a StatefulSet with multiple replicas in Kubernetes?",
    "answer": "To set up a StatefulSet with multiple replicas, you need to define the `replicas` field in the StatefulSet spec. For example, set `replicas: 3` to launch 3 replicas of the nginx container.",
    "uuid": "c1b09876-14af-4bea-8766-05785da9c28d",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set up a StatefulSet with multiple replicas in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Components\nRunbook Content: ComponentsThe example below demonstrates the components of a StatefulSet.  \n```yaml\napiVersion: v1\nkind: Service\nmetadata:\nname: nginx\nlabels:\napp: nginx\nspec:\nports:\n- port: 80\nname: web\nclusterIP: None\nselector:\napp: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nname: web\nspec:\nselector:\nmatchLabels:\napp: nginx # has to match .spec.template.metadata.labels\nserviceName: \"nginx\"\nreplicas: 3 # by default is 1\nminReadySeconds: 10 # by default is 0\ntemplate:\nmetadata:\nlabels:\napp: nginx # has to match .spec.selector.matchLabels\nspec:\nterminationGracePeriodSeconds: 10\ncontainers:\n- name: nginx\nimage: registry.k8s.io/nginx-slim:0.24\nports:\n- containerPort: 80\nname: web\nvolumeMounts:\n- name: www\nmountPath: /usr/share/nginx/html\nvolumeClaimTemplates:\n- metadata:\nname: www\nspec:\naccessModes: [ \"ReadWriteOnce\" ]\nstorageClassName: \"my-storage-class\"\nresources:\nrequests:\nstorage: 1Gi\n```  \n{{< note >}}\nThis example uses the `ReadWriteOnce` access mode, for simplicity. For\nproduction use, the Kubernetes project recommends using the `ReadWriteOncePod`\naccess mode instead.\n{{< /note >}}  \nIn the above example:  \n* A Headless Service, named `nginx`, is used to control the network domain.\n* The StatefulSet, named `web`, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.\n* The `volumeClaimTemplates` will provide stable storage using\n[PersistentVolumes](/docs/concepts/storage/persistent-volumes/) provisioned by a\nPersistentVolume Provisioner.  \nThe name of a StatefulSet object must be a valid\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\n"
  },
  {
    "question": "How do I set the number of Pods in a ReplicaSet?",
    "answer": "You can set the number of Pods by specifying `.spec.replicas` in the ReplicaSet manifest.",
    "uuid": "a7cf9eaf-4e3f-4d6b-b903-9a80491a9349",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set the number of Pods in a ReplicaSet?\nThis is about the following runbook:\nRunbook Title: Replicas\nRunbook Content: Writing a ReplicaSet manifestReplicasYou can specify how many Pods should run concurrently by setting `.spec.replicas`. The ReplicaSet will create/delete\nits Pods to match this number.  \nIf you do not specify `.spec.replicas`, then it defaults to 1.\n"
  },
  {
    "question": "What should I do if I need to troubleshoot a bug in a Pod?",
    "answer": "You can run an ephemeral container in the existing Pod to help troubleshoot the hard-to-reproduce bug.",
    "uuid": "05c3808b-c5eb-418a-b293-6b59dc3624d6",
    "question_with_context": "A user asked the following question:\nQuestion: What should I do if I need to troubleshoot a bug in a Pod?\nThis is about the following runbook:\nRunbook Title: Understanding ephemeral containers\nRunbook Content: Understanding ephemeral containers{{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} are the fundamental building\nblock of Kubernetes applications. Since Pods are intended to be disposable and\nreplaceable, you cannot add a container to a Pod once it has been created.\nInstead, you usually delete and replace Pods in a controlled fashion using\n{{< glossary_tooltip text=\"deployments\" term_id=\"deployment\" >}}.  \nSometimes it's necessary to inspect the state of an existing Pod, however, for\nexample to troubleshoot a hard-to-reproduce bug. In these cases you can run\nan ephemeral container in an existing Pod to inspect its state and run\narbitrary commands.\n"
  },
  {
    "question": "What happens to Pods when scaling down a StatefulSet?",
    "answer": "When scaling down, Pods are terminated in reverse order, from {N-1..0}, and all successors must be completely shutdown before a Pod is terminated.",
    "uuid": "9ac25c13-f08a-45d7-9a78-6e7471102045",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to Pods when scaling down a StatefulSet?\nThis is about the following runbook:\nRunbook Title: Deployment and Scaling Guarantees\nRunbook Content: Deployment and Scaling Guarantees* For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.\n* When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.\n* Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.\n* Before a Pod is terminated, all of its successors must be completely shutdown.  \nThe StatefulSet should not specify a `pod.Spec.TerminationGracePeriodSeconds` of 0. This practice\nis unsafe and strongly discouraged. For further explanation, please refer to\n[force deleting StatefulSet Pods](/docs/tasks/run-application/force-delete-stateful-set-pod/).  \nWhen the nginx example above is created, three Pods will be deployed in the order\nweb-0, web-1, web-2. web-1 will not be deployed before web-0 is\n[Running and Ready](/docs/concepts/workloads/pods/pod-lifecycle/), and web-2 will not be deployed until\nweb-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before\nweb-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and\nbecomes Running and Ready.  \nIf a user were to scale the deployed example by patching the StatefulSet such that\n`replicas=1`, web-2 would be terminated first. web-1 would not be terminated until web-2\nis fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and\nis completely shutdown, but prior to web-1's termination, web-1 would not be terminated\nuntil web-0 is Running and Ready.\n"
  },
  {
    "question": "What should I check after creating the Deployment?",
    "answer": "After creating the Deployment, run `kubectl get deployments` to check if the Deployment was created successfully.",
    "uuid": "ee0c5994-ec43-4d92-90a4-465781cca613",
    "question_with_context": "A user asked the following question:\nQuestion: What should I check after creating the Deployment?\nThis is about the following runbook:\nRunbook Title: Creating a Deployment\nRunbook Content: Creating a DeploymentThe following is an example of a Deployment. It creates a ReplicaSet to bring up three `nginx` Pods:  \n{{% code_sample file=\"controllers/nginx-deployment.yaml\" %}}  \nIn this example:  \n* A Deployment named `nginx-deployment` is created, indicated by the\n`.metadata.name` field. This name will become the basis for the ReplicaSets\nand Pods which are created later. See [Writing a Deployment Spec](#writing-a-deployment-spec)\nfor more details.\n* The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the `.spec.replicas` field.\n* The `.spec.selector` field defines how the created ReplicaSet finds which Pods to manage.\nIn this case, you select a label that is defined in the Pod template (`app: nginx`).\nHowever, more sophisticated selection rules are possible,\nas long as the Pod template itself satisfies the rule.  \n{{< note >}}\nThe `.spec.selector.matchLabels` field is a map of {key,value} pairs.\nA single {key,value} in the `matchLabels` map is equivalent to an element of `matchExpressions`,\nwhose `key` field is \"key\", the `operator` is \"In\", and the `values` array contains only \"value\".\nAll of the requirements, from both `matchLabels` and `matchExpressions`, must be satisfied in order to match.\n{{< /note >}}  \n* The `template` field contains the following sub-fields:\n* The Pods are labeled `app: nginx`using the `.metadata.labels` field.\n* The Pod template's specification, or `.template.spec` field, indicates that\nthe Pods run one container, `nginx`, which runs the `nginx`\n[Docker Hub](https://hub.docker.com/) image at version 1.14.2.\n* Create one container and name it `nginx` using the `.spec.template.spec.containers[0].name` field.  \nBefore you begin, make sure your Kubernetes cluster is up and running.\nFollow the steps given below to create the above Deployment:  \n1. Create the Deployment by running the following command:  \n```shell\nkubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml\n```  \n2. Run `kubectl get deployments` to check if the Deployment was created.  \nIf the Deployment is still being created, the output is similar to the following:\n```\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   0/3     0            0           1s\n```\nWhen you inspect the Deployments in your cluster, the following fields are displayed:\n* `NAME` lists the names of the Deployments in the namespace.\n* `READY` displays how many replicas of the application are available to your users. It follows the pattern ready/desired.\n* `UP-TO-DATE` displays the number of replicas that have been updated to achieve the desired state.\n* `AVAILABLE` displays how many replicas of the application are available to your users.\n* `AGE` displays the amount of time that the application has been running.  \nNotice how the number of desired replicas is 3 according to `.spec.replicas` field.  \n3. To see the Deployment rollout status, run `kubectl rollout status deployment/nginx-deployment`.  \nThe output is similar to:\n```\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\ndeployment \"nginx-deployment\" successfully rolled out\n```  \n4. Run the `kubectl get deployments` again a few seconds later.\nThe output is similar to this:\n```\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           18s\n```\nNotice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.  \n5. To see the ReplicaSet (`rs`) created by the Deployment, run `kubectl get rs`. The output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY   AGE\nnginx-deployment-75675f5897   3         3         3       18s\n```\nReplicaSet output shows the following fields:  \n* `NAME` lists the names of the ReplicaSets in the namespace.\n* `DESIRED` displays the desired number of _replicas_ of the application, which you define when you create the Deployment. This is the _desired state_.\n* `CURRENT` displays how many replicas are currently running.\n* `READY` displays how many replicas of the application are available to your users.\n* `AGE` displays the amount of time that the application has been running.  \nNotice that the name of the ReplicaSet is always formatted as\n`[DEPLOYMENT-NAME]-[HASH]`. This name will become the basis for the Pods\nwhich are created.  \nThe `HASH` string is the same as the `pod-template-hash` label on the ReplicaSet.  \n6. To see the labels automatically generated for each Pod, run `kubectl get pods --show-labels`.\nThe output is similar to:\n```\nNAME                                READY     STATUS    RESTARTS   AGE       LABELS\nnginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\nnginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\nnginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=75675f5897\n```\nThe created ReplicaSet ensures that there are three `nginx` Pods.  \n{{< note >}}\nYou must specify an appropriate selector and Pod template labels in a Deployment\n(in this case, `app: nginx`).  \nDo not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.\n{{< /note >}}\n"
  },
  {
    "question": "How can I prevent my newly created Pods from being terminated by a ReplicaSet?",
    "answer": "To prevent newly created Pods from being terminated by a ReplicaSet, ensure that the bare Pods do not have labels that match the selector of the ReplicaSet.",
    "uuid": "8e114e96-35c1-4692-9fde-ad59dc692936",
    "question_with_context": "A user asked the following question:\nQuestion: How can I prevent my newly created Pods from being terminated by a ReplicaSet?\nThis is about the following runbook:\nRunbook Title: Non-Template Pod acquisitions\nRunbook Content: Non-Template Pod acquisitionsWhile you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have\nlabels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited\nto owning Pods specified by its template-- it can acquire other Pods in the manner specified in the previous sections.  \nTake the previous frontend ReplicaSet example, and the Pods specified in the following manifest:  \n{{% code_sample file=\"pods/pod-rs.yaml\" %}}  \nAs those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend\nReplicaSet, they will immediately be acquired by it.  \nSuppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to\nfulfill its replica count requirement:  \n```shell\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\n```  \nThe new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over\nits desired count.  \nFetching the Pods:  \n```shell\nkubectl get pods\n```  \nThe output shows that the new Pods are either already terminated, or in the process of being terminated:  \n```\nNAME             READY   STATUS        RESTARTS   AGE\nfrontend-b2zdv   1/1     Running       0          10m\nfrontend-vcmts   1/1     Running       0          10m\nfrontend-wtsmm   1/1     Running       0          10m\npod1             0/1     Terminating   0          1s\npod2             0/1     Terminating   0          1s\n```  \nIf you create the Pods first:  \n```shell\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\n```  \nAnd then create the ReplicaSet however:  \n```shell\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\n```  \nYou shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the\nnumber of its new Pods and the original matches its desired count. As fetching the Pods:  \n```shell\nkubectl get pods\n```  \nWill reveal in its output:\n```\nNAME             READY   STATUS    RESTARTS   AGE\nfrontend-hmmj2   1/1     Running   0          9s\npod1             1/1     Running   0          36s\npod2             1/1     Running   0          36s\n```  \nIn this manner, a ReplicaSet can own a non-homogeneous set of Pods\n"
  },
  {
    "question": "Can I add metadata to the Jobs created by the CronJob template?",
    "answer": "Yes, you can specify common metadata for the templated Jobs, such as labels or annotations.",
    "uuid": "32104032-2881-477e-9ae8-0ce7e581115a",
    "question_with_context": "A user asked the following question:\nQuestion: Can I add metadata to the Jobs created by the CronJob template?\nThis is about the following runbook:\nRunbook Title: Job template\nRunbook Content: Writing a CronJob specJob templateThe `.spec.jobTemplate` defines a template for the Jobs that the CronJob creates, and it is required.\nIt has exactly the same schema as a [Job](/docs/concepts/workloads/controllers/job/), except that\nit is nested and does not have an `apiVersion` or `kind`.\nYou can specify common metadata for the templated Jobs, such as\n{{< glossary_tooltip text=\"labels\" term_id=\"label\" >}} or\n{{< glossary_tooltip text=\"annotations\" term_id=\"annotation\" >}}.\nFor information about writing a Job `.spec`, see [Writing a Job Spec](/docs/concepts/workloads/controllers/job/#writing-a-job-spec).\n"
  },
  {
    "question": "When should I consider using a Job in Kubernetes?",
    "answer": "Consider using a Job for pods that are expected to terminate on their own, such as batch jobs.",
    "uuid": "f2635e85-8b38-4061-9b25-dafc38e876d4",
    "question_with_context": "A user asked the following question:\nQuestion: When should I consider using a Job in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Job\nRunbook Content: Alternatives to ReplicationControllerJobUse a [`Job`](/docs/concepts/workloads/controllers/job/) instead of a ReplicationController for pods that are expected to terminate on their own\n(that is, batch jobs).\n"
  },
  {
    "question": "How do I ensure memory availability for my Kubernetes pods using Memory QoS?",
    "answer": "You can ensure memory availability for your Kubernetes pods by setting the `memory.min` to the memory requests of your containers. This reserves memory resources that are never reclaimed by the kernel.",
    "uuid": "1c557ec4-1ee2-4d85-b0ed-2c405ce09422",
    "question_with_context": "A user asked the following question:\nQuestion: How do I ensure memory availability for my Kubernetes pods using Memory QoS?\nThis is about the following runbook:\nRunbook Title: Memory QoS with cgroup v2\nRunbook Content: Memory QoS with cgroup v2{{< feature-state feature_gate_name=\"MemoryQoS\" >}}  \nMemory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes.\nMemory requests and limits of containers in pod are used to set specific interfaces `memory.min`\nand `memory.high` provided by the memory controller. When `memory.min` is set to memory requests,\nmemory resources are reserved and never reclaimed by the kernel; this is how Memory QoS ensures\nmemory availability for Kubernetes pods. And if memory limits are set in the container,\nthis means that the system needs to limit container memory usage; Memory QoS uses `memory.high`\nto throttle workload approaching its memory limit, ensuring that the system is not overwhelmed\nby instantaneous memory allocation.  \nMemory QoS relies on QoS class to determine which settings to apply; however, these are different\nmechanisms that both provide controls over quality of service.\n"
  },
  {
    "question": "How do I know if my deployment is progressing?",
    "answer": "You can determine if your deployment is progressing while it is rolling out a new ReplicaSet.",
    "uuid": "8a84aa43-65af-4e4e-84fb-c7dc462327a9",
    "question_with_context": "A user asked the following question:\nQuestion: How do I know if my deployment is progressing?\nThis is about the following runbook:\nRunbook Title: Deployment status\nRunbook Content: Deployment statusA Deployment enters various states during its lifecycle. It can be [progressing](#progressing-deployment) while\nrolling out a new ReplicaSet, it can be [complete](#complete-deployment), or it can [fail to progress](#failed-deployment).\n"
  },
  {
    "question": "How can I see the details of my updated Deployment after a rollout?",
    "answer": "After a successful rollout, you can view the details of your updated Deployment by running `kubectl describe deployments`. This will provide you with information such as the number of replicas, the current image being used, and the status of the Pods.",
    "uuid": "b4a16ae1-aef1-489a-ac8d-f28e9c5cea4f",
    "question_with_context": "A user asked the following question:\nQuestion: How can I see the details of my updated Deployment after a rollout?\nThis is about the following runbook:\nRunbook Title: Updating a Deployment\nRunbook Content: Updating a Deployment{{< note >}}\nA Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, `.spec.template`)\nis changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.\n{{< /note >}}  \nFollow the steps given below to update your Deployment:  \n1. Let's update the nginx Pods to use the `nginx:1.16.1` image instead of the `nginx:1.14.2` image.  \n```shell\nkubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1\n```  \nor use the following command:  \n```shell\nkubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n```\nwhere `deployment/nginx-deployment` indicates the Deployment,\n`nginx` indicates the Container the update will take place and\n`nginx:1.16.1` indicates the new image and its tag.  \nThe output is similar to:  \n```\ndeployment.apps/nginx-deployment image updated\n```  \nAlternatively, you can `edit` the Deployment and change `.spec.template.spec.containers[0].image` from `nginx:1.14.2` to `nginx:1.16.1`:  \n```shell\nkubectl edit deployment/nginx-deployment\n```  \nThe output is similar to:  \n```\ndeployment.apps/nginx-deployment edited\n```  \n2. To see the rollout status, run:  \n```shell\nkubectl rollout status deployment/nginx-deployment\n```  \nThe output is similar to this:  \n```\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\n```  \nor  \n```\ndeployment \"nginx-deployment\" successfully rolled out\n```  \nGet more details on your updated Deployment:  \n* After the rollout succeeds, you can view the Deployment by running `kubectl get deployments`.\nThe output is similar to this:  \n```ini\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           36s\n```  \n* Run `kubectl get rs` to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it\nup to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.  \n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME                          DESIRED   CURRENT   READY   AGE\nnginx-deployment-1564180365   3         3         3       6s\nnginx-deployment-2035384211   0         0         0       36s\n```  \n* Running `get pods` should now show only the new Pods:  \n```shell\nkubectl get pods\n```  \nThe output is similar to this:\n```\nNAME                                READY     STATUS    RESTARTS   AGE\nnginx-deployment-1564180365-khku8   1/1       Running   0          14s\nnginx-deployment-1564180365-nacti   1/1       Running   0          14s\nnginx-deployment-1564180365-z9gth   1/1       Running   0          14s\n```  \nNext time you want to update these Pods, you only need to update the Deployment's Pod template again.  \nDeployment ensures that only a certain number of Pods are down while they are being updated. By default,\nit ensures that at least 75% of the desired number of Pods are up (25% max unavailable).  \nDeployment also ensures that only a certain number of Pods are created above the desired number of Pods.\nBy default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).  \nFor example, if you look at the above Deployment closely, you will see that it first creates a new Pod,\nthen deletes an old Pod, and creates another new one. It does not kill old Pods until a sufficient number of\nnew Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.\nIt makes sure that at least 3 Pods are available and that at max 4 Pods in total are available. In case of\na Deployment with 4 replicas, the number of Pods would be between 3 and 5.  \n* Get details of your Deployment:\n```shell\nkubectl describe deployments\n```\nThe output is similar to this:\n```\nName:                   nginx-deployment\nNamespace:              default\nCreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000\nLabels:                 app=nginx\nAnnotations:            deployment.kubernetes.io/revision=2\nSelector:               app=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx\nContainers:\nnginx:\nImage:        nginx:1.16.1\nPort:         80/TCP\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    NewReplicaSetAvailable\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)\nEvents:\nType    Reason             Age   From                   Message\n----    ------             ----  ----                   -------\nNormal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3\nNormal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1\nNormal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2\nNormal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2\nNormal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1\nNormal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3\nNormal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0\n```\nHere you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)\nand scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet\n(nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet\nto 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times.\nIt then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy.\nFinally, you'll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.  \n{{< note >}}\nKubernetes doesn't count terminating Pods when calculating the number of `availableReplicas`, which must be between\n`replicas - maxUnavailable` and `replicas + maxSurge`. As a result, you might notice that there are more Pods than\nexpected during a rollout, and that the total resources consumed by the Deployment is more than `replicas + maxSurge`\nuntil the `terminationGracePeriodSeconds` of the terminating Pods expires.\n{{< /note >}}\n"
  },
  {
    "question": "What happens with init containers before the main application container starts?",
    "answer": "Init containers run and complete their tasks before the main application container starts.",
    "uuid": "54fadf58-20c3-4458-a642-1125c157ce38",
    "question_with_context": "A user asked the following question:\nQuestion: What happens with init containers before the main application container starts?\nThis is about the following runbook:\nRunbook Title: Differences from sidecar containers\nRunbook Content: Understanding init containersDifferences from sidecar containersInit containers run and complete their tasks before the main application container starts.\nUnlike [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers),\ninit containers are not continuously running alongside the main containers.  \nInit containers run to completion sequentially, and the main container does not start\nuntil all the init containers have successfully completed.  \ninit containers do not support `lifecycle`, `livenessProbe`, `readinessProbe`, or\n`startupProbe` whereas sidecar containers support all these [probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe) to control their lifecycle.  \nInit containers share the same resources (CPU, memory, network) with the main application\ncontainers but do not interact directly with them. They can, however, use shared volumes\nfor data exchange.\n"
  },
  {
    "question": "What happens if I try to change the pod-template-hash label?",
    "answer": "Do not change this label.",
    "uuid": "dcfab6c2-ad36-498e-9cb0-3172bb40bb58",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I try to change the pod-template-hash label?\nThis is about the following runbook:\nRunbook Title: Pod-template-hash label\nRunbook Content: Creating a DeploymentPod-template-hash label{{< caution >}}\nDo not change this label.\n{{< /caution >}}  \nThe `pod-template-hash` label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.  \nThis label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the `PodTemplate` of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,\nand in any existing Pods that the ReplicaSet might have.\n"
  },
  {
    "question": "How can I initiate a voluntary disruption in my application?",
    "answer": "You can initiate a voluntary disruption by deleting the deployment or controller managing the pod, updating a deployment's pod template which causes a restart, or directly deleting a pod.",
    "uuid": "bb0d0eb6-2a22-433f-9f88-1e135bbb3f35",
    "question_with_context": "A user asked the following question:\nQuestion: How can I initiate a voluntary disruption in my application?\nThis is about the following runbook:\nRunbook Title: Voluntary and involuntary disruptions\nRunbook Content: Voluntary and involuntary disruptionsPods do not disappear until someone (a person or a controller) destroys them, or\nthere is an unavoidable hardware or system software error.  \nWe call these unavoidable cases *involuntary disruptions* to\nan application.  Examples are:  \n- a hardware failure of the physical machine backing the node\n- cluster administrator deletes VM (instance) by mistake\n- cloud provider or hypervisor failure makes VM disappear\n- a kernel panic\n- the node disappears from the cluster due to cluster network partition\n- eviction of a pod due to the node being [out-of-resources](/docs/concepts/scheduling-eviction/node-pressure-eviction/).  \nExcept for the out-of-resources condition, all these conditions\nshould be familiar to most users; they are not specific\nto Kubernetes.  \nWe call other cases *voluntary disruptions*.  These include both\nactions initiated by the application owner and those initiated by a Cluster\nAdministrator.  Typical application owner actions include:  \n- deleting the deployment or other controller that manages the pod\n- updating a deployment's pod template causing a restart\n- directly deleting a pod (e.g. by accident)  \nCluster administrator actions include:  \n- [Draining a node](/docs/tasks/administer-cluster/safely-drain-node/) for repair or upgrade.\n- Draining a node from a cluster to scale the cluster down (learn about\n[Cluster Autoscaling](https://github.com/kubernetes/autoscaler/#readme)).\n- Removing a pod from a node to permit something else to fit on that node.  \nThese actions might be taken directly by the cluster administrator, or by automation\nrun by the cluster administrator, or by your cluster hosting provider.  \nAsk your cluster administrator or consult your cloud provider or distribution documentation\nto determine if any sources of voluntary disruptions are enabled for your cluster.\nIf none are enabled, you can skip creating Pod Disruption Budgets.  \n{{< caution >}}\nNot all voluntary disruptions are constrained by Pod Disruption Budgets. For example,\ndeleting deployments or pods bypasses Pod Disruption Budgets.\n{{< /caution >}}\n"
  },
  {
    "question": "What should I use if I want rolling updates for my application?",
    "answer": "You should use Deployment, as it provides rolling update functionality.",
    "uuid": "e26c811e-7890-41ce-a5d1-b3cb50c3171d",
    "question_with_context": "A user asked the following question:\nQuestion: What should I use if I want rolling updates for my application?\nThis is about the following runbook:\nRunbook Title: Deployment (Recommended)\nRunbook Content: Alternatives to ReplicationControllerDeployment (Recommended)[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is a higher-level API object that updates its underlying Replica Sets and their Pods. Deployments are recommended if you want the rolling update functionality,  because they are declarative, server-side, and have additional features.\n"
  },
  {
    "question": "How can I adopt sidecar containers in my setup?",
    "answer": "You can learn how to adopt sidecar containers by following the tutorial on [Adopt Sidecar Containers](/docs/tutorials/configuration/pod-sidecar-containers/).",
    "uuid": "97187912-7f92-4a08-8ac9-ee4f6f8614f1",
    "question_with_context": "A user asked the following question:\nQuestion: How can I adopt sidecar containers in my setup?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn how to [Adopt Sidecar Containers](/docs/tutorials/configuration/pod-sidecar-containers/)\n* Read a blog post on [native sidecar containers](/blog/2023/08/25/native-sidecar-containers/).\n* Read about [creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).\n* Learn about the [types of probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe): liveness, readiness, startup probe.\n* Learn about [pod overhead](/docs/concepts/scheduling-eviction/pod-overhead/).\n"
  },
  {
    "question": "What does a Deployment with a sidecar container look like in YAML?",
    "answer": "A Deployment with a sidecar container is represented in YAML format, which includes the definition of both the main application container and the sidecar container.",
    "uuid": "1c8053ad-eb0a-4c2e-b800-cd13e4d87aeb",
    "question_with_context": "A user asked the following question:\nQuestion: What does a Deployment with a sidecar container look like in YAML?\nThis is about the following runbook:\nRunbook Title: Example application {#sidecar-example}\nRunbook Content: Sidecar containers in Kubernetes {#pod-sidecar-containers}Example application {#sidecar-example}Here's an example of a Deployment with two containers, one of which is a sidecar:  \n{{% code_sample language=\"yaml\" file=\"application/deployment-sidecar.yaml\" %}}\n"
  },
  {
    "question": "What's the advantage of using a DaemonSet over creating individual Pods?",
    "answer": "A DaemonSet replaces Pods that are deleted or terminated, ensuring they are always running, which is not the case with individual Pods.",
    "uuid": "09737193-e72c-4bb7-b2ba-0df8fb63e8f1",
    "question_with_context": "A user asked the following question:\nQuestion: What's the advantage of using a DaemonSet over creating individual Pods?\nThis is about the following runbook:\nRunbook Title: Bare Pods\nRunbook Content: Alternatives to DaemonSetBare PodsIt is possible to create Pods directly which specify a particular node to run on.  However,\na DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of\nnode failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should\nuse a DaemonSet rather than creating individual Pods.\n"
  },
  {
    "question": "What does a DaemonSet YAML file look like?",
    "answer": "A DaemonSet YAML file describes the DaemonSet configuration, such as the Docker image to run, for example, the `daemonset.yaml` file runs the fluentd-elasticsearch Docker image.",
    "uuid": "12883777-a181-4e6f-8df0-c02fc7d6b296",
    "question_with_context": "A user asked the following question:\nQuestion: What does a DaemonSet YAML file look like?\nThis is about the following runbook:\nRunbook Title: Create a DaemonSet\nRunbook Content: Writing a DaemonSet SpecCreate a DaemonSetYou can describe a DaemonSet in a YAML file. For example, the `daemonset.yaml` file below\ndescribes a DaemonSet that runs the fluentd-elasticsearch Docker image:  \n{{% code_sample file=\"controllers/daemonset.yaml\" %}}  \nCreate a DaemonSet based on the YAML file:  \n```\nkubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml\n```\n"
  },
  {
    "question": "When should I use a DaemonSet instead of a ReplicaSet?",
    "answer": "Use a DaemonSet instead of a ReplicaSet for Pods that provide a machine-level function, such as machine monitoring or machine logging.",
    "uuid": "b8c7d5a8-b648-448e-9077-d2f5c2cd0a42",
    "question_with_context": "A user asked the following question:\nQuestion: When should I use a DaemonSet instead of a ReplicaSet?\nThis is about the following runbook:\nRunbook Title: DaemonSet\nRunbook Content: Alternatives to ReplicaSetDaemonSetUse a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicaSet for Pods that provide a\nmachine-level function, such as machine monitoring or machine logging. These Pods have a lifetime that is tied\nto a machine lifetime: the Pod needs to be running on the machine before other Pods start, and are\nsafe to terminate when the machine is otherwise ready to be rebooted/shutdown.\n"
  },
  {
    "question": "What happens if a pod uses a UID/GID outside the valid range?",
    "answer": "If a pod uses a UID/GID outside the valid range of 0-65535, those files will be seen as belonging to the overflow ID, usually 65534, and it is not possible to modify those files even by running as the 65534 user/group.",
    "uuid": "7598d4f3-b249-4018-b9c3-ee61832205b4",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if a pod uses a UID/GID outside the valid range?\nThis is about the following runbook:\nRunbook Title: Introduction\nRunbook Content: IntroductionUser namespaces is a Linux feature that allows to map users in the container to\ndifferent users in the host. Furthermore, the capabilities granted to a pod in\na user namespace are valid only in the namespace and void outside of it.  \nA pod can opt-in to use user namespaces by setting the `pod.spec.hostUsers` field\nto `false`.  \nThe kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way\nto guarantee that no two pods on the same node use the same mapping.  \nThe `runAsUser`, `runAsGroup`, `fsGroup`, etc. fields in the `pod.spec` always\nrefer to the user inside the container.  \nThe valid UIDs/GIDs when this feature is enabled is the range 0-65535. This\napplies to files and processes (`runAsUser`, `runAsGroup`, etc.).  \nFiles using a UID/GID outside this range will be seen as belonging to the\noverflow ID, usually 65534 (configured in `/proc/sys/kernel/overflowuid` and\n`/proc/sys/kernel/overflowgid`). However, it is not possible to modify those\nfiles, even by running as the 65534 user/group.  \nMost applications that need to run as root but don't access other host\nnamespaces or resources, should continue to run fine without any changes needed\nif user namespaces is activated.\n"
  },
  {
    "question": "What steps should I follow to deploy a stateful application?",
    "answer": "You can follow an example of deploying a stateful application by checking out the tutorial on [deploying a stateful application](/docs/tutorials/stateful-application/basic-stateful-set/).",
    "uuid": "7cc8b8c0-c5e0-4035-a800-fb017cb6f40c",
    "question_with_context": "A user asked the following question:\nQuestion: What steps should I follow to deploy a stateful application?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Find out how to use StatefulSets\n* Follow an example of [deploying a stateful application](/docs/tutorials/stateful-application/basic-stateful-set/).\n* Follow an example of [deploying Cassandra with Stateful Sets](/docs/tutorials/stateful-application/cassandra/).\n* Follow an example of [running a replicated stateful application](/docs/tasks/run-application/run-replicated-stateful-application/).\n* Learn how to [scale a StatefulSet](/docs/tasks/run-application/scale-stateful-set/).\n* Learn what's involved when you [delete a StatefulSet](/docs/tasks/run-application/delete-stateful-set/).\n* Learn how to [configure a Pod to use a volume for storage](/docs/tasks/configure-pod-container/configure-volume-storage/).\n* Learn how to [configure a Pod to use a PersistentVolume for storage](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).\n* `StatefulSet` is a top-level resource in the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/stateful-set-v1\" >}}\nobject definition to understand the API for stateful sets.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how\nyou can use it to manage application availability during disruptions.\n"
  },
  {
    "question": "How do I manage ReplicaSets when using Deployments?",
    "answer": "When you use Deployments, you don't have to worry about managing the ReplicaSets that they create, as Deployments own and manage their ReplicaSets.",
    "uuid": "ca016763-c204-4ab9-af89-09d3611e05e7",
    "question_with_context": "A user asked the following question:\nQuestion: How do I manage ReplicaSets when using Deployments?\nThis is about the following runbook:\nRunbook Title: Deployment (recommended)\nRunbook Content: Alternatives to ReplicaSetDeployment (recommended)[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is an object which can own ReplicaSets and update\nthem and their Pods via declarative, server-side rolling updates.\nWhile ReplicaSets can be used independently, today they're mainly used by Deployments as a mechanism to orchestrate Pod\ncreation, deletion and updates. When you use Deployments you don't have to worry about managing the ReplicaSets that\nthey create. Deployments own and manage their ReplicaSets.\nAs such, it is recommended to use Deployments when you want ReplicaSets.\n"
  },
  {
    "question": "Where can I find the schema for the Pod Template in a DaemonSet?",
    "answer": "The schema for the Pod Template in a DaemonSet can be found in the `.spec.template` field, which is one of the required fields in `.spec`.",
    "uuid": "bab8b054-cd64-4729-adc1-9ce936434d83",
    "question_with_context": "A user asked the following question:\nQuestion: Where can I find the schema for the Pod Template in a DaemonSet?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a DaemonSet SpecPod TemplateThe `.spec.template` is one of the required fields in `.spec`.  \nThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates).\nIt has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}},\nexcept it is nested and does not have an `apiVersion` or `kind`.  \nIn addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate\nlabels (see [pod selector](#pod-selector)).  \nA Pod Template in a DaemonSet must have a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)\nequal to `Always`, or be unspecified, which defaults to `Always`.\n"
  },
  {
    "question": "Is there anything else I need to add to the ReplicaSet manifest besides the basic fields?",
    "answer": "Yes, a ReplicaSet also needs a `.spec` section.",
    "uuid": "32b4ad04-06de-4aa3-aaf8-7abe327878bf",
    "question_with_context": "A user asked the following question:\nQuestion: Is there anything else I need to add to the ReplicaSet manifest besides the basic fields?\nThis is about the following runbook:\nRunbook Title: Writing a ReplicaSet manifest\nRunbook Content: Writing a ReplicaSet manifestAs with all other Kubernetes API objects, a ReplicaSet needs the `apiVersion`, `kind`, and `metadata` fields.\nFor ReplicaSets, the `kind` is always a ReplicaSet.  \nWhen the control plane creates new Pods for a ReplicaSet, the `.metadata.name` of the\nReplicaSet is part of the basis for naming those Pods. The name of a ReplicaSet must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).  \nA ReplicaSet also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\n"
  },
  {
    "question": "Can I monitor the results of different release tracks separately?",
    "answer": "Yes, you can monitor the results of different release tracks separately by managing the ReplicationControllers independently, allowing you to test and observe the performance of each track.",
    "uuid": "9d164dd7-2202-43ad-8c69-d4bfabe6d624",
    "question_with_context": "A user asked the following question:\nQuestion: Can I monitor the results of different release tracks separately?\nThis is about the following runbook:\nRunbook Title: Multiple release tracks\nRunbook Content: Common usage patternsMultiple release tracksIn addition to running multiple releases of an application while a rolling update is in progress, it's common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.  \nFor instance, a service might target all pods with `tier in (frontend), environment in (prod)`.  Now say you have 10 replicated pods that make up this tier.  But you want to be able to 'canary' a new version of this component.  You could set up a ReplicationController with `replicas` set to 9 for the bulk of the replicas, with labels `tier=frontend, environment=prod, track=stable`, and another ReplicationController with `replicas` set to 1 for the canary, with labels `tier=frontend, environment=prod, track=canary`.  Now the service is covering both the canary and non-canary pods.  But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.\n"
  },
  {
    "question": "How can I learn about using Pods in Kubernetes?",
    "answer": "You can learn about Pods by visiting the [Pods documentation](/docs/concepts/workloads/pods).",
    "uuid": "7cc8b8c0-c5e0-4035-a800-fb017cb6f40c",
    "question_with_context": "A user asked the following question:\nQuestion: How can I learn about using Pods in Kubernetes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Find out how to use StatefulSets\n* Follow an example of [deploying a stateful application](/docs/tutorials/stateful-application/basic-stateful-set/).\n* Follow an example of [deploying Cassandra with Stateful Sets](/docs/tutorials/stateful-application/cassandra/).\n* Follow an example of [running a replicated stateful application](/docs/tasks/run-application/run-replicated-stateful-application/).\n* Learn how to [scale a StatefulSet](/docs/tasks/run-application/scale-stateful-set/).\n* Learn what's involved when you [delete a StatefulSet](/docs/tasks/run-application/delete-stateful-set/).\n* Learn how to [configure a Pod to use a volume for storage](/docs/tasks/configure-pod-container/configure-volume-storage/).\n* Learn how to [configure a Pod to use a PersistentVolume for storage](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).\n* `StatefulSet` is a top-level resource in the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/stateful-set-v1\" >}}\nobject definition to understand the API for stateful sets.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how\nyou can use it to manage application availability during disruptions.\n"
  },
  {
    "question": "Why does StatefulSet not automatically revert to a working configuration after a failed rollout?",
    "answer": "Due to a known issue, StatefulSet will continue to wait for the broken Pod to become Ready, which never happens, before it will attempt to revert to the working configuration.",
    "uuid": "10973aa5-42f5-4429-b4da-5de9cc6ae0c6",
    "question_with_context": "A user asked the following question:\nQuestion: Why does StatefulSet not automatically revert to a working configuration after a failed rollout?\nThis is about the following runbook:\nRunbook Title: Forced rollback\nRunbook Content: Rolling UpdatesForced rollbackWhen using [Rolling Updates](#rolling-updates) with the default\n[Pod Management Policy](#pod-management-policies) (`OrderedReady`),\nit's possible to get into a broken state that requires manual intervention to repair.  \nIf you update the Pod template to a configuration that never becomes Running and\nReady (for example, due to a bad binary or application-level configuration error),\nStatefulSet will stop the rollout and wait.  \nIn this state, it's not enough to revert the Pod template to a good configuration.\nDue to a [known issue](https://github.com/kubernetes/kubernetes/issues/67250),\nStatefulSet will continue to wait for the broken Pod to become Ready\n(which never happens) before it will attempt to revert it back to the working\nconfiguration.  \nAfter reverting the template, you must also delete any Pods that StatefulSet had\nalready attempted to run with the bad configuration.\nStatefulSet will then begin to recreate the Pods using the reverted template.\n"
  },
  {
    "question": "Where can I find more details about the Replication Controller API object?",
    "answer": "More details about the Replication Controller API object can be found at the provided documentation link.",
    "uuid": "8b3b13a3-93f7-494b-9af4-9c468e018a0a",
    "question_with_context": "A user asked the following question:\nQuestion: Where can I find more details about the Replication Controller API object?\nThis is about the following runbook:\nRunbook Title: API Object\nRunbook Content: API ObjectReplication controller is a top-level resource in the Kubernetes REST API. More details about the\nAPI object can be found at:\n[ReplicationController API object](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#replicationcontroller-v1-core).\n"
  },
  {
    "question": "Can you tell me the format for job labels in Kubernetes?",
    "answer": "The format for job labels in Kubernetes includes the `batch.kubernetes.io/` prefix followed by `job-name` and `controller-uid`.",
    "uuid": "13c6c55a-b9e2-4882-b8ba-820637b8834f",
    "question_with_context": "A user asked the following question:\nQuestion: Can you tell me the format for job labels in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Job Labels\nRunbook Content: Writing a Job specJob LabelsJob labels will have `batch.kubernetes.io/` prefix for `job-name` and `controller-uid`.\n"
  },
  {
    "question": "How do I create a DaemonSet using a YAML file?",
    "answer": "You can create a DaemonSet by describing it in a YAML file and then applying it using the command: `kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml`.",
    "uuid": "12883777-a181-4e6f-8df0-c02fc7d6b296",
    "question_with_context": "A user asked the following question:\nQuestion: How do I create a DaemonSet using a YAML file?\nThis is about the following runbook:\nRunbook Title: Create a DaemonSet\nRunbook Content: Writing a DaemonSet SpecCreate a DaemonSetYou can describe a DaemonSet in a YAML file. For example, the `daemonset.yaml` file below\ndescribes a DaemonSet that runs the fluentd-elasticsearch Docker image:  \n{{% code_sample file=\"controllers/daemonset.yaml\" %}}  \nCreate a DaemonSet based on the YAML file:  \n```\nkubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml\n```\n"
  },
  {
    "question": "What happens when a container in a Pod crashes for the first time?",
    "answer": "Kubernetes attempts an immediate restart based on the Pod `restartPolicy`.",
    "uuid": "1c1fb0a0-20b3-42b2-a6ae-6e80cddd5a2b",
    "question_with_context": "A user asked the following question:\nQuestion: What happens when a container in a Pod crashes for the first time?\nThis is about the following runbook:\nRunbook Title: How Pods handle problems with containers {#container-restarts}\nRunbook Content: How Pods handle problems with containers {#container-restarts}Kubernetes manages container failures within Pods using a [`restartPolicy`](#restart-policy) defined in the Pod `spec`. This policy determines how Kubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:  \n1. **Initial crash**: Kubernetes attempts an immediate restart based on the Pod `restartPolicy`.\n1. **Repeated crashes**: After the initial crash Kubernetes applies an exponential\nbackoff delay for subsequent restarts, described in [`restartPolicy`](#restart-policy).\nThis prevents rapid, repeated restart attempts from overloading the system.\n1. **CrashLoopBackOff state**: This indicates that the backoff delay mechanism is currently\nin effect for a given container that is in a crash loop, failing and restarting repeatedly.\n1. **Backoff reset**: If a container runs successfully for a certain duration\n(e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash\nas the first one.  \nIn practice, a `CrashLoopBackOff` is a condition or event that might be seen as output\nfrom the `kubectl` command, while describing or listing Pods, when a container in the Pod\nfails to start properly and then continually tries and fails in a loop.  \nIn other words, when a container enters the crash loop, Kubernetes applies the\nexponential backoff delay mentioned in the [Container restart policy](#restart-policy).\nThis mechanism prevents a faulty container from overwhelming the system with continuous\nfailed start attempts.  \nThe `CrashLoopBackOff` can be caused by issues like the following:  \n* Application errors that cause the container to exit.\n* Configuration errors, such as incorrect environment variables or missing\nconfiguration files.\n* Resource constraints, where the container might not have enough memory or CPU\nto start properly.\n* Health checks failing if the application doesn't start serving within the\nexpected time.\n* Container liveness probes or startup probes returning a `Failure` result\nas mentioned in the [probes section](#container-probes).  \nTo investigate the root cause of a `CrashLoopBackOff` issue, a user can:  \n1. **Check logs**: Use `kubectl logs <name-of-pod>` to check the logs of the container.\nThis is often the most direct way to diagnose the issue causing the crashes.\n1. **Inspect events**: Use `kubectl describe pod <name-of-pod>` to see events\nfor the Pod, which can provide hints about configuration or resource issues.\n1. **Review configuration**: Ensure that the Pod configuration, including\nenvironment variables and mounted volumes, is correct and that all required\nexternal resources are available.\n1. **Check resource limits**: Make sure that the container has enough CPU\nand memory allocated. Sometimes, increasing the resources in the Pod definition\ncan resolve the issue.\n1. **Debug application**: There might exist bugs or misconfigurations in the\napplication code. Running this container image locally or in a development\nenvironment can help diagnose application specific issues.\n"
  },
  {
    "question": "Can init containers access secrets that app containers can't?",
    "answer": "Yes, init containers can run with a different view of the filesystem than app containers in the same Pod, allowing them access to secrets that app containers cannot access.",
    "uuid": "b91fb861-090f-47da-af13-3ee58314590a",
    "question_with_context": "A user asked the following question:\nQuestion: Can init containers access secrets that app containers can't?\nThis is about the following runbook:\nRunbook Title: Using init containers\nRunbook Content: Using init containersBecause init containers have separate images from app containers, they\nhave some advantages for start-up related code:  \n* Init containers can contain utilities or custom code for setup that are not present in an app\nimage. For example, there is no need to make an image `FROM` another image just to use a tool like\n`sed`, `awk`, `python`, or `dig` during setup.\n* The application image builder and deployer roles can work independently without\nthe need to jointly build a single app image.\n* Init containers can run with a different view of the filesystem than app containers in the\nsame Pod. Consequently, they can be given access to\n{{< glossary_tooltip text=\"Secrets\" term_id=\"secret\" >}} that app containers cannot access.\n* Because init containers run to completion before any app containers start, init containers offer\na mechanism to block or delay app container startup until a set of preconditions are met. Once\npreconditions are met, all of the app containers in a Pod can start in parallel.\n* Init containers can securely run utilities or custom code that would otherwise make an app\ncontainer image less secure. By keeping unnecessary tools separate you can limit the attack\nsurface of your app container image.\n"
  },
  {
    "question": "Is the Pod Template in a DaemonSet the same as a regular Pod?",
    "answer": "Yes, the Pod Template in a DaemonSet has exactly the same schema as a regular Pod, but it is nested and does not include `apiVersion` or `kind`.",
    "uuid": "bab8b054-cd64-4729-adc1-9ce936434d83",
    "question_with_context": "A user asked the following question:\nQuestion: Is the Pod Template in a DaemonSet the same as a regular Pod?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a DaemonSet SpecPod TemplateThe `.spec.template` is one of the required fields in `.spec`.  \nThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates).\nIt has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}},\nexcept it is nested and does not have an `apiVersion` or `kind`.  \nIn addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate\nlabels (see [pod selector](#pod-selector)).  \nA Pod Template in a DaemonSet must have a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)\nequal to `Always`, or be unspecified, which defaults to `Always`.\n"
  },
  {
    "question": "What happens if I don't specify a selector in my ReplicationController?",
    "answer": "If you don't specify a selector in your ReplicationController, it will default to the labels defined in `.spec.template.metadata.labels`.",
    "uuid": "dde9ed5c-ca0f-432d-86b0-d04044890f61",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I don't specify a selector in my ReplicationController?\nThis is about the following runbook:\nRunbook Title: Pod Selector\nRunbook Content: Writing a ReplicationController ManifestPod SelectorThe `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/#label-selectors). A ReplicationController\nmanages all the pods with labels that match the selector. It does not distinguish\nbetween pods that it created or deleted and pods that another person or process created or\ndeleted. This allows the ReplicationController to be replaced without affecting the running pods.  \nIf specified, the `.spec.template.metadata.labels` must be equal to the `.spec.selector`, or it will\nbe rejected by the API.  If `.spec.selector` is unspecified, it will be defaulted to\n`.spec.template.metadata.labels`.  \nAlso you should not normally create any pods whose labels match this selector, either directly, with\nanother ReplicationController, or with another controller such as Job. If you do so, the\nReplicationController thinks that it created the other pods.  Kubernetes does not stop you\nfrom doing this.  \nIf you do end up with multiple controllers that have overlapping selectors, you\nwill have to manage the deletion yourself (see [below](#working-with-replicationcontrollers)).\n"
  },
  {
    "question": "What command do I use to see the details of a specific revision in the rollout history?",
    "answer": "To see the details of a specific revision in the rollout history, use the command: `kubectl rollout history deployment/nginx-deployment --revision=2`.",
    "uuid": "c8336b44-ba83-495d-a40f-70bec028eb5e",
    "question_with_context": "A user asked the following question:\nQuestion: What command do I use to see the details of a specific revision in the rollout history?\nThis is about the following runbook:\nRunbook Title: Checking Rollout History of a Deployment\nRunbook Content: Rolling Back a DeploymentChecking Rollout History of a DeploymentFollow the steps given below to check the rollout history:  \n1. First, check the revisions of this Deployment:\n```shell\nkubectl rollout history deployment/nginx-deployment\n```\nThe output is similar to this:\n```\ndeployments \"nginx-deployment\"\nREVISION    CHANGE-CAUSE\n1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml\n2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n3           kubectl set image deployment/nginx-deployment nginx=nginx:1.161\n```  \n`CHANGE-CAUSE` is copied from the Deployment annotation `kubernetes.io/change-cause` to its revisions upon creation. You can specify the`CHANGE-CAUSE` message by:  \n* Annotating the Deployment with `kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"image updated to 1.16.1\"`\n* Manually editing the manifest of the resource.  \n2. To see the details of each revision, run:\n```shell\nkubectl rollout history deployment/nginx-deployment --revision=2\n```  \nThe output is similar to this:\n```\ndeployments \"nginx-deployment\" revision 2\nLabels:       app=nginx\npod-template-hash=1159050644\nAnnotations:  kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\nContainers:\nnginx:\nImage:      nginx:1.16.1\nPort:       80/TCP\nQoS Tier:\ncpu:      BestEffort\nmemory:   BestEffort\nEnvironment Variables:      <none>\nNo volumes.\n```\n"
  },
  {
    "question": "What happens if I don't set the pod deletion cost annotation?",
    "answer": "If you don't set the pod deletion cost annotation, the implicit value for the pod will be 0.",
    "uuid": "fa3e9bee-939f-4b73-81a0-8fdc39ef990e",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I don't set the pod deletion cost annotation?\nThis is about the following runbook:\nRunbook Title: Pod deletion cost\nRunbook Content: Working with ReplicaSetsPod deletion cost{{< feature-state for_k8s_version=\"v1.22\" state=\"beta\" >}}  \nUsing the [`controller.kubernetes.io/pod-deletion-cost`](/docs/reference/labels-annotations-taints/#pod-deletion-cost)\nannotation, users can set a preference regarding which pods to remove first when downscaling a ReplicaSet.  \nThe annotation should be set on the pod, the range is [-2147483648, 2147483647]. It represents the cost of\ndeleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletion\ncost are preferred to be deleted before pods with higher deletion cost.  \nThe implicit value for this annotation for pods that don't set it is 0; negative values are permitted.\nInvalid values will be rejected by the API server.  \nThis feature is beta and enabled by default. You can disable it using the\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n`PodDeletionCost` in both kube-apiserver and kube-controller-manager.  \n{{< note >}}\n- This is honored on a best-effort basis, so it does not offer any guarantees on pod deletion order.\n- Users should avoid updating the annotation frequently, such as updating it based on a metric value,\nbecause doing so will generate a significant number of pod updates on the apiserver.\n{{< /note >}}  \n#### Example Use Case  \nThe different pods of an application could have different utilization levels. On scale down, the application\nmay prefer to remove the pods with lower utilization. To avoid frequently updating the pods, the application\nshould update `controller.kubernetes.io/pod-deletion-cost` once before issuing a scale down (setting the\nannotation to a value proportional to pod utilization level). This works if the application itself controls\nthe down scaling; for example, the driver pod of a Spark deployment.\n"
  },
  {
    "question": "Can you explain how containers within a Pod share resources?",
    "answer": "Pods enable data sharing and communication among their constituent containers.",
    "uuid": "2f882126-59b6-4fde-b172-808681096f36",
    "question_with_context": "A user asked the following question:\nQuestion: Can you explain how containers within a Pod share resources?\nThis is about the following runbook:\nRunbook Title: Resource sharing and communication\nRunbook Content: Resource sharing and communicationPods enable data sharing and communication among their constituent\ncontainers.\n"
  },
  {
    "question": "Will my changes to a CronJob affect currently running Pods?",
    "answer": "No, changes made to a CronJob do not update existing Jobs or their Pods that are already running.",
    "uuid": "5a6fe8e3-cb0d-4820-801d-38af84d4820e",
    "question_with_context": "A user asked the following question:\nQuestion: Will my changes to a CronJob affect currently running Pods?\nThis is about the following runbook:\nRunbook Title: Modifying a CronJob\nRunbook Content: CronJob limitations {#cron-job-limitations}Modifying a CronJobBy design, a CronJob contains a template for _new_ Jobs.\nIf you modify an existing CronJob, the changes you make will apply to new Jobs that\nstart to run after your modification is complete. Jobs (and their Pods) that have already\nstarted continue to run without changes.\nThat is, the CronJob does _not_ update existing Jobs, even if those remain running.\n"
  },
  {
    "question": "When should I use multiple containers in a single Pod?",
    "answer": "You should use multiple containers in a single Pod only in specific instances where your containers are tightly coupled and need to share resources.",
    "uuid": "fd5321e5-73a8-403a-a163-7b1ae4c03016",
    "question_with_context": "A user asked the following question:\nQuestion: When should I use multiple containers in a single Pod?\nThis is about the following runbook:\nRunbook Title: What is a Pod?\nRunbook Content: What is a Pod?{{< note >}}\nYou need to install a [container runtime](/docs/setup/production-environment/container-runtimes/)\ninto each node in the cluster so that Pods can run there.\n{{< /note >}}  \nThe shared context of a Pod is a set of Linux namespaces, cgroups, and\npotentially other facets of isolation - the same things that isolate a {{< glossary_tooltip text=\"container\" term_id=\"container\" >}}. Within a Pod's context, the individual applications may have\nfurther sub-isolations applied.  \nA Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.  \nPods in a Kubernetes cluster are used in two main ways:  \n* **Pods that run a single container**. The \"one-container-per-Pod\" model is the\nmost common Kubernetes use case; in this case, you can think of a Pod as a\nwrapper around a single container; Kubernetes manages Pods rather than managing\nthe containers directly.\n* **Pods that run multiple containers that need to work together**. A Pod can\nencapsulate an application composed of\n[multiple co-located containers](#how-pods-manage-multiple-containers) that are\ntightly coupled and need to share resources. These co-located containers\nform a single cohesive unit.  \nGrouping multiple co-located and co-managed containers in a single Pod is a\nrelatively advanced use case. You should use this pattern only in specific\ninstances in which your containers are tightly coupled.  \nYou don't need to run multiple containers to provide replication (for resilience\nor capacity); if you need multiple replicas, see\n[Workload management](/docs/concepts/workloads/controllers/).\n"
  },
  {
    "question": "What are the constraints for assigning subordinate ID ranges to the kubelet user?",
    "answer": "The constraints for assigning subordinate ID ranges to the kubelet user include: the starting UID must be a multiple of 65536 and at least 65536, the count must be a multiple of 65536 and at least `65536 x <maxPods>`, both user IDs and group IDs must have the same range, and the configuration must be a single line without overlaps.",
    "uuid": "2930fca9-8c14-475b-b6c2-66e40d0482ab",
    "question_with_context": "A user asked the following question:\nQuestion: What are the constraints for assigning subordinate ID ranges to the kubelet user?\nThis is about the following runbook:\nRunbook Title: Set up a node to support user namespaces\nRunbook Content: Set up a node to support user namespacesBy default, the kubelet assigns pods UIDs/GIDs above the range 0-65535, based on\nthe assumption that the host's files and processes use UIDs/GIDs within this\nrange, which is standard for most Linux distributions. This approach prevents\nany overlap between the UIDs/GIDs of the host and those of the pods.  \nAvoiding the overlap is important to mitigate the impact of vulnerabilities such\nas [CVE-2021-25741][CVE-2021-25741], where a pod can potentially read arbitrary\nfiles in the host. If the UIDs/GIDs of the pod and the host don't overlap, it is\nlimited what a pod would be able to do: the pod UID/GID won't match the host's\nfile owner/group.  \nThe kubelet can use a custom range for user IDs and group IDs for pods. To\nconfigure a custom range, the node needs to have:  \n* A user `kubelet` in the system (you cannot use any other username here)\n* The binary `getsubids` installed (part of [shadow-utils][shadow-utils]) and\nin the `PATH` for the kubelet binary.\n* A configuration of subordinate UIDs/GIDs for the `kubelet` user (see\n[`man 5 subuid`](https://man7.org/linux/man-pages/man5/subuid.5.html) and\n[`man 5 subgid`](https://man7.org/linux/man-pages/man5/subgid.5.html)).  \nThis setting only gathers the UID/GID range configuration and does not change\nthe user executing the `kubelet`.  \nYou must follow some constraints for the subordinate ID range that you assign\nto the `kubelet` user:  \n* The subordinate user ID, that starts the UID range for Pods, **must** be a\nmultiple of 65536 and must also be greater than or equal to 65536. In other\nwords, you cannot use any ID from the range 0-65535 for Pods; the kubelet\nimposes this restriction to make it difficult to create an accidentally insecure\nconfiguration.  \n* The subordinate ID count must be a multiple of 65536  \n* The subordinate ID count must be at least `65536 x <maxPods>` where `<maxPods>`\nis the maximum number of pods that can run on the node.  \n* You must assign the same range for both user IDs and for group IDs, It doesn't\nmatter if other users have user ID ranges that don't align with the group ID\nranges.  \n* None of the assigned ranges should overlap with any other assignment.  \n* The subordinate configuration must be only one line. In other words, you can't\nhave multiple ranges.  \nFor example, you could define `/etc/subuid` and `/etc/subgid` to both have\nthese entries for the `kubelet` user:  \n```\n# The format is\n#   name:firstID:count of IDs\n# where\n# - firstID is 65536 (the minimum value possible)\n# - count of IDs is 110 (default limit for number of) * 65536\nkubelet:65536:7208960\n```  \n[CVE-2021-25741]: https://github.com/kubernetes/kubernetes/issues/104980\n[shadow-utils]: https://github.com/shadow-maint/shadow\n"
  },
  {
    "question": "What happens to PVCs when a StatefulSet is deleted?",
    "answer": "When a StatefulSet is deleted, the behavior of PVCs depends on the `whenDeleted` policy. If set to `Delete`, all PVCs from the `volumeClaimTemplate` are deleted after their Pods have been deleted. If set to `Retain`, the PVCs are not affected and remain intact.",
    "uuid": "587393e3-236c-4943-ba0f-3f3e589d8e1d",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to PVCs when a StatefulSet is deleted?\nThis is about the following runbook:\nRunbook Title: PersistentVolumeClaim retention\nRunbook Content: PersistentVolumeClaim retention{{< feature-state for_k8s_version=\"v1.27\" state=\"beta\" >}}  \nThe optional `.spec.persistentVolumeClaimRetentionPolicy` field controls if\nand how PVCs are deleted during the lifecycle of a StatefulSet. You must enable the\n`StatefulSetAutoDeletePVC` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\non the API server and the controller manager to use this field.\nOnce enabled, there are two policies you can configure for each StatefulSet:  \n`whenDeleted`\n: configures the volume retention behavior that applies when the StatefulSet is deleted  \n`whenScaled`\n: configures the volume retention behavior that applies when the replica count of\nthe StatefulSet   is reduced; for example, when scaling down the set.  \nFor each policy that you can configure, you can set the value to either `Delete` or `Retain`.  \n`Delete`\n: The PVCs created from the StatefulSet `volumeClaimTemplate` are deleted for each Pod\naffected by the policy. With the `whenDeleted` policy all PVCs from the\n`volumeClaimTemplate` are deleted after their Pods have been deleted. With the\n`whenScaled` policy, only PVCs corresponding to Pod replicas being scaled down are\ndeleted, after their Pods have been deleted.  \n`Retain` (default)\n: PVCs from the `volumeClaimTemplate` are not affected when their Pod is\ndeleted. This is the behavior before this new feature.  \nBear in mind that these policies **only** apply when Pods are being removed due to the\nStatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSet\nfails due to node failure, and the control plane creates a replacement Pod, the StatefulSet\nretains the existing PVC.  The existing volume is unaffected, and the cluster will attach it to\nthe node where the new Pod is about to launch.  \nThe default for policies is `Retain`, matching the StatefulSet behavior before this new feature.  \nHere is an example policy.  \n```yaml\napiVersion: apps/v1\nkind: StatefulSet\n...\nspec:\npersistentVolumeClaimRetentionPolicy:\nwhenDeleted: Retain\nwhenScaled: Delete\n...\n```  \nThe StatefulSet {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} adds\n[owner references](/docs/concepts/overview/working-with-objects/owners-dependents/#owner-references-in-object-specifications)\nto its PVCs, which are then deleted by the {{<glossary_tooltip text=\"garbage collector\"\nterm_id=\"garbage-collection\">}} after the Pod is terminated. This enables the Pod to\ncleanly unmount all volumes before the PVCs are deleted (and before the backing PV and\nvolume are deleted, depending on the retain policy).  When you set the `whenDeleted`\npolicy to `Delete`, an owner reference to the StatefulSet instance is placed on all PVCs\nassociated with that StatefulSet.  \nThe `whenScaled` policy must delete PVCs only when a Pod is scaled down, and not when a\nPod is deleted for another reason. When reconciling, the StatefulSet controller compares\nits desired replica count to the actual Pods present on the cluster. Any StatefulSet Pod\nwhose id greater than the replica count is condemned and marked for deletion. If the\n`whenScaled` policy is `Delete`, the condemned Pods are first set as owners to the\nassociated StatefulSet template PVCs, before the Pod is deleted. This causes the PVCs\nto be garbage collected after only the condemned Pods have terminated.  \nThis means that if the controller crashes and restarts, no Pod will be deleted before its\nowner reference has been updated appropriate to the policy. If a condemned Pod is\nforce-deleted while the controller is down, the owner reference may or may not have been\nset up, depending on when the controller crashed. It may take several reconcile loops to\nupdate the owner references, so some condemned Pods may have set up owner references and\nothers may not. For this reason we recommend waiting for the controller to come back up,\nwhich will verify owner references before terminating Pods. If that is not possible, the\noperator should verify the owner references on PVCs to ensure the expected objects are\ndeleted when Pods are force-deleted.\n"
  },
  {
    "question": "How do I ensure that a DaemonSet Pod runs on all eligible nodes?",
    "answer": "You can use a DaemonSet to ensure that all eligible nodes run a copy of a Pod. The DaemonSet controller creates a Pod for each eligible node.",
    "uuid": "81b12bb6-fa7a-4fe6-be3d-8d0b8b3b6f16",
    "question_with_context": "A user asked the following question:\nQuestion: How do I ensure that a DaemonSet Pod runs on all eligible nodes?\nThis is about the following runbook:\nRunbook Title: How Daemon Pods are scheduled\nRunbook Content: How Daemon Pods are scheduledA DaemonSet can be used to ensure that all eligible nodes run a copy of a Pod.\nThe DaemonSet controller creates a Pod for each eligible node and adds the\n`spec.affinity.nodeAffinity` field of the Pod to match the target host. After\nthe Pod is created, the default scheduler typically takes over and then binds\nthe Pod to the target host by setting the `.spec.nodeName` field.  If the new\nPod cannot fit on the node, the default scheduler may preempt (evict) some of\nthe existing Pods based on the\n[priority](/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority)\nof the new Pod.  \n{{< note >}}\nIf it's important that the DaemonSet pod run on each node, it's often desirable\nto set the `.spec.template.spec.priorityClassName` of the DaemonSet to a\n[PriorityClass](/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass)\nwith a higher priority to ensure that this eviction occurs.\n{{< /note >}}  \nThe user can specify a different scheduler for the Pods of the DaemonSet, by\nsetting the `.spec.template.spec.schedulerName` field of the DaemonSet.  \nThe original node affinity specified at the\n`.spec.template.spec.affinity.nodeAffinity` field (if specified) is taken into\nconsideration by the DaemonSet controller when evaluating the eligible nodes,\nbut is replaced on the created Pod with the node affinity that matches the name\nof the eligible node.  \n```yaml\nnodeAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\nnodeSelectorTerms:\n- matchFields:\n- key: metadata.name\noperator: In\nvalues:\n- target-host-name\n```\n"
  },
  {
    "question": "Can I use Volume Claim Templates if I already have a PersistentVolume?",
    "answer": "Yes, you can use Volume Claim Templates if the cluster already contains a PersistentVolume with the correct StorageClass and sufficient available storage space.",
    "uuid": "d8e5a620-bc12-4764-83a5-c62b0dbe00e5",
    "question_with_context": "A user asked the following question:\nQuestion: Can I use Volume Claim Templates if I already have a PersistentVolume?\nThis is about the following runbook:\nRunbook Title: Volume Claim Templates\nRunbook Content: ComponentsVolume Claim TemplatesYou can set the `.spec.volumeClaimTemplates` field to create a\n[PersistentVolumeClaim](/docs/concepts/storage/persistent-volumes/).\nThis will provide stable storage to the StatefulSet if either  \n* The StorageClass specified for the volume claim is set up to use [dynamic\nprovisioning](/docs/concepts/storage/dynamic-provisioning/), or\n* The cluster already contains a PersistentVolume with the correct StorageClass\nand sufficient available storage space.\n"
  },
  {
    "question": "What do I need to define in a Pod's spec to use the downward API?",
    "answer": "At the API level, the `spec` for a Pod always defines at least one Container.",
    "uuid": "6c1fc433-91dd-4560-b46a-714611e8c6df",
    "question_with_context": "A user asked the following question:\nQuestion: What do I need to define in a Pod's spec to use the downward API?\nThis is about the following runbook:\nRunbook Title: Available fields\nRunbook Content: Available fieldsOnly some Kubernetes API fields are available through the downward API. This\nsection lists which fields you can make available.  \nYou can pass information from available Pod-level fields using `fieldRef`.\nAt the API level, the `spec` for a Pod always defines at least one\n[Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container).\nYou can pass information from available Container-level fields using\n`resourceFieldRef`.\n"
  },
  {
    "question": "What is the Kubernetes version that supports ephemeral containers?",
    "answer": "Ephemeral containers are stable and supported in Kubernetes version v1.25.",
    "uuid": "d5b9a50c-34b5-4157-87bf-799e6f3b49a7",
    "question_with_context": "A user asked the following question:\nQuestion: What is the Kubernetes version that supports ephemeral containers?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- verb\n- yujuhong\ntitle: Ephemeral Containers\ncontent_type: concept\nweight: 60\n---  \n<!-- overview -->  \n{{< feature-state state=\"stable\" for_k8s_version=\"v1.25\" >}}  \nThis page provides an overview of ephemeral containers: a special type of container\nthat runs temporarily in an existing {{< glossary_tooltip term_id=\"pod\" >}} to\naccomplish user-initiated actions such as troubleshooting. You use ephemeral\ncontainers to inspect services rather than to build applications.  \n<!-- body -->\n"
  },
  {
    "question": "What happens if I remove a key from the Deployment selector?",
    "answer": "Removing a key from the Deployment selector does not require any changes in the Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but the removed label will still exist in any existing Pods and ReplicaSets.",
    "uuid": "c1e9adb7-e706-4369-84eb-4e82b8b7280f",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I remove a key from the Deployment selector?\nThis is about the following runbook:\nRunbook Title: Label selector updates\nRunbook Content: Updating a DeploymentLabel selector updatesIt is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.\nIn any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped\nall of the implications.  \n{{< note >}}\nIn API version `apps/v1`, a Deployment's label selector is immutable after it gets created.\n{{< /note >}}  \n* Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too,\notherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does\nnot select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and\ncreating a new ReplicaSet.\n* Selector updates changes the existing value in a selector key -- result in the same behavior as additions.\n* Selector removals removes an existing key from the Deployment selector -- do not require any changes in the\nPod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the\nremoved label still exists in any existing Pods and ReplicaSets.\n"
  },
  {
    "question": "Why should I use a ReplicaSet instead of just creating Bare Pods?",
    "answer": "A ReplicaSet replaces Pods that are deleted or terminated for any reason, such as node failure or disruptive node maintenance. It acts like a process supervisor, managing multiple Pods across multiple nodes, which is beneficial even if your application requires only a single Pod.",
    "uuid": "36b2c33b-2aa8-44cf-8d6f-16d0c955fcbd",
    "question_with_context": "A user asked the following question:\nQuestion: Why should I use a ReplicaSet instead of just creating Bare Pods?\nThis is about the following runbook:\nRunbook Title: Bare Pods\nRunbook Content: Alternatives to ReplicaSetBare PodsUnlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or\nterminated for any reason, such as in the case of node failure or disruptive node maintenance,\nsuch as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your\napplication requires only a single Pod. Think of it similarly to a process supervisor, only it\nsupervises multiple Pods across multiple nodes instead of individual processes on a single node. A\nReplicaSet delegates local container restarts to some agent on the node such as Kubelet.\n"
  },
  {
    "question": "Why would I use static pods instead of DaemonSet?",
    "answer": "Static Pods do not depend on the apiserver, making them useful in cluster bootstrapping cases.",
    "uuid": "6953f252-a8be-413b-88f9-79019a12149d",
    "question_with_context": "A user asked the following question:\nQuestion: Why would I use static pods instead of DaemonSet?\nThis is about the following runbook:\nRunbook Title: Static Pods\nRunbook Content: Alternatives to DaemonSetStatic PodsIt is possible to create Pods by writing a file to a certain directory watched by Kubelet.  These\nare called [static pods](/docs/tasks/configure-pod-container/static-pod/).\nUnlike DaemonSet, static Pods cannot be managed with kubectl\nor other Kubernetes API clients.  Static Pods do not depend on the apiserver, making them useful\nin cluster bootstrapping cases.  Also, static Pods may be deprecated in the future.\n"
  },
  {
    "question": "What does a Replication Controller manage?",
    "answer": "A Replication Controller manages Pods which are not expected to terminate, such as web servers.",
    "uuid": "8e8361be-2ec5-43d2-991d-11726e6ad39e",
    "question_with_context": "A user asked the following question:\nQuestion: What does a Replication Controller manage?\nThis is about the following runbook:\nRunbook Title: Replication Controller\nRunbook Content: AlternativesReplication ControllerJobs are complementary to [Replication Controllers](/docs/concepts/workloads/controllers/replicationcontroller/).\nA Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Job\nmanages Pods that are expected to terminate (e.g. batch tasks).  \nAs discussed in [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/), `Job` is *only* appropriate\nfor pods with `RestartPolicy` equal to `OnFailure` or `Never`.\n(Note: If `RestartPolicy` is not set, the default value is `Always`.)\n"
  },
  {
    "question": "When should I use a DaemonSet instead of a ReplicationController?",
    "answer": "Use a DaemonSet instead of a ReplicationController for pods that provide a machine-level function, such as machine monitoring or machine logging.",
    "uuid": "a1f10dda-df4c-4ca7-af28-c0ffbdb2b978",
    "question_with_context": "A user asked the following question:\nQuestion: When should I use a DaemonSet instead of a ReplicationController?\nThis is about the following runbook:\nRunbook Title: DaemonSet\nRunbook Content: Alternatives to ReplicationControllerDaemonSetUse a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicationController for pods that provide a\nmachine-level function, such as machine monitoring or machine logging.  These pods have a lifetime that is tied\nto a machine lifetime: the pod needs to be running on the machine before other pods start, and are\nsafe to terminate when the machine is otherwise ready to be rebooted/shutdown.\n"
  },
  {
    "question": "What happens to the pods managed by a DaemonSet when the machine is ready to be rebooted?",
    "answer": "The pods managed by a DaemonSet are safe to terminate when the machine is otherwise ready to be rebooted or shutdown.",
    "uuid": "a1f10dda-df4c-4ca7-af28-c0ffbdb2b978",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to the pods managed by a DaemonSet when the machine is ready to be rebooted?\nThis is about the following runbook:\nRunbook Title: DaemonSet\nRunbook Content: Alternatives to ReplicationControllerDaemonSetUse a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicationController for pods that provide a\nmachine-level function, such as machine monitoring or machine logging.  These pods have a lifetime that is tied\nto a machine lifetime: the pod needs to be running on the machine before other pods start, and are\nsafe to terminate when the machine is otherwise ready to be rebooted/shutdown.\n"
  },
  {
    "question": "Can I use ReplicaSets independently, or should I always use Deployments?",
    "answer": "While ReplicaSets can be used independently, it is recommended to use Deployments when you want ReplicaSets.",
    "uuid": "ca016763-c204-4ab9-af89-09d3611e05e7",
    "question_with_context": "A user asked the following question:\nQuestion: Can I use ReplicaSets independently, or should I always use Deployments?\nThis is about the following runbook:\nRunbook Title: Deployment (recommended)\nRunbook Content: Alternatives to ReplicaSetDeployment (recommended)[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is an object which can own ReplicaSets and update\nthem and their Pods via declarative, server-side rolling updates.\nWhile ReplicaSets can be used independently, today they're mainly used by Deployments as a mechanism to orchestrate Pod\ncreation, deletion and updates. When you use Deployments you don't have to worry about managing the ReplicaSets that\nthey create. Deployments own and manage their ReplicaSets.\nAs such, it is recommended to use Deployments when you want ReplicaSets.\n"
  },
  {
    "question": "What type of applications can I use ReplicationControllers for?",
    "answer": "ReplicationControllers can be used to maintain availability of replicated stateless servers, master-elected, sharded, and worker-pool applications.",
    "uuid": "774fd40c-9a28-4f8d-8358-3dd7aacbfe1e",
    "question_with_context": "A user asked the following question:\nQuestion: What type of applications can I use ReplicationControllers for?\nThis is about the following runbook:\nRunbook Title: Writing programs for Replication\nRunbook Content: Writing programs for ReplicationPods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the [RabbitMQ work queues](https://www.rabbitmq.com/tutorials/tutorial-two-python.html), as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.\n"
  },
  {
    "question": "Where can I find information on managing application availability during disruptions?",
    "answer": "You can read about managing application availability during disruptions by checking the [PodDisruptionBudget documentation](/docs/concepts/workloads/pods/disruptions/).",
    "uuid": "6fec0856-c741-43b2-a0b8-f6040e9c9289",
    "question_with_context": "A user asked the following question:\nQuestion: Where can I find information on managing application availability during disruptions?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Learn about [Deployments](/docs/concepts/workloads/controllers/deployment/).\n* [Run a Stateless Application Using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/),\nwhich relies on ReplicaSets to work.\n* `ReplicaSet` is a top-level resource in the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/replica-set-v1\" >}}\nobject definition to understand the API for replica sets.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how\nyou can use it to manage application availability during disruptions.\n"
  },
  {
    "question": "What should I do if my StatefulSet rollout is stuck due to a bad Pod configuration?",
    "answer": "You need to revert the Pod template to a good configuration, but also delete any Pods that StatefulSet had already attempted to run with the bad configuration.",
    "uuid": "10973aa5-42f5-4429-b4da-5de9cc6ae0c6",
    "question_with_context": "A user asked the following question:\nQuestion: What should I do if my StatefulSet rollout is stuck due to a bad Pod configuration?\nThis is about the following runbook:\nRunbook Title: Forced rollback\nRunbook Content: Rolling UpdatesForced rollbackWhen using [Rolling Updates](#rolling-updates) with the default\n[Pod Management Policy](#pod-management-policies) (`OrderedReady`),\nit's possible to get into a broken state that requires manual intervention to repair.  \nIf you update the Pod template to a configuration that never becomes Running and\nReady (for example, due to a bad binary or application-level configuration error),\nStatefulSet will stop the rollout and wait.  \nIn this state, it's not enough to revert the Pod template to a good configuration.\nDue to a [known issue](https://github.com/kubernetes/kubernetes/issues/67250),\nStatefulSet will continue to wait for the broken Pod to become Ready\n(which never happens) before it will attempt to revert it back to the working\nconfiguration.  \nAfter reverting the template, you must also delete any Pods that StatefulSet had\nalready attempted to run with the bad configuration.\nStatefulSet will then begin to recreate the Pods using the reverted template.\n"
  },
  {
    "question": "How can I classify a Pod as BestEffort?",
    "answer": "A Pod is classified as `BestEffort` if it doesn't meet the criteria for either `Guaranteed` or `Burstable`. This means none of the Containers in the Pod can have a memory limit or request, and none can have a CPU limit or request.",
    "uuid": "e78046a9-5089-4963-892b-d25b03565051",
    "question_with_context": "A user asked the following question:\nQuestion: How can I classify a Pod as BestEffort?\nThis is about the following runbook:\nRunbook Title: BestEffort\nRunbook Content: Quality of Service classesBestEffortPods in the `BestEffort` QoS class can use node resources that aren't specifically assigned\nto Pods in other QoS classes. For example, if you have a node with 16 CPU cores available to the\nkubelet, and you assign 4 CPU cores to a `Guaranteed` Pod, then a Pod in the `BestEffort`\nQoS class can try to use any amount of the remaining 12 CPU cores.  \nThe kubelet prefers to evict `BestEffort` Pods if the node comes under resource pressure.  \n#### Criteria  \nA Pod has a QoS class of `BestEffort` if it doesn't meet the criteria for either `Guaranteed`\nor `Burstable`. In other words, a Pod is `BestEffort` only if none of the Containers in the Pod have a\nmemory limit or a memory request, and none of the Containers in the Pod have a\nCPU limit or a CPU request.\nContainers in a Pod can request other resources (not CPU or memory) and still be classified as\n`BestEffort`.\n"
  },
  {
    "question": "Is there a specific format for the name of a DaemonSet object?",
    "answer": "Yes, the name of a DaemonSet object must be a valid DNS subdomain name.",
    "uuid": "c8eedb18-162d-4815-a69f-ec4adc67d4da",
    "question_with_context": "A user asked the following question:\nQuestion: Is there a specific format for the name of a DaemonSet object?\nThis is about the following runbook:\nRunbook Title: Required Fields\nRunbook Content: Writing a DaemonSet SpecRequired FieldsAs with all other Kubernetes config, a DaemonSet needs `apiVersion`, `kind`, and `metadata` fields.  For\ngeneral information about working with config files, see\n[running stateless applications](/docs/tasks/run-application/run-stateless-application-deployment/)\nand [object management using kubectl](/docs/concepts/overview/working-with-objects/object-management/).  \nThe name of a DaemonSet object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).  \nA DaemonSet also needs a\n[`.spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)\nsection.\n"
  },
  {
    "question": "Can I use a success policy with other terminating policies like `.spec.backoffLimit`?",
    "answer": "Yes, but if you specify both a success policy and terminating policies such as `.spec.backoffLimit`, once the Job meets either policy, the job controller will respect the terminating policy and ignore the success policy.",
    "uuid": "2062c569-9100-42c5-916e-a54cde4a384d",
    "question_with_context": "A user asked the following question:\nQuestion: Can I use a success policy with other terminating policies like `.spec.backoffLimit`?\nThis is about the following runbook:\nRunbook Title: Success policy {#success-policy}\nRunbook Content: Success policy {#success-policy}{{< feature-state feature_gate_name=\"JobSuccessPolicy\" >}}  \n{{< note >}}\nYou can only configure a success policy for an Indexed Job if you have the\n`JobSuccessPolicy` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nenabled in your cluster.\n{{< /note >}}  \nWhen creating an Indexed Job, you can define when a Job can be declared as succeeded using a `.spec.successPolicy`,\nbased on the pods that succeeded.  \nBy default, a Job succeeds when the number of succeeded Pods equals `.spec.completions`.\nThese are some situations where you might want additional control for declaring a Job succeeded:  \n* When running simulations with different parameters,\nyou might not need all the simulations to succeed for the overall Job to be successful.\n* When following a leader-worker pattern, only the success of the leader determines the success or\nfailure of a Job. Examples of this are frameworks like MPI and PyTorch etc.  \nYou can configure a success policy, in the `.spec.successPolicy` field,\nto meet the above use cases. This policy can handle Job success based on the\nsucceeded pods. After the Job meets the success policy, the job controller terminates the lingering Pods.\nA success policy is defined by rules. Each rule can take one of the following forms:  \n* When you specify the `succeededIndexes` only,\nonce all indexes specified in the `succeededIndexes` succeed, the job controller marks the Job as succeeded.\nThe `succeededIndexes` must be a list of intervals between 0 and `.spec.completions-1`.\n* When you specify the `succeededCount` only,\nonce the number of succeeded indexes reaches the `succeededCount`, the job controller marks the Job as succeeded.\n* When you specify both `succeededIndexes` and `succeededCount`,\nonce the number of succeeded indexes from the subset of indexes specified in the `succeededIndexes` reaches the `succeededCount`,\nthe job controller marks the Job as succeeded.  \nNote that when you specify multiple rules in the `.spec.successPolicy.rules`,\nthe job controller evaluates the rules in order. Once the Job meets a rule, the job controller ignores remaining rules.  \nHere is a manifest for a Job with `successPolicy`:  \n{{% code_sample file=\"/controllers/job-success-policy.yaml\" %}}  \nIn the example above, both `succeededIndexes` and `succeededCount` have been specified.\nTherefore, the job controller will mark the Job as succeeded and terminate the lingering Pods\nwhen either of the specified indexes, 0, 2, or 3, succeed.\nThe Job that meets the success policy gets the `SuccessCriteriaMet` condition with a `SuccessPolicy` reason.\nAfter the removal of the lingering Pods is issued, the Job gets the `Complete` condition.  \nNote that the `succeededIndexes` is represented as intervals separated by a hyphen.\nThe number are listed in represented by the first and last element of the series, separated by a hyphen.  \n{{< note >}}\nWhen you specify both a success policy and some terminating policies such as `.spec.backoffLimit` and `.spec.podFailurePolicy`,\nonce the Job meets either policy, the job controller respects the terminating policy and ignores the success policy.\n{{< /note >}}\n"
  },
  {
    "question": "What will happen if I create Pods before the ReplicaSet?",
    "answer": "If you create Pods before the ReplicaSet, the ReplicaSet will acquire those Pods and will only create new ones according to its spec until the total number of Pods matches its desired count.",
    "uuid": "8e114e96-35c1-4692-9fde-ad59dc692936",
    "question_with_context": "A user asked the following question:\nQuestion: What will happen if I create Pods before the ReplicaSet?\nThis is about the following runbook:\nRunbook Title: Non-Template Pod acquisitions\nRunbook Content: Non-Template Pod acquisitionsWhile you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have\nlabels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited\nto owning Pods specified by its template-- it can acquire other Pods in the manner specified in the previous sections.  \nTake the previous frontend ReplicaSet example, and the Pods specified in the following manifest:  \n{{% code_sample file=\"pods/pod-rs.yaml\" %}}  \nAs those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend\nReplicaSet, they will immediately be acquired by it.  \nSuppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to\nfulfill its replica count requirement:  \n```shell\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\n```  \nThe new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over\nits desired count.  \nFetching the Pods:  \n```shell\nkubectl get pods\n```  \nThe output shows that the new Pods are either already terminated, or in the process of being terminated:  \n```\nNAME             READY   STATUS        RESTARTS   AGE\nfrontend-b2zdv   1/1     Running       0          10m\nfrontend-vcmts   1/1     Running       0          10m\nfrontend-wtsmm   1/1     Running       0          10m\npod1             0/1     Terminating   0          1s\npod2             0/1     Terminating   0          1s\n```  \nIf you create the Pods first:  \n```shell\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\n```  \nAnd then create the ReplicaSet however:  \n```shell\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\n```  \nYou shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the\nnumber of its new Pods and the original matches its desired count. As fetching the Pods:  \n```shell\nkubectl get pods\n```  \nWill reveal in its output:\n```\nNAME             READY   STATUS    RESTARTS   AGE\nfrontend-hmmj2   1/1     Running   0          9s\npod1             1/1     Running   0          36s\npod2             1/1     Running   0          36s\n```  \nIn this manner, a ReplicaSet can own a non-homogeneous set of Pods\n"
  },
  {
    "question": "What values can I use for maxUnavailable?",
    "answer": "The value for maxUnavailable can be an absolute number (like `5`) or a percentage of desired Pods (like `10%`).",
    "uuid": "a3ea79ab-53cd-423b-9b9c-b3a792971e4b",
    "question_with_context": "A user asked the following question:\nQuestion: What values can I use for maxUnavailable?\nThis is about the following runbook:\nRunbook Title: Maximum unavailable Pods\nRunbook Content: Rolling UpdatesMaximum unavailable Pods{{< feature-state for_k8s_version=\"v1.24\" state=\"alpha\" >}}  \nYou can control the maximum number of Pods that can be unavailable during an update\nby specifying the `.spec.updateStrategy.rollingUpdate.maxUnavailable` field.\nThe value can be an absolute number (for example, `5`) or a percentage of desired\nPods (for example, `10%`). Absolute number is calculated from the percentage value\nby rounding it up. This field cannot be 0. The default setting is 1.  \nThis field applies to all Pods in the range `0` to `replicas - 1`. If there is any\nunavailable Pod in the range `0` to `replicas - 1`, it will be counted towards\n`maxUnavailable`.  \n{{< note >}}\nThe `maxUnavailable` field is in Alpha stage and it is honored only by API servers\nthat are running with the `MaxUnavailableStatefulSet`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nenabled.\n{{< /note >}}\n"
  },
  {
    "question": "How can I set up extra conditions for Pod readiness in Kubernetes?",
    "answer": "You can set up extra conditions for Pod readiness by using the `readinessGates` in the Pod's `spec`. This allows you to specify a list of additional conditions that the kubelet evaluates for Pod readiness.",
    "uuid": "80aa7fcf-1153-4512-888e-5ad9a9ee7e49",
    "question_with_context": "A user asked the following question:\nQuestion: How can I set up extra conditions for Pod readiness in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Pod readiness {#pod-readiness-gate}\nRunbook Content: Pod conditionsPod readiness {#pod-readiness-gate}{{< feature-state for_k8s_version=\"v1.14\" state=\"stable\" >}}  \nYour application can inject extra feedback or signals into PodStatus:\n_Pod readiness_. To use this, set `readinessGates` in the Pod's `spec` to\nspecify a list of additional conditions that the kubelet evaluates for Pod readiness.  \nReadiness gates are determined by the current state of `status.condition`\nfields for the Pod. If Kubernetes cannot find such a condition in the\n`status.conditions` field of a Pod, the status of the condition\nis defaulted to \"`False`\".  \nHere is an example:  \n```yaml\nkind: Pod\n...\nspec:\nreadinessGates:\n- conditionType: \"www.example.com/feature-1\"\nstatus:\nconditions:\n- type: Ready                              # a built in PodCondition\nstatus: \"False\"\nlastProbeTime: null\nlastTransitionTime: 2018-01-01T00:00:00Z\n- type: \"www.example.com/feature-1\"        # an extra PodCondition\nstatus: \"False\"\nlastProbeTime: null\nlastTransitionTime: 2018-01-01T00:00:00Z\ncontainerStatuses:\n- containerID: docker://abcd...\nready: true\n...\n```  \nThe Pod conditions you add must have names that meet the Kubernetes\n[label key format](/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set).\n"
  },
  {
    "question": "What determines the cgroup settings for a Pod in Linux?",
    "answer": "The cgroup settings for a Pod in Linux are determined by the effective Pod request and limit.",
    "uuid": "7628a1f1-f2c6-4155-b996-04c612243bc8",
    "question_with_context": "A user asked the following question:\nQuestion: What determines the cgroup settings for a Pod in Linux?\nThis is about the following runbook:\nRunbook Title: Sidecar containers and Linux cgroups {#cgroups}\nRunbook Content: Resource sharing within containersSidecar containers and Linux cgroups {#cgroups}On Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod\nrequest and limit, the same as the scheduler.\n"
  },
  {
    "question": "How do I set up a Pod Selector in a ReplicaSet manifest?",
    "answer": "You can set up a Pod Selector in a ReplicaSet manifest by using the `.spec.selector` field, which is a label selector. For example, you can specify it like this: `matchLabels: tier: frontend`.",
    "uuid": "cfa73a27-18df-42d8-ab40-6d33b3c3d057",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set up a Pod Selector in a ReplicaSet manifest?\nThis is about the following runbook:\nRunbook Title: Pod Selector\nRunbook Content: Writing a ReplicaSet manifestPod SelectorThe `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/). As discussed\n[earlier](#how-a-replicaset-works) these are the labels used to identify potential Pods to acquire. In our\n`frontend.yaml` example, the selector was:  \n```yaml\nmatchLabels:\ntier: frontend\n```  \nIn the ReplicaSet, `.spec.template.metadata.labels` must match `spec.selector`, or it will\nbe rejected by the API.  \n{{< note >}}\nFor 2 ReplicaSets specifying the same `.spec.selector` but different\n`.spec.template.metadata.labels` and `.spec.template.spec` fields, each ReplicaSet ignores the\nPods created by the other ReplicaSet.\n{{< /note >}}\n"
  },
  {
    "question": "What happens to Pods that are removed from a ReplicaSet?",
    "answer": "Pods that are removed in this way will be replaced automatically, assuming that the number of replicas is not also changed.",
    "uuid": "4bb0841c-9aaa-4613-8b54-ae0f90a0d5fc",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to Pods that are removed from a ReplicaSet?\nThis is about the following runbook:\nRunbook Title: Isolating Pods from a ReplicaSet\nRunbook Content: Working with ReplicaSetsIsolating Pods from a ReplicaSetYou can remove Pods from a ReplicaSet by changing their labels. This technique may be used to remove Pods\nfrom service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (\nassuming that the number of replicas is not also changed).\n"
  },
  {
    "question": "What command do I use to rollback to a specific revision of my deployment?",
    "answer": "To rollback to a specific revision, use the command: `kubectl rollout undo deployment/nginx-deployment --to-revision=2`. This specifies the revision you want to rollback to.",
    "uuid": "d78a0bd2-e7b7-4234-ab63-1513286ffe17",
    "question_with_context": "A user asked the following question:\nQuestion: What command do I use to rollback to a specific revision of my deployment?\nThis is about the following runbook:\nRunbook Title: Rolling Back to a Previous Revision\nRunbook Content: Rolling Back a DeploymentRolling Back to a Previous RevisionFollow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.  \n1. Now you've decided to undo the current rollout and rollback to the previous revision:\n```shell\nkubectl rollout undo deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment rolled back\n```\nAlternatively, you can rollback to a specific revision by specifying it with `--to-revision`:  \n```shell\nkubectl rollout undo deployment/nginx-deployment --to-revision=2\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment rolled back\n```  \nFor more details about rollout related commands, read [`kubectl rollout`](/docs/reference/generated/kubectl/kubectl-commands#rollout).  \nThe Deployment is now rolled back to a previous stable revision. As you can see, a `DeploymentRollback` event\nfor rolling back to revision 2 is generated from Deployment controller.  \n2. Check if the rollback was successful and the Deployment is running as expected, run:\n```shell\nkubectl get deployment nginx-deployment\n```  \nThe output is similar to this:\n```\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           30m\n```\n3. Get the description of the Deployment:\n```shell\nkubectl describe deployment nginx-deployment\n```\nThe output is similar to this:\n```\nName:                   nginx-deployment\nNamespace:              default\nCreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500\nLabels:                 app=nginx\nAnnotations:            deployment.kubernetes.io/revision=4\nkubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\nSelector:               app=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\nLabels:  app=nginx\nContainers:\nnginx:\nImage:        nginx:1.16.1\nPort:         80/TCP\nHost Port:    0/TCP\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nConditions:\nType           Status  Reason\n----           ------  ------\nAvailable      True    MinimumReplicasAvailable\nProgressing    True    NewReplicaSetAvailable\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)\nEvents:\nType    Reason              Age   From                   Message\n----    ------              ----  ----                   -------\nNormal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0\nNormal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1\nNormal  DeploymentRollback  15s   deployment-controller  Rolled back deployment \"nginx-deployment\" to revision 2\nNormal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0\n```\n"
  },
  {
    "question": "How can I tell if my Kubernetes Deployment is complete?",
    "answer": "You can check if a Deployment has completed by using the command `kubectl rollout status`. If the rollout completed successfully, it will return a zero exit code.",
    "uuid": "ec777193-0c02-4298-af6c-786b43f49577",
    "question_with_context": "A user asked the following question:\nQuestion: How can I tell if my Kubernetes Deployment is complete?\nThis is about the following runbook:\nRunbook Title: Complete Deployment\nRunbook Content: Deployment statusComplete DeploymentKubernetes marks a Deployment as _complete_ when it has the following characteristics:  \n* All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any\nupdates you've requested have been completed.\n* All of the replicas associated with the Deployment are available.\n* No old replicas for the Deployment are running.  \nWhen the rollout becomes \u201ccomplete\u201d, the Deployment controller sets a condition with the following\nattributes to the Deployment's `.status.conditions`:  \n* `type: Progressing`\n* `status: \"True\"`\n* `reason: NewReplicaSetAvailable`  \nThis `Progressing` condition will retain a status value of `\"True\"` until a new rollout\nis initiated. The condition holds even when availability of replicas changes (which\ndoes instead affect the `Available` condition).  \nYou can check if a Deployment has completed by using `kubectl rollout status`. If the rollout completed\nsuccessfully, `kubectl rollout status` returns a zero exit code.  \n```shell\nkubectl rollout status deployment/nginx-deployment\n```\nThe output is similar to this:\n```\nWaiting for rollout to finish: 2 of 3 updated replicas are available...\ndeployment \"nginx-deployment\" successfully rolled out\n```\nand the exit status from `kubectl rollout` is 0 (success):\n```shell\necho $?\n```\n```\n0\n```\n"
  },
  {
    "question": "How can I ensure my Pod only receives traffic when it's ready?",
    "answer": "To ensure your Pod only receives traffic when it's ready, specify a readiness probe. This means the Pod will start without receiving any traffic and will only start receiving traffic after the readiness probe succeeds.",
    "uuid": "d976ddc4-e932-4d9a-b691-089fcfdc239b",
    "question_with_context": "A user asked the following question:\nQuestion: How can I ensure my Pod only receives traffic when it's ready?\nThis is about the following runbook:\nRunbook Title: Types of probe\nRunbook Content: Container probesTypes of probeThe kubelet can optionally perform and react to three kinds of probes on running\ncontainers:  \n`livenessProbe`\n: Indicates whether the container is running. If\nthe liveness probe fails, the kubelet kills the container, and the container\nis subjected to its [restart policy](#restart-policy). If a container does not\nprovide a liveness probe, the default state is `Success`.  \n`readinessProbe`\n: Indicates whether the container is ready to respond to requests.\nIf the readiness probe fails, the endpoints controller removes the Pod's IP\naddress from the endpoints of all Services that match the Pod. The default\nstate of readiness before the initial delay is `Failure`. If a container does\nnot provide a readiness probe, the default state is `Success`.  \n`startupProbe`\n: Indicates whether the application within the container is started.\nAll other probes are disabled if a startup probe is provided, until it succeeds.\nIf the startup probe fails, the kubelet kills the container, and the container\nis subjected to its [restart policy](#restart-policy). If a container does not\nprovide a startup probe, the default state is `Success`.  \nFor more information about how to set up a liveness, readiness, or startup probe,\nsee [Configure Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).  \n#### When should you use a liveness probe?  \nIf the process in your container is able to crash on its own whenever it\nencounters an issue or becomes unhealthy, you do not necessarily need a liveness\nprobe; the kubelet will automatically perform the correct action in accordance\nwith the Pod's `restartPolicy`.  \nIf you'd like your container to be killed and restarted if a probe fails, then\nspecify a liveness probe, and specify a `restartPolicy` of Always or OnFailure.  \n#### When should you use a readiness probe?  \nIf you'd like to start sending traffic to a Pod only when a probe succeeds,\nspecify a readiness probe. In this case, the readiness probe might be the same\nas the liveness probe, but the existence of the readiness probe in the spec means\nthat the Pod will start without receiving any traffic and only start receiving\ntraffic after the probe starts succeeding.  \nIf you want your container to be able to take itself down for maintenance, you\ncan specify a readiness probe that checks an endpoint specific to readiness that\nis different from the liveness probe.  \nIf your app has a strict dependency on back-end services, you can implement both\na liveness and a readiness probe. The liveness probe passes when the app itself\nis healthy, but the readiness probe additionally checks that each required\nback-end service is available. This helps you avoid directing traffic to Pods\nthat can only respond with error messages.  \nIf your container needs to work on loading large data, configuration files, or\nmigrations during startup, you can use a\n[startup probe](#when-should-you-use-a-startup-probe). However, if you want to\ndetect the difference between an app that has failed and an app that is still\nprocessing its startup data, you might prefer a readiness probe.  \n{{< note >}}\nIf you want to be able to drain requests when the Pod is deleted, you do not\nnecessarily need a readiness probe; on deletion, the Pod automatically puts itself\ninto an unready state regardless of whether the readiness probe exists.\nThe Pod remains in the unready state while it waits for the containers in the Pod\nto stop.\n{{< /note >}}  \n#### When should you use a startup probe?  \nStartup probes are useful for Pods that have containers that take a long time to\ncome into service. Rather than set a long liveness interval, you can configure\na separate configuration for probing the container as it starts up, allowing\na time longer than the liveness interval would allow.  \nIf your container usually starts in more than\n`initialDelaySeconds + failureThreshold \u00d7 periodSeconds`, you should specify a\nstartup probe that checks the same endpoint as the liveness probe. The default for\n`periodSeconds` is 10s. You should then set its `failureThreshold` high enough to\nallow the container to start, without changing the default values of the liveness\nprobe. This helps to protect against deadlocks.\n"
  },
  {
    "question": "What happens to my Deployment when I pause the rollout?",
    "answer": "When you pause the rollout, the initial state of the Deployment continues to function, but new updates to the Deployment will not have any effect until the rollout is resumed.",
    "uuid": "90ea9916-2644-45dc-91aa-e401a8fd8985",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to my Deployment when I pause the rollout?\nThis is about the following runbook:\nRunbook Title: Pausing and Resuming a rollout of a Deployment {#pausing-and-resuming-a-deployment}\nRunbook Content: Pausing and Resuming a rollout of a Deployment {#pausing-and-resuming-a-deployment}When you update a Deployment, or plan to, you can pause rollouts\nfor that Deployment before you trigger one or more updates. When\nyou're ready to apply those changes, you resume rollouts for the\nDeployment. This approach allows you to\napply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.  \n* For example, with a Deployment that was created:  \nGet the Deployment details:\n```shell\nkubectl get deploy\n```\nThe output is similar to this:\n```\nNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx     3         3         3            3           1m\n```\nGet the rollout status:\n```shell\nkubectl get rs\n```\nThe output is similar to this:\n```\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   3         3         3         1m\n```  \n* Pause by running the following command:\n```shell\nkubectl rollout pause deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment paused\n```  \n* Then update the image of the Deployment:\n```shell\nkubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment image updated\n```  \n* Notice that no new rollout started:\n```shell\nkubectl rollout history deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\ndeployments \"nginx\"\nREVISION  CHANGE-CAUSE\n1   <none>\n```\n* Get the rollout status to verify that the existing ReplicaSet has not changed:\n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   3         3         3         2m\n```  \n* You can make as many updates as you wish, for example, update the resources that will be used:\n```shell\nkubectl set resources deployment/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment resource requirements updated\n```  \nThe initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to\nthe Deployment will not have any effect as long as the Deployment rollout is paused.  \n* Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:\n```shell\nkubectl rollout resume deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment resumed\n```\n* {{< glossary_tooltip text=\"Watch\" term_id=\"watch\" >}} the status of the rollout until it's done.\n```shell\nkubectl get rs --watch\n```  \nThe output is similar to this:\n```\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   2         2         2         2m\nnginx-3926361531   2         2         0         6s\nnginx-3926361531   2         2         1         18s\nnginx-2142116321   1         2         2         2m\nnginx-2142116321   1         2         2         2m\nnginx-3926361531   3         2         1         18s\nnginx-3926361531   3         2         1         18s\nnginx-2142116321   1         1         1         2m\nnginx-3926361531   3         3         1         18s\nnginx-3926361531   3         3         2         19s\nnginx-2142116321   0         1         1         2m\nnginx-2142116321   0         1         1         2m\nnginx-2142116321   0         0         0         2m\nnginx-3926361531   3         3         3         20s\n```\n* Get the status of the latest rollout:\n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   0         0         0         2m\nnginx-3926361531   3         3         3         28s\n```\n{{< note >}}\nYou cannot rollback a paused Deployment until you resume it.\n{{< /note >}}\n"
  },
  {
    "question": "How do I set the revision history limit for a Deployment?",
    "answer": ".spec.revisionHistoryLimit is the optional field you can set to specify the number of old ReplicaSets to retain.",
    "uuid": "78cfccb2-ac41-418e-8478-5152faccf9ad",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set the revision history limit for a Deployment?\nThis is about the following runbook:\nRunbook Title: Revision History Limit\nRunbook Content: Writing a Deployment SpecRevision History LimitA Deployment's revision history is stored in the ReplicaSets it controls.  \n`.spec.revisionHistoryLimit` is an optional field that specifies the number of old ReplicaSets to retain\nto allow rollback. These old ReplicaSets consume resources in `etcd` and crowd the output of `kubectl get rs`. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.  \nMore specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.\nIn this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.\n"
  },
  {
    "question": "What are the main differences between ephemeral containers and regular containers?",
    "answer": "Ephemeral containers lack guarantees for resources or execution, will never be automatically restarted, and many fields in their `ContainerSpec` are incompatible and disallowed compared to regular containers.",
    "uuid": "c230d4f9-10c4-4fab-85d1-5f145bf80b9b",
    "question_with_context": "A user asked the following question:\nQuestion: What are the main differences between ephemeral containers and regular containers?\nThis is about the following runbook:\nRunbook Title: What is an ephemeral container?\nRunbook Content: Understanding ephemeral containersWhat is an ephemeral container?Ephemeral containers differ from other containers in that they lack guarantees\nfor resources or execution, and they will never be automatically restarted, so\nthey are not appropriate for building applications.  Ephemeral containers are\ndescribed using the same `ContainerSpec` as regular containers, but many fields\nare incompatible and disallowed for ephemeral containers.  \n- Ephemeral containers may not have ports, so fields such as `ports`,\n`livenessProbe`, `readinessProbe` are disallowed.\n- Pod resource allocations are immutable, so setting `resources` is disallowed.\n- For a complete list of allowed fields, see the [EphemeralContainer reference\ndocumentation](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#ephemeralcontainer-v1-core).  \nEphemeral containers are created using a special `ephemeralcontainers` handler\nin the API rather than by adding them directly to `pod.spec`, so it's not\npossible to add an ephemeral container using `kubectl edit`.  \nLike regular containers, you may not change or remove an ephemeral container\nafter you have added it to a Pod.  \n{{< note >}}\nEphemeral containers are not supported by [static pods](/docs/tasks/configure-pod-container/static-pod/).\n{{< /note >}}\n"
  },
  {
    "question": "How can I learn about the lifecycle of a Pod in Kubernetes?",
    "answer": "You can learn about the lifecycle of a Pod by visiting the [lifecycle of a Pod](/docs/concepts/workloads/pods/pod-lifecycle/) documentation.",
    "uuid": "3aaea08e-fa59-48c7-bdc9-d009a973c467",
    "question_with_context": "A user asked the following question:\nQuestion: How can I learn about the lifecycle of a Pod in Kubernetes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about the [lifecycle of a Pod](/docs/concepts/workloads/pods/pod-lifecycle/).\n* Learn about [RuntimeClass](/docs/concepts/containers/runtime-class/) and how you can use it to\nconfigure different Pods with different container runtime configurations.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how you can use it to manage application availability during disruptions.\n* Pod is a top-level resource in the Kubernetes REST API.\nThe {{< api-reference page=\"workload-resources/pod-v1\" >}}\nobject definition describes the object in detail.\n* [The Distributed System Toolkit: Patterns for Composite Containers](/blog/2015/06/the-distributed-system-toolkit-patterns/) explains common layouts for Pods with more than one container.\n* Read about [Pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/)  \nTo understand the context for why Kubernetes wraps a common Pod API in other resources (such as {{< glossary_tooltip text=\"StatefulSets\" term_id=\"statefulset\" >}} or {{< glossary_tooltip text=\"Deployments\" term_id=\"deployment\" >}}), you can read about the prior art, including:  \n* [Aurora](https://aurora.apache.org/documentation/latest/reference/configuration/#job-schema)\n* [Borg](https://research.google.com/pubs/pub43438.html)\n* [Marathon](https://github.com/d2iq-archive/marathon)\n* [Omega](https://research.google/pubs/pub41684/)\n* [Tupperware](https://engineering.fb.com/data-center-engineering/tupperware/).\n"
  },
  {
    "question": "What is the restart policy I can set in a Pod Template?",
    "answer": "The only allowed value for the restart policy in `.spec.template.spec.restartPolicy` is `Always`, which is also the default.",
    "uuid": "f5fef35e-c8c9-4faf-91b0-30b8c16870df",
    "question_with_context": "A user asked the following question:\nQuestion: What is the restart policy I can set in a Pod Template?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a ReplicaSet manifestPod TemplateThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates) which is also\nrequired to have labels in place. In our `frontend.yaml` example we had one label: `tier: frontend`.\nBe careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.  \nFor the template's [restart policy](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) field,\n`.spec.template.spec.restartPolicy`, the only allowed value is `Always`, which is the default.\n"
  },
  {
    "question": "How can I force delete a Pod immediately using kubectl?",
    "answer": "To force delete a Pod immediately, use the command `kubectl delete pod <pod-name> --grace-period=0 --force`. This sets the grace period to 0, which forcibly and immediately deletes the Pod from the API server.",
    "uuid": "ef9926ff-1785-4fce-ad48-0e54d5164076",
    "question_with_context": "A user asked the following question:\nQuestion: How can I force delete a Pod immediately using kubectl?\nThis is about the following runbook:\nRunbook Title: Forced Pod termination {#pod-termination-forced}\nRunbook Content: Termination of Pods {#pod-termination}Forced Pod termination {#pod-termination-forced}{{< caution >}}\nForced deletions can be potentially disruptive for some workloads and their Pods.\n{{< /caution >}}  \nBy default, all deletes are graceful within 30 seconds. The `kubectl delete` command supports\nthe `--grace-period=<seconds>` option which allows you to override the default and specify your\nown value.  \nSetting the grace period to `0` forcibly and immediately deletes the Pod from the API\nserver. If the Pod was still running on a node, that forcible deletion triggers the kubelet to\nbegin immediate cleanup.  \nUsing kubectl, You must specify an additional flag `--force` along with `--grace-period=0`\nin order to perform force deletions.  \nWhen a force deletion is performed, the API server does not wait for confirmation\nfrom the kubelet that the Pod has been terminated on the node it was running on. It\nremoves the Pod in the API immediately so a new Pod can be created with the same\nname. On the node, Pods that are set to terminate immediately will still be given\na small grace period before being force killed.  \n{{< caution >}}\nImmediate deletion does not wait for confirmation that the running resource has been terminated.\nThe resource may continue to run on the cluster indefinitely.\n{{< /caution >}}  \nIf you need to force-delete Pods that are part of a StatefulSet, refer to the task\ndocumentation for\n[deleting Pods from a StatefulSet](/docs/tasks/run-application/force-delete-stateful-set-pod/).\n"
  },
  {
    "question": "What command can I use to check the current ReplicaSets deployed in my Kubernetes cluster?",
    "answer": "You can check the current ReplicaSets deployed by using the command: `kubectl get rs`.",
    "uuid": "777a6e1f-269c-4a4b-bb11-8314466689a7",
    "question_with_context": "A user asked the following question:\nQuestion: What command can I use to check the current ReplicaSets deployed in my Kubernetes cluster?\nThis is about the following runbook:\nRunbook Title: Example\nRunbook Content: Example{{% code_sample file=\"controllers/frontend.yaml\" %}}  \nSaving this manifest into `frontend.yaml` and submitting it to a Kubernetes cluster will\ncreate the defined ReplicaSet and the Pods that it manages.  \n```shell\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\n```  \nYou can then get the current ReplicaSets deployed:  \n```shell\nkubectl get rs\n```  \nAnd see the frontend one you created:  \n```\nNAME       DESIRED   CURRENT   READY   AGE\nfrontend   3         3         3       6s\n```  \nYou can also check on the state of the ReplicaSet:  \n```shell\nkubectl describe rs/frontend\n```  \nAnd you will see output similar to:  \n```\nName:         frontend\nNamespace:    default\nSelector:     tier=frontend\nLabels:       app=guestbook\ntier=frontend\nAnnotations:  <none>\nReplicas:     3 current / 3 desired\nPods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\nLabels:  tier=frontend\nContainers:\nphp-redis:\nImage:        us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5\nPort:         <none>\nHost Port:    <none>\nEnvironment:  <none>\nMounts:       <none>\nVolumes:        <none>\nEvents:\nType    Reason            Age   From                   Message\n----    ------            ----  ----                   -------\nNormal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-gbgfx\nNormal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-rwz57\nNormal  SuccessfulCreate  13s   replicaset-controller  Created pod: frontend-wkl7w\n```  \nAnd lastly you can check for the Pods brought up:  \n```shell\nkubectl get pods\n```  \nYou should see Pod information similar to:  \n```\nNAME             READY   STATUS    RESTARTS   AGE\nfrontend-gbgfx   1/1     Running   0          10m\nfrontend-rwz57   1/1     Running   0          10m\nfrontend-wkl7w   1/1     Running   0          10m\n```  \nYou can also verify that the owner reference of these pods is set to the frontend ReplicaSet.\nTo do this, get the yaml of one of the Pods running:  \n```shell\nkubectl get pods frontend-gbgfx -o yaml\n```  \nThe output will look similar to this, with the frontend ReplicaSet's info set in the metadata's ownerReferences field:  \n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: \"2024-02-28T22:30:44Z\"\ngenerateName: frontend-\nlabels:\ntier: frontend\nname: frontend-gbgfx\nnamespace: default\nownerReferences:\n- apiVersion: apps/v1\nblockOwnerDeletion: true\ncontroller: true\nkind: ReplicaSet\nname: frontend\nuid: e129deca-f864-481b-bb16-b27abfd92292\n...\n```\n"
  },
  {
    "question": "How do I define a Job that uses a sidecar container in Kubernetes?",
    "answer": "You can define a Job that uses a sidecar container by using Kubernetes-style init containers in your Job specification.",
    "uuid": "cea747b2-14bf-46e4-8b81-84de7311f981",
    "question_with_context": "A user asked the following question:\nQuestion: How do I define a Job that uses a sidecar container in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Jobs with sidecar containers\nRunbook Content: Sidecar containers and Pod lifecycleJobs with sidecar containersIf you define a Job that uses sidecar using Kubernetes-style init containers,\nthe sidecar container in each Pod does not prevent the Job from completing after the\nmain container has finished.  \nHere's an example of a Job with two containers, one of which is a sidecar:  \n{{% code_sample language=\"yaml\" file=\"application/job/job-sidecar.yaml\" %}}\n"
  },
  {
    "question": "Where can I find more information about adding a mechanism for job cleanup?",
    "answer": "Refer to the [Kubernetes Enhancement Proposal](https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md) for details on adding this mechanism.",
    "uuid": "43f2b79a-07ba-4f39-b38d-30a721c0ba62",
    "question_with_context": "A user asked the following question:\nQuestion: Where can I find more information about adding a mechanism for job cleanup?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Read [Clean up Jobs automatically](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically)  \n* Refer to the [Kubernetes Enhancement Proposal](https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md)\n(KEP) for adding this mechanism.\n"
  },
  {
    "question": "How do I create a Pod with an init container?",
    "answer": "You can learn how to create a Pod that has an init container by following the guide on [Creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).",
    "uuid": "0c12ec01-ca4f-4a8e-882d-aa50385118bd",
    "question_with_context": "A user asked the following question:\nQuestion: How do I create a Pod with an init container?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}Learn more about the following:\n* [Creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).\n* [Debug init containers](/docs/tasks/debug/debug-application/debug-init-containers/).\n* Overview of [kubelet](/docs/reference/command-line-tools-reference/kubelet/) and [kubectl](/docs/reference/kubectl/).\n* [Types of probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe): liveness, readiness, startup probe.\n* [Sidecar containers](/docs/concepts/workloads/pods/sidecar-containers).\n"
  },
  {
    "question": "What should I do if a DaemonSet rollout doesn't work as expected?",
    "answer": "If a DaemonSet rollout doesn't work as expected, you can perform a rollback on the DaemonSet by following the instructions in the [rollback documentation](/docs/tasks/manage-daemon/rollback-daemon-set/).",
    "uuid": "78872c54-c791-4133-a1cf-4eaaee6d6b91",
    "question_with_context": "A user asked the following question:\nQuestion: What should I do if a DaemonSet rollout doesn't work as expected?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Learn about [static Pods](#static-pods), which are useful for running Kubernetes\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} components.\n* Find out how to use DaemonSets\n* [Perform a rolling update on a DaemonSet](/docs/tasks/manage-daemon/update-daemon-set/)\n* [Perform a rollback on a DaemonSet](/docs/tasks/manage-daemon/rollback-daemon-set/)\n(for example, if a roll out didn't work how you expected).\n* Understand [how Kubernetes assigns Pods to Nodes](/docs/concepts/scheduling-eviction/assign-pod-node/).\n* Learn about [device plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) and\n[add ons](/docs/concepts/cluster-administration/addons/), which often run as DaemonSets.\n* `DaemonSet` is a top-level resource in the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/daemon-set-v1\" >}}\nobject definition to understand the API for daemon sets.\n"
  },
  {
    "question": "How do I set up a Pod Selector in a ReplicationController?",
    "answer": "You set up a Pod Selector in a ReplicationController by defining the `.spec.selector` field with a label selector that matches the labels of the pods you want the ReplicationController to manage.",
    "uuid": "dde9ed5c-ca0f-432d-86b0-d04044890f61",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set up a Pod Selector in a ReplicationController?\nThis is about the following runbook:\nRunbook Title: Pod Selector\nRunbook Content: Writing a ReplicationController ManifestPod SelectorThe `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/#label-selectors). A ReplicationController\nmanages all the pods with labels that match the selector. It does not distinguish\nbetween pods that it created or deleted and pods that another person or process created or\ndeleted. This allows the ReplicationController to be replaced without affecting the running pods.  \nIf specified, the `.spec.template.metadata.labels` must be equal to the `.spec.selector`, or it will\nbe rejected by the API.  If `.spec.selector` is unspecified, it will be defaulted to\n`.spec.template.metadata.labels`.  \nAlso you should not normally create any pods whose labels match this selector, either directly, with\nanother ReplicationController, or with another controller such as Job. If you do so, the\nReplicationController thinks that it created the other pods.  Kubernetes does not stop you\nfrom doing this.  \nIf you do end up with multiple controllers that have overlapping selectors, you\nwill have to manage the deletion yourself (see [below](#working-with-replicationcontrollers)).\n"
  },
  {
    "question": "What\u2019s an example of using a single Job to manage Pods?",
    "answer": "An example is a Job that starts a Pod running a script to initiate a Spark master controller, which then runs a Spark driver and handles cleanup.",
    "uuid": "d4bb2cba-8fac-4ec4-b008-19bbb332677c",
    "question_with_context": "A user asked the following question:\nQuestion: What\u2019s an example of using a single Job to manage Pods?\nThis is about the following runbook:\nRunbook Title: Single Job starts controller Pod\nRunbook Content: AlternativesSingle Job starts controller PodAnother pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort\nof custom controller for those Pods. This allows the most flexibility, but may be somewhat\ncomplicated to get started with and offers less integration with Kubernetes.  \nOne example of this pattern would be a Job which starts a Pod which runs a script that in turn\nstarts a Spark master controller (see [spark example](https://github.com/kubernetes/examples/tree/master/staging/spark/README.md)),\nruns a spark driver, and then cleans up.  \nAn advantage of this approach is that the overall process gets the completion guarantee of a Job\nobject, but maintains complete control over what Pods are created and how work is assigned to them.\n"
  },
  {
    "question": "How do I create a Pod in Kubernetes?",
    "answer": "You will rarely create individual Pods directly in Kubernetes, as they are designed to be ephemeral and disposable. Instead, Pods are usually created indirectly by a controller.",
    "uuid": "982771bb-a347-4ca0-9eca-9fae80b51c11",
    "question_with_context": "A user asked the following question:\nQuestion: How do I create a Pod in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Working with Pods\nRunbook Content: Working with PodsYou'll rarely create individual Pods directly in Kubernetes\u2014even singleton Pods. This\nis because Pods are designed as relatively ephemeral, disposable entities. When\na Pod gets created (directly by you, or indirectly by a\n{{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}), the new Pod is\nscheduled to run on a {{< glossary_tooltip term_id=\"node\" >}} in your cluster.\nThe Pod remains on that node until the Pod finishes execution, the Pod object is deleted,\nthe Pod is *evicted* for lack of resources, or the node fails.  \n{{< note >}}\nRestarting a container in a Pod should not be confused with restarting a Pod. A Pod\nis not a process, but an environment for running container(s). A Pod persists until\nit is deleted.\n{{< /note >}}  \nThe name of a Pod must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostname.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\n"
  },
  {
    "question": "Why are Deployments recommended over ReplicationControllers?",
    "answer": "Deployments are recommended because they are declarative, server-side, and have additional features.",
    "uuid": "e26c811e-7890-41ce-a5d1-b3cb50c3171d",
    "question_with_context": "A user asked the following question:\nQuestion: Why are Deployments recommended over ReplicationControllers?\nThis is about the following runbook:\nRunbook Title: Deployment (Recommended)\nRunbook Content: Alternatives to ReplicationControllerDeployment (Recommended)[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is a higher-level API object that updates its underlying Replica Sets and their Pods. Deployments are recommended if you want the rolling update functionality,  because they are declarative, server-side, and have additional features.\n"
  },
  {
    "question": "Can multiple CronJobs run their Jobs concurrently?",
    "answer": "Yes, multiple CronJobs can run their respective Jobs concurrently, regardless of the concurrency policies set for each individual CronJob.",
    "uuid": "a72fe435-f306-46a4-9cec-d746caf5aa0a",
    "question_with_context": "A user asked the following question:\nQuestion: Can multiple CronJobs run their Jobs concurrently?\nThis is about the following runbook:\nRunbook Title: Concurrency policy\nRunbook Content: Writing a CronJob specConcurrency policyThe `.spec.concurrencyPolicy` field is also optional.\nIt specifies how to treat concurrent executions of a Job that is created by this CronJob.\nThe spec may specify only one of the following concurrency policies:  \n* `Allow` (default): The CronJob allows concurrently running Jobs\n* `Forbid`: The CronJob does not allow concurrent runs; if it is time for a new Job run and the\nprevious Job run hasn't finished yet, the CronJob skips the new Job run. Also note that when the\nprevious Job run finishes, `.spec.startingDeadlineSeconds` is still taken into account and may\nresult in a new Job run.\n* `Replace`: If it is time for a new Job run and the previous Job run hasn't finished yet, the\nCronJob replaces the currently running Job run with a new Job run  \nNote that concurrency policy only applies to the Jobs created by the same CronJob.\nIf there are multiple CronJobs, their respective Jobs are always allowed to run concurrently.\n"
  },
  {
    "question": "How can I manage resources for my Pods and Containers?",
    "answer": "You can learn about resource management for Pods and Containers by visiting the resource management documentation.",
    "uuid": "7af14a53-79ed-4809-960d-f7117c037583",
    "question_with_context": "A user asked the following question:\nQuestion: How can I manage resources for my Pods and Containers?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [resource management for Pods and Containers](/docs/concepts/configuration/manage-resources-containers/).\n* Learn about [Node-pressure eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\n* Learn about [Pod priority and preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/).\n* Learn about [Pod disruptions](/docs/concepts/workloads/pods/disruptions/).\n* Learn how to [assign memory resources to containers and pods](/docs/tasks/configure-pod-container/assign-memory-resource/).\n* Learn how to [assign CPU resources to containers and pods](/docs/tasks/configure-pod-container/assign-cpu-resource/).\n* Learn how to [configure Quality of Service for Pods](/docs/tasks/configure-pod-container/quality-service-pod/).\n"
  },
  {
    "question": "How does Kubernetes handle application changes during a deployment?",
    "answer": "Kubernetes progressively rolls out changes to your application or its configuration while monitoring application health to ensure it doesn't kill all your instances at the same time. If something goes wrong, Kubernetes will rollback the change for you.",
    "uuid": "848b2682-fd01-48b0-acd4-4fedd2d6bb96",
    "question_with_context": "A user asked the following question:\nQuestion: How does Kubernetes handle application changes during a deployment?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- janetkuo\ntitle: Deployments\napi_metadata:\n- apiVersion: \"apps/v1\"\nkind: \"Deployment\"\nfeature:\ntitle: Automated rollouts and rollbacks\ndescription: >\nKubernetes progressively rolls out changes to your application or its configuration, while monitoring application health to ensure it doesn't kill all your instances at the same time. If something goes wrong, Kubernetes will rollback the change for you. Take advantage of a growing ecosystem of deployment solutions.\ndescription: >-\nA Deployment manages a set of Pods to run an application workload, usually one that doesn't maintain state.\ncontent_type: concept\nweight: 10\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nA _Deployment_ provides declarative updates for {{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}} and\n{{< glossary_tooltip term_id=\"replica-set\" text=\"ReplicaSets\" >}}.  \nYou describe a _desired state_ in a Deployment, and the Deployment {{< glossary_tooltip term_id=\"controller\" >}} changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.  \n{{< note >}}\nDo not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.\n{{< /note >}}  \n<!-- body -->\n"
  },
  {
    "question": "Can the identity of a StatefulSet Pod change if it moves to a different node?",
    "answer": "No, the identity remains the same even if the Pod is (re)scheduled on a different node.",
    "uuid": "4e7ec3e4-16f8-4a73-be68-962c085e83d2",
    "question_with_context": "A user asked the following question:\nQuestion: Can the identity of a StatefulSet Pod change if it moves to a different node?\nThis is about the following runbook:\nRunbook Title: Pod Identity\nRunbook Content: Pod IdentityStatefulSet Pods have a unique identity that consists of an ordinal, a\nstable network identity, and stable storage. The identity sticks to the Pod,\nregardless of which node it's (re)scheduled on.\n"
  },
  {
    "question": "How do I set the progress deadline seconds for my Deployment?",
    "answer": "You can set the progress deadline seconds by specifying the `.spec.progressDeadlineSeconds` field in your Deployment spec. This field is optional and defines the number of seconds to wait for your Deployment to progress.",
    "uuid": "1378b07a-4204-46a6-975f-2038e6cf7a93",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set the progress deadline seconds for my Deployment?\nThis is about the following runbook:\nRunbook Title: Progress Deadline Seconds\nRunbook Content: Writing a Deployment SpecProgress Deadline Seconds`.spec.progressDeadlineSeconds` is an optional field that specifies the number of seconds you want\nto wait for your Deployment to progress before the system reports back that the Deployment has\n[failed progressing](#failed-deployment) - surfaced as a condition with `type: Progressing`, `status: \"False\"`.\nand `reason: ProgressDeadlineExceeded` in the status of the resource. The Deployment controller will keep\nretrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment\ncontroller will roll back a Deployment as soon as it observes such a condition.  \nIf specified, this field needs to be greater than `.spec.minReadySeconds`.\n"
  },
  {
    "question": "Where can I find more information about container probes?",
    "answer": "You can read more about container probes in the Pod Lifecycle documentation.",
    "uuid": "e0e75efe-999e-4223-8685-43aa64af6fe9",
    "question_with_context": "A user asked the following question:\nQuestion: Where can I find more information about container probes?\nThis is about the following runbook:\nRunbook Title: Container probes\nRunbook Content: Container probesA _probe_ is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet can invoke different actions:  \n- `ExecAction` (performed with the help of the container runtime)\n- `TCPSocketAction` (checked directly by the kubelet)\n- `HTTPGetAction` (checked directly by the kubelet)  \nYou can read more about [probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)\nin the Pod Lifecycle documentation.\n"
  },
  {
    "question": "How do I specify a change cause when updating my deployment?",
    "answer": "You can specify a change cause when updating your deployment by annotating it with: `kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"image updated to 1.16.1\"` or by manually editing the manifest of the resource.",
    "uuid": "c8336b44-ba83-495d-a40f-70bec028eb5e",
    "question_with_context": "A user asked the following question:\nQuestion: How do I specify a change cause when updating my deployment?\nThis is about the following runbook:\nRunbook Title: Checking Rollout History of a Deployment\nRunbook Content: Rolling Back a DeploymentChecking Rollout History of a DeploymentFollow the steps given below to check the rollout history:  \n1. First, check the revisions of this Deployment:\n```shell\nkubectl rollout history deployment/nginx-deployment\n```\nThe output is similar to this:\n```\ndeployments \"nginx-deployment\"\nREVISION    CHANGE-CAUSE\n1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml\n2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n3           kubectl set image deployment/nginx-deployment nginx=nginx:1.161\n```  \n`CHANGE-CAUSE` is copied from the Deployment annotation `kubernetes.io/change-cause` to its revisions upon creation. You can specify the`CHANGE-CAUSE` message by:  \n* Annotating the Deployment with `kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"image updated to 1.16.1\"`\n* Manually editing the manifest of the resource.  \n2. To see the details of each revision, run:\n```shell\nkubectl rollout history deployment/nginx-deployment --revision=2\n```  \nThe output is similar to this:\n```\ndeployments \"nginx-deployment\" revision 2\nLabels:       app=nginx\npod-template-hash=1159050644\nAnnotations:  kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\nContainers:\nnginx:\nImage:      nginx:1.16.1\nPort:       80/TCP\nQoS Tier:\ncpu:      BestEffort\nmemory:   BestEffort\nEnvironment Variables:      <none>\nNo volumes.\n```\n"
  },
  {
    "question": "What happens if I don't set .spec.selector and .metadata.labels in my Deployment?",
    "answer": "In API version apps/v1, .spec.selector and .metadata.labels do not default to .spec.template.metadata.labels if not set, so they must be set explicitly.",
    "uuid": "72dcff4e-b7af-403e-b9c5-65f77814e35b",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I don't set .spec.selector and .metadata.labels in my Deployment?\nThis is about the following runbook:\nRunbook Title: Selector\nRunbook Content: Writing a Deployment SpecSelector`.spec.selector` is a required field that specifies a [label selector](/docs/concepts/overview/working-with-objects/labels/)\nfor the Pods targeted by this Deployment.  \n`.spec.selector` must match `.spec.template.metadata.labels`, or it will be rejected by the API.  \nIn API version `apps/v1`, `.spec.selector` and `.metadata.labels` do not default to `.spec.template.metadata.labels` if not set. So they must be set explicitly. Also note that `.spec.selector` is immutable after creation of the Deployment in `apps/v1`.  \nA Deployment may terminate Pods whose labels match the selector if their template is different\nfrom `.spec.template` or if the total number of such Pods exceeds `.spec.replicas`. It brings up new\nPods with `.spec.template` if the number of Pods is less than the desired number.  \n{{< note >}}\nYou should not create other Pods whose labels match this selector, either directly, by creating\nanother Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you\ndo so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.\n{{< /note >}}  \nIf you have multiple controllers that have overlapping selectors, the controllers will fight with each\nother and won't behave correctly.\n"
  },
  {
    "question": "What happens when the TTL for a finished Job expires?",
    "answer": "When the TTL expires, the Job becomes eligible for cascading removal, meaning it will be deleted along with its dependent objects.",
    "uuid": "dacd1f0c-3d62-41d8-b508-7033d1771c37",
    "question_with_context": "A user asked the following question:\nQuestion: What happens when the TTL for a finished Job expires?\nThis is about the following runbook:\nRunbook Title: Cleanup for finished Jobs\nRunbook Content: Cleanup for finished JobsThe TTL-after-finished controller is only supported for Jobs. You can use this mechanism to clean\nup finished Jobs (either `Complete` or `Failed`) automatically by specifying the\n`.spec.ttlSecondsAfterFinished` field of a Job, as in this\n[example](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically).  \nThe TTL-after-finished controller assumes that a Job is eligible to be cleaned up\nTTL seconds after the Job has finished. The timer starts once the\nstatus condition of the Job changes to show that the Job is either `Complete` or `Failed`; once the TTL has\nexpired, that Job becomes eligible for\n[cascading](/docs/concepts/architecture/garbage-collection/#cascading-deletion) removal. When the\nTTL-after-finished controller cleans up a job, it will delete it cascadingly, that is to say it will delete\nits dependent objects together with it.  \nKubernetes honors object lifecycle guarantees on the Job, such as waiting for\n[finalizers](/docs/concepts/overview/working-with-objects/finalizers/).  \nYou can set the TTL seconds at any time. Here are some examples for setting the\n`.spec.ttlSecondsAfterFinished` field of a Job:  \n* Specify this field in the Job manifest, so that a Job can be cleaned up\nautomatically some time after it finishes.\n* Manually set this field of existing, already finished Jobs, so that they become eligible\nfor cleanup.\n* Use a\n[mutating admission webhook](/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\nto set this field dynamically at Job creation time. Cluster administrators can\nuse this to enforce a TTL policy for finished jobs.\n* Use a\n[mutating admission webhook](/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\nto set this field dynamically after the Job has finished, and choose\ndifferent TTL values based on job status, labels. For this case, the webhook needs\nto detect changes to the `.status` of the Job and only set a TTL when the Job\nis being marked as completed.\n* Write your own controller to manage the cleanup TTL for Jobs that match a particular\n{{< glossary_tooltip term_id=\"selector\" text=\"selector\" >}}.\n"
  },
  {
    "question": "What fields do I need to include in a ReplicationController manifest?",
    "answer": "A ReplicationController manifest needs to include `apiVersion`, `kind`, and `metadata` fields.",
    "uuid": "851d57d3-2b7b-435b-8b2a-5e8a979ebe1f",
    "question_with_context": "A user asked the following question:\nQuestion: What fields do I need to include in a ReplicationController manifest?\nThis is about the following runbook:\nRunbook Title: Writing a ReplicationController Manifest\nRunbook Content: Writing a ReplicationController ManifestAs with all other Kubernetes config, a ReplicationController needs `apiVersion`, `kind`, and `metadata` fields.  \nWhen the control plane creates new Pods for a ReplicationController, the `.metadata.name` of the\nReplicationController is part of the basis for naming those Pods.  The name of a ReplicationController must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).  \nFor general information about working with configuration files, see [object management](/docs/concepts/overview/working-with-objects/object-management/).  \nA ReplicationController also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\n"
  },
  {
    "question": "What happens if I set the failed jobs history limit to 0?",
    "answer": "If you set the `.spec.failedJobsHistoryLimit` field to `0`, it will not keep any failed jobs.",
    "uuid": "089a44db-e919-49a4-840b-bf1b874c0089",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I set the failed jobs history limit to 0?\nThis is about the following runbook:\nRunbook Title: Jobs history limits\nRunbook Content: Writing a CronJob specJobs history limitsThe `.spec.successfulJobsHistoryLimit` and `.spec.failedJobsHistoryLimit` fields specify\nhow many completed and failed Jobs should be kept. Both fields are optional.  \n* `.spec.successfulJobsHistoryLimit`: This field specifies the number of successful finished\njobs to keep. The default value is `3`. Setting this field to `0` will not keep any successful jobs.  \n* `.spec.failedJobsHistoryLimit`: This field specifies the number of failed finished jobs to keep.\nThe default value is `1`. Setting this field to `0` will not keep any failed jobs.  \nFor another way to clean up Jobs automatically, see\n[Clean up finished Jobs automatically](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically).\n"
  },
  {
    "question": "When should I use a ReplicaSet instead of a Deployment?",
    "answer": "You should use a ReplicaSet directly only if you require custom update orchestration or don't require updates at all.",
    "uuid": "358c56ea-c719-4376-bb6f-ae0814a0c9e4",
    "question_with_context": "A user asked the following question:\nQuestion: When should I use a ReplicaSet instead of a Deployment?\nThis is about the following runbook:\nRunbook Title: ReplicaSet\nRunbook Content: Alternatives to ReplicationControllerReplicaSet[`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is the next-generation ReplicationController that supports the new [set-based label selector](/docs/concepts/overview/working-with-objects/labels/#set-based-requirement).\nIt's mainly used by [Deployment](/docs/concepts/workloads/controllers/deployment/) as a mechanism to orchestrate pod creation, deletion and updates.\nNote that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don't require updates at all.\n"
  },
  {
    "question": "Does changing the init container image trigger a Pod restart?",
    "answer": "No, changing the init container image does not trigger a Pod restart, nor does losing the init container completion record due to garbage collection.",
    "uuid": "e83fd9c9-edbf-4bb7-9f52-872dafae4721",
    "question_with_context": "A user asked the following question:\nQuestion: Does changing the init container image trigger a Pod restart?\nThis is about the following runbook:\nRunbook Title: Pod restart reasons\nRunbook Content: Detailed behaviorPod restart reasonsA Pod can restart, causing re-execution of init containers, for the following\nreasons:  \n* The Pod infrastructure container is restarted. This is uncommon and would\nhave to be done by someone with root access to nodes.\n* All containers in a Pod are terminated while `restartPolicy` is set to Always,\nforcing a restart, and the init container completion record has been lost due\nto {{< glossary_tooltip text=\"garbage collection\" term_id=\"garbage-collection\" >}}.  \nThe Pod will not be restarted when the init container image is changed, or the\ninit container completion record has been lost due to garbage collection. This\napplies for Kubernetes v1.20 and later. If you are using an earlier version of\nKubernetes, consult the documentation for the version you are using.\n"
  },
  {
    "question": "What happens to Burstable Pods during Node resource pressure?",
    "answer": "Burstable Pods are evicted only after all BestEffort Pods are evicted in the event of Node resource pressure.",
    "uuid": "18a14693-75e5-4ec0-a516-403a2e95c342",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to Burstable Pods during Node resource pressure?\nThis is about the following runbook:\nRunbook Title: Burstable\nRunbook Content: Quality of Service classesBurstablePods that are `Burstable` have some lower-bound resource guarantees based on the request, but\ndo not require a specific limit. If a limit is not specified, it defaults to a\nlimit equivalent to the capacity of the Node, which allows the Pods to flexibly increase\ntheir resources if resources are available. In the event of Pod eviction due to Node\nresource pressure, these Pods are evicted only after all `BestEffort` Pods are evicted.\nBecause a `Burstable` Pod can include a Container that has no resource limits or requests, a Pod\nthat is `Burstable` can try to use any amount of node resources.  \n#### Criteria  \nA Pod is given a QoS class of `Burstable` if:  \n* The Pod does not meet the criteria for QoS class `Guaranteed`.\n* At least one Container in the Pod has a memory or CPU request or limit.\n"
  },
  {
    "question": "How do I set a specific time zone for my CronJob in Kubernetes?",
    "answer": "You can specify a time zone for a CronJob by setting `.spec.timeZone` to the name of a valid time zone.",
    "uuid": "d75c492d-0e14-4cb0-b7d0-47797c58d264",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set a specific time zone for my CronJob in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Time zones\nRunbook Content: Writing a CronJob specTime zones{{< feature-state for_k8s_version=\"v1.27\" state=\"stable\" >}}  \nFor CronJobs with no time zone specified, the {{< glossary_tooltip term_id=\"kube-controller-manager\" text=\"kube-controller-manager\" >}}\ninterprets schedules relative to its local time zone.  \nYou can specify a time zone for a CronJob by setting `.spec.timeZone` to the name\nof a valid [time zone](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).\nFor example, setting `.spec.timeZone: \"Etc/UTC\"` instructs Kubernetes to interpret\nthe schedule relative to Coordinated Universal Time.  \nA time zone database from the Go standard library is included in the binaries and used as a fallback in case an external database is not available on the system.\n"
  },
  {
    "question": "What should I use if my Pods need a distinct identity and persistent storage?",
    "answer": "If your Pods need a distinct identity and persistent storage, you should use a StatefulSet. A StatefulSet associates each Pod with a PersistentVolume, ensuring that if one of the Pods fails, Kubernetes will create a replacement Pod connected to the same PersistentVolume.",
    "uuid": "07f756fa-8145-40e5-a51d-263eebd4db85",
    "question_with_context": "A user asked the following question:\nQuestion: What should I use if my Pods need a distinct identity and persistent storage?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: \"Workload Management\"\nweight: 20\nsimple_list: true\n---  \nKubernetes provides several built-in APIs for declarative management of your\n{{< glossary_tooltip text=\"workloads\" term_id=\"workload\" >}}\nand the components of those workloads.  \nUltimately, your applications run as containers inside\n{{< glossary_tooltip term_id=\"Pod\" text=\"Pods\" >}}; however, managing individual\nPods would be a lot of effort. For example, if a Pod fails, you probably want to\nrun a new Pod to replace it. Kubernetes can do that for you.  \nYou use the Kubernetes API to create a workload\n{{< glossary_tooltip text=\"object\" term_id=\"object\" >}} that represents a higher abstraction level\nthan a Pod, and then the Kubernetes\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} automatically manages\nPod objects on your behalf, based on the specification for the workload object you defined.  \nThe built-in APIs for managing workloads are:  \n[Deployment](/docs/concepts/workloads/controllers/deployment/) (and, indirectly, [ReplicaSet](/docs/concepts/workloads/controllers/replicaset/)),\nthe most common way to run an application on your cluster.\nDeployment is a good fit for managing a stateless application workload on your cluster, where\nany Pod in the Deployment is interchangeable and can be replaced if needed.\n(Deployments are a replacement for the legacy\n{{< glossary_tooltip text=\"ReplicationController\" term_id=\"replication-controller\" >}} API).  \nA [StatefulSet](/docs/concepts/workloads/controllers/statefulset/) lets you\nmanage one or more Pods \u2013 all running the same application code \u2013 where the Pods rely\non having a distinct identity. This is different from a Deployment where the Pods are\nexpected to be interchangeable.\nThe most common use for a StatefulSet is to be able to make a link between its Pods and\ntheir persistent storage. For example, you can run a StatefulSet that associates each Pod\nwith a [PersistentVolume](/docs/concepts/storage/persistent-volumes/). If one of the Pods\nin the StatefulSet fails, Kubernetes makes a replacement Pod that is connected to the\nsame PersistentVolume.  \nA [DaemonSet](/docs/concepts/workloads/controllers/daemonset/) defines Pods that provide\nfacilities that are local to a specific {{< glossary_tooltip text=\"node\" term_id=\"node\" >}};\nfor example, a driver that lets containers on that node access a storage system. You use a DaemonSet\nwhen the driver, or other node-level service, has to run on the node where it's useful.\nEach Pod in a DaemonSet performs a role similar to a system daemon on a classic Unix / POSIX\nserver.\nA DaemonSet might be fundamental to the operation of your cluster,\nsuch as a plugin to let that node access\n[cluster networking](/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-network-model),\nit might help you to manage the node,\nor it could provide less essential facilities that enhance the container platform you are running.\nYou can run DaemonSets (and their pods) across every node in your cluster, or across just a subset (for example,\nonly install the GPU accelerator driver on nodes that have a GPU installed).  \nYou can use a [Job](/docs/concepts/workloads/controllers/job/) and / or\na [CronJob](/docs/concepts/workloads/controllers/cron-jobs/) to\ndefine tasks that run to completion and then stop. A Job represents a one-off task,\nwhereas each CronJob repeats according to a schedule.  \nOther topics in this section:\n<!-- relies on simple_list: true in the front matter -->\n"
  },
  {
    "question": "How does a ReplicationController handle local container restarts?",
    "answer": "ReplicationControllers delegate local container restarts to an agent on the node, such as the Kubelet.",
    "uuid": "dcd5e71d-468b-41bb-a7df-933f79da1357",
    "question_with_context": "A user asked the following question:\nQuestion: How does a ReplicationController handle local container restarts?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a ReplicationController ManifestPod TemplateThe `.spec.template` is the only required field of the `.spec`.  \nThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}, except it is nested and does not have an `apiVersion` or `kind`.  \nIn addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate\nlabels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [pod selector](#pod-selector).  \nOnly a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is allowed, which is the default if not specified.  \nFor local container restarts, ReplicationControllers delegate to an agent on the node,\nfor example the [Kubelet](/docs/reference/command-line-tools-reference/kubelet/).\n"
  },
  {
    "question": "What should I do if I'm using distroless images and need to debug?",
    "answer": "Since distroless images lack a shell and debugging utilities, you can use ephemeral containers to help troubleshoot these images.",
    "uuid": "63f63340-1e9b-4798-a2f9-c4bc89519c5a",
    "question_with_context": "A user asked the following question:\nQuestion: What should I do if I'm using distroless images and need to debug?\nThis is about the following runbook:\nRunbook Title: Uses for ephemeral containers\nRunbook Content: Uses for ephemeral containersEphemeral containers are useful for interactive troubleshooting when `kubectl\nexec` is insufficient because a container has crashed or a container image\ndoesn't include debugging utilities.  \nIn particular, [distroless images](https://github.com/GoogleContainerTools/distroless)\nenable you to deploy minimal container images that reduce attack surface\nand exposure to bugs and vulnerabilities. Since distroless images do not include a\nshell or any debugging utilities, it's difficult to troubleshoot distroless\nimages using `kubectl exec` alone.  \nWhen using ephemeral containers, it's helpful to enable [process namespace\nsharing](/docs/tasks/configure-pod-container/share-process-namespace/) so\nyou can view processes in other containers.\n"
  },
  {
    "question": "What happens if I extend the TTL after it has already expired?",
    "answer": "If you extend the TTL period after the existing `ttlSecondsAfterFinished` period has expired, Kubernetes doesn't guarantee to retain that Job, even if the update returns a successful API response.",
    "uuid": "ccf5d38f-25cc-445c-9b6f-511202e0067d",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I extend the TTL after it has already expired?\nThis is about the following runbook:\nRunbook Title: Updating TTL for finished Jobs\nRunbook Content: CaveatsUpdating TTL for finished JobsYou can modify the TTL period, e.g. `.spec.ttlSecondsAfterFinished` field of Jobs,\nafter the job is created or has finished. If you extend the TTL period after the\nexisting `ttlSecondsAfterFinished` period has expired, Kubernetes doesn't guarantee\nto retain that Job, even if an update to extend the TTL returns a successful API\nresponse.\n"
  },
  {
    "question": "Who is this guide intended for besides application owners?",
    "answer": "The guide is also intended for cluster administrators who want to perform automated cluster actions, like upgrading and autoscaling clusters.",
    "uuid": "763c9c29-c8fc-4d23-b2aa-b81f5556eb17",
    "question_with_context": "A user asked the following question:\nQuestion: Who is this guide intended for besides application owners?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- erictune\n- foxish\n- davidopp\ntitle: Disruptions\ncontent_type: concept\nweight: 70\n---  \n<!-- overview -->\nThis guide is for application owners who want to build\nhighly available applications, and thus need to understand\nwhat types of disruptions can happen to Pods.  \nIt is also for cluster administrators who want to perform automated\ncluster actions, like upgrading and autoscaling clusters.  \n<!-- body -->\n"
  },
  {
    "question": "What happens when I choose 'Parallel' pod management for my StatefulSet?",
    "answer": "Choosing 'Parallel' pod management allows the StatefulSet controller to launch or terminate all Pods in parallel without waiting for them to become Running and Ready or completely terminated.",
    "uuid": "f4aedcdf-818f-4262-81fb-676de632b565",
    "question_with_context": "A user asked the following question:\nQuestion: What happens when I choose 'Parallel' pod management for my StatefulSet?\nThis is about the following runbook:\nRunbook Title: Pod Management Policies\nRunbook Content: Deployment and Scaling GuaranteesPod Management PoliciesStatefulSet allows you to relax its ordering guarantees while\npreserving its uniqueness and identity guarantees via its `.spec.podManagementPolicy` field.  \n#### OrderedReady Pod Management  \n`OrderedReady` pod management is the default for StatefulSets. It implements the behavior\ndescribed [above](#deployment-and-scaling-guarantees).  \n#### Parallel Pod Management  \n`Parallel` pod management tells the StatefulSet controller to launch or\nterminate all Pods in parallel, and to not wait for Pods to become Running\nand Ready or completely terminated prior to launching or terminating another\nPod. This option only affects the behavior for scaling operations. Updates are not\naffected.\n"
  },
  {
    "question": "How do I decide between using one Job object for each work item or a single Job object for all work items?",
    "answer": "Using one Job object for each work item creates overhead for managing large numbers of Job objects, while a single Job for all work items is better for handling large quantities.",
    "uuid": "fccc0d2d-0d8d-406a-abad-54cc2af67cdb",
    "question_with_context": "A user asked the following question:\nQuestion: How do I decide between using one Job object for each work item or a single Job object for all work items?\nThis is about the following runbook:\nRunbook Title: Job patterns\nRunbook Content: Job patternsThe Job object can be used to process a set of independent but related *work items*.\nThese might be emails to be sent, frames to be rendered, files to be transcoded,\nranges of keys in a NoSQL database to scan, and so on.  \nIn a complex system, there may be multiple different sets of work items. Here we are just\nconsidering one set of work items that the user wants to manage together &mdash; a *batch job*.  \nThere are several different patterns for parallel computation, each with strengths and weaknesses.\nThe tradeoffs are:  \n- One Job object for each work item, versus a single Job object for all work items.\nOne Job per work item creates some overhead for the user and for the system to manage\nlarge numbers of Job objects.\nA single Job for all work items is better for large numbers of items.\n- Number of Pods created equals number of work items, versus each Pod can process multiple work items.\nWhen the number of Pods equals the number of work items, the Pods typically\nrequires less modification to existing code and containers. Having each Pod\nprocess multiple work items is better for large numbers of items.\n- Several approaches use a work queue. This requires running a queue service,\nand modifications to the existing program or container to make it use the work queue.\nOther approaches are easier to adapt to an existing containerised application.\n- When the Job is associated with a\n[headless Service](/docs/concepts/services-networking/service/#headless-services),\nyou can enable the Pods within a Job to communicate with each other to\ncollaborate in a computation.  \nThe tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.\nThe pattern names are also links to examples and more detailed description.  \n|                  Pattern                        | Single Job object | Fewer pods than work items? | Use app unmodified? |\n| ----------------------------------------------- |:-----------------:|:---------------------------:|:-------------------:|\n| [Queue with Pod Per Work Item]                  |         \u2713         |                             |      sometimes      |\n| [Queue with Variable Pod Count]                 |         \u2713         |             \u2713               |                     |\n| [Indexed Job with Static Work Assignment]       |         \u2713         |                             |          \u2713          |\n| [Job with Pod-to-Pod Communication]             |         \u2713         |         sometimes           |      sometimes      |\n| [Job Template Expansion]                        |                   |                             |          \u2713          |  \nWhen you specify completions with `.spec.completions`, each Pod created by the Job controller\nhas an identical [`spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\nThis means that all pods for a task will have the same command line and the same\nimage, the same volumes, and (almost) the same environment variables. These patterns\nare different ways to arrange for pods to work on different things.  \nThis table shows the required settings for `.spec.parallelism` and `.spec.completions` for each of the patterns.\nHere, `W` is the number of work items.  \n|             Pattern                             | `.spec.completions` |  `.spec.parallelism` |\n| ----------------------------------------------- |:-------------------:|:--------------------:|\n| [Queue with Pod Per Work Item]                  |          W          |        any           |\n| [Queue with Variable Pod Count]                 |         null        |        any           |\n| [Indexed Job with Static Work Assignment]       |          W          |        any           |\n| [Job with Pod-to-Pod Communication]             |          W          |         W            |\n| [Job Template Expansion]                        |          1          |     should be 1      |  \n[Queue with Pod Per Work Item]: /docs/tasks/job/coarse-parallel-processing-work-queue/\n[Queue with Variable Pod Count]: /docs/tasks/job/fine-parallel-processing-work-queue/\n[Indexed Job with Static Work Assignment]: /docs/tasks/job/indexed-parallel-processing-static/\n[Job with Pod-to-Pod Communication]: /docs/tasks/job/job-with-pod-to-pod-communication/\n[Job Template Expansion]: /docs/tasks/job/parallel-processing-expansion/\n"
  },
  {
    "question": "How do I determine the effective resource limits for my init containers?",
    "answer": "The effective init request/limit for a resource is the highest of any particular resource request or limit defined on all init containers. If any resource has no limit specified, it is considered as the highest limit.",
    "uuid": "f167786c-a6f8-44bc-9953-436d2ee5d436",
    "question_with_context": "A user asked the following question:\nQuestion: How do I determine the effective resource limits for my init containers?\nThis is about the following runbook:\nRunbook Title: Resource sharing within containers\nRunbook Content: Detailed behaviorResource sharing within containersGiven the order of execution for init, sidecar and app containers, the following rules\nfor resource usage apply:  \n* The highest of any particular resource request or limit defined on all init\ncontainers is the *effective init request/limit*. If any resource has no\nresource limit specified this is considered as the highest limit.\n* The Pod's *effective request/limit* for a resource is the higher of:\n* the sum of all app containers request/limit for a resource\n* the effective init request/limit for a resource\n* Scheduling is done based on effective requests/limits, which means\ninit containers can reserve resources for initialization that are not used\nduring the life of the Pod.\n* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the\nQoS tier for init containers and app containers alike.  \nQuota and limits are applied based on the effective Pod request and\nlimit.\n"
  },
  {
    "question": "What happens if I don't set the .spec.ordinals.start field?",
    "answer": "If you don't set the `.spec.ordinals.start` field, it defaults to nil, meaning that no specific starting ordinal will be assigned to the Pods.",
    "uuid": "8d7b0a8d-77a1-4ba1-9b6b-0137ac3f5447",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I don't set the .spec.ordinals.start field?\nThis is about the following runbook:\nRunbook Title: Start ordinal\nRunbook Content: Pod IdentityStart ordinal{{< feature-state feature_gate_name=\"StatefulSetStartOrdinal\" >}}  \n`.spec.ordinals` is an optional field that allows you to configure the integer\nordinals assigned to each Pod. It defaults to nil. Within the field, you can\nconfigure the following options:  \n* `.spec.ordinals.start`: If the `.spec.ordinals.start` field is set, Pods will\nbe assigned ordinals from `.spec.ordinals.start` up through\n`.spec.ordinals.start + .spec.replicas - 1`.\n"
  },
  {
    "question": "What are sidecar containers used for in a Pod?",
    "answer": "Sidecar containers are used to enhance or extend the functionality of the primary app container by providing additional services such as logging, monitoring, security, or data synchronization.",
    "uuid": "5330b7c1-3b64-45be-a905-eb63e7d6f1a0",
    "question_with_context": "A user asked the following question:\nQuestion: What are sidecar containers used for in a Pod?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: Sidecar Containers\ncontent_type: concept\nweight: 50\n---  \n<!-- overview -->\n{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \nSidecar containers are the secondary containers that run along with the main\napplication container within the same {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}.\nThese containers are used to enhance or to extend the functionality of the primary _app\ncontainer_ by providing additional services, or functionality such as logging, monitoring,\nsecurity, or data synchronization, without directly altering the primary application code.  \nTypically, you only have one app container in a Pod. For example, if you have a web\napplication that requires a local webserver, the local webserver is a sidecar and the\nweb application itself is the app container.  \n<!-- body -->\n"
  },
  {
    "question": "What are the terminal states a Job can have in Kubernetes?",
    "answer": "A Job can have two terminal states: `Complete` (succeeded) and `Failed`.",
    "uuid": "7c22487c-7361-4574-b313-371d70083d04",
    "question_with_context": "A user asked the following question:\nQuestion: What are the terminal states a Job can have in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Terminal Job conditions\nRunbook Content: Job termination and cleanupTerminal Job conditionsA Job has two possible terminal states, each of which has a corresponding Job\ncondition:\n* Succeeded:  Job condition `Complete`\n* Failed: Job condition `Failed`  \nJobs fail for the following reasons:\n- The number of Pod failures exceeded the specified `.spec.backoffLimit` in the Job\nspecification. For details, see [Pod backoff failure policy](#pod-backoff-failure-policy).\n- The Job runtime exceeded the specified `.spec.activeDeadlineSeconds`\n- An indexed Job that used `.spec.backoffLimitPerIndex` has failed indexes.\nFor details, see [Backoff limit per index](#backoff-limit-per-index).\n- The number of failed indexes in the Job exceeded the specified\n`spec.maxFailedIndexes`. For details, see [Backoff limit per index](#backoff-limit-per-index)\n- A failed Pod matches a rule in `.spec.podFailurePolicy` that has the `FailJob`\naction. For details about how Pod failure policy rules might affect failure\nevaluation, see [Pod failure policy](#pod-failure-policy).  \nJobs succeed for the following reasons:\n- The number of succeeded Pods reached the specified `.spec.completions`\n- The criteria specified in `.spec.successPolicy` are met. For details, see\n[Success policy](#success-policy).  \nIn Kubernetes v1.31 and later the Job controller delays the addition of the\nterminal conditions,`Failed` or `Complete`, until all of the Job Pods are terminated.  \nIn Kubernetes v1.30 and earlier, the Job controller added the `Complete` or the\n`Failed` Job terminal conditions as soon as the Job termination process was\ntriggered and all Pod finalizers were removed. However, some Pods would still\nbe running or terminating at the moment that the terminal condition was added.  \nIn Kubernetes v1.31 and later, the controller only adds the Job terminal conditions\n_after_ all of the Pods are terminated. You can enable this behavior by using the\n`JobManagedBy` or the `JobPodReplacementPolicy` (enabled by default)\n[feature gates](/docs/reference/command-line-tools-reference/feature-gates/).\n"
  },
  {
    "question": "What can I update in a Job's pod template before it starts?",
    "answer": "Before a Job starts, you can update fields in its pod template such as node affinity, node selector, tolerations, labels, annotations, and scheduling gates.",
    "uuid": "acacbd25-d702-425a-b0fe-33bc46cd8447",
    "question_with_context": "A user asked the following question:\nQuestion: What can I update in a Job's pod template before it starts?\nThis is about the following runbook:\nRunbook Title: Mutable Scheduling Directives\nRunbook Content: Advanced usageMutable Scheduling Directives{{< feature-state for_k8s_version=\"v1.27\" state=\"stable\" >}}  \nIn most cases, a parallel job will want the pods to run with constraints,\nlike all in the same zone, or all either on GPU model x or y but not a mix of both.  \nThe [suspend](#suspending-a-job) field is the first step towards achieving those semantics. Suspend allows a\ncustom queue controller to decide when a job should start; However, once a job is unsuspended,\na custom queue controller has no influence on where the pods of a job will actually land.  \nThis feature allows updating a Job's scheduling directives before it starts, which gives custom queue\ncontrollers the ability to influence pod placement while at the same time offloading actual\npod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that have never\nbeen unsuspended before.  \nThe fields in a Job's pod template that can be updated are node affinity, node selector,\ntolerations, labels, annotations and [scheduling gates](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/).\n"
  },
  {
    "question": "Should I use ReplicationController or ReplicaSet for my application?",
    "answer": "You should prefer using ReplicaSets over ReplicationControllers due to their enhanced capabilities.",
    "uuid": "411176d4-ec1e-4705-92d3-510e86344c18",
    "question_with_context": "A user asked the following question:\nQuestion: Should I use ReplicationController or ReplicaSet for my application?\nThis is about the following runbook:\nRunbook Title: ReplicationController\nRunbook Content: Alternatives to ReplicaSetReplicationControllerReplicaSets are the successors to [ReplicationControllers](/docs/concepts/workloads/controllers/replicationcontroller/).\nThe two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-based\nselector requirements as described in the [labels user guide](/docs/concepts/overview/working-with-objects/labels/#label-selectors).\nAs such, ReplicaSets are preferred over ReplicationControllers\n"
  },
  {
    "question": "What happens if I don't clean up finished jobs?",
    "answer": "If finished jobs are not cleaned up, they will remain in the system and can cause increased pressure on the API server.",
    "uuid": "a1722372-ac16-4f51-8937-08c454f34040",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I don't clean up finished jobs?\nThis is about the following runbook:\nRunbook Title: Clean up finished jobs automatically\nRunbook Content: Clean up finished jobs automaticallyFinished Jobs are usually no longer needed in the system. Keeping them around in\nthe system will put pressure on the API server. If the Jobs are managed directly\nby a higher level controller, such as\n[CronJobs](/docs/concepts/workloads/controllers/cron-jobs/), the Jobs can be\ncleaned up by CronJobs based on the specified capacity-based cleanup policy.\n"
  },
  {
    "question": "What happens if Kubernetes can't find a condition in the Pod's status.conditions?",
    "answer": "If Kubernetes cannot find a condition in the `status.conditions` field of a Pod, the status of that condition is defaulted to 'False'.",
    "uuid": "80aa7fcf-1153-4512-888e-5ad9a9ee7e49",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if Kubernetes can't find a condition in the Pod's status.conditions?\nThis is about the following runbook:\nRunbook Title: Pod readiness {#pod-readiness-gate}\nRunbook Content: Pod conditionsPod readiness {#pod-readiness-gate}{{< feature-state for_k8s_version=\"v1.14\" state=\"stable\" >}}  \nYour application can inject extra feedback or signals into PodStatus:\n_Pod readiness_. To use this, set `readinessGates` in the Pod's `spec` to\nspecify a list of additional conditions that the kubelet evaluates for Pod readiness.  \nReadiness gates are determined by the current state of `status.condition`\nfields for the Pod. If Kubernetes cannot find such a condition in the\n`status.conditions` field of a Pod, the status of the condition\nis defaulted to \"`False`\".  \nHere is an example:  \n```yaml\nkind: Pod\n...\nspec:\nreadinessGates:\n- conditionType: \"www.example.com/feature-1\"\nstatus:\nconditions:\n- type: Ready                              # a built in PodCondition\nstatus: \"False\"\nlastProbeTime: null\nlastTransitionTime: 2018-01-01T00:00:00Z\n- type: \"www.example.com/feature-1\"        # an extra PodCondition\nstatus: \"False\"\nlastProbeTime: null\nlastTransitionTime: 2018-01-01T00:00:00Z\ncontainerStatuses:\n- containerID: docker://abcd...\nready: true\n...\n```  \nThe Pod conditions you add must have names that meet the Kubernetes\n[label key format](/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set).\n"
  },
  {
    "question": "When should I specify the `.spec.selector` field in a Job spec?",
    "answer": "In almost all cases, you should not specify the `.spec.selector` field.",
    "uuid": "b5cfac30-0d8a-4fbd-a4a6-f3e95e4f801b",
    "question_with_context": "A user asked the following question:\nQuestion: When should I specify the `.spec.selector` field in a Job spec?\nThis is about the following runbook:\nRunbook Title: Pod selector\nRunbook Content: Writing a Job specPod selectorThe `.spec.selector` field is optional. In almost all cases you should not specify it.\nSee section [specifying your own pod selector](#specifying-your-own-pod-selector).\n"
  },
  {
    "question": "Do I need to include a spec section in my DaemonSet configuration?",
    "answer": "Yes, a DaemonSet also needs a `.spec` section.",
    "uuid": "c8eedb18-162d-4815-a69f-ec4adc67d4da",
    "question_with_context": "A user asked the following question:\nQuestion: Do I need to include a spec section in my DaemonSet configuration?\nThis is about the following runbook:\nRunbook Title: Required Fields\nRunbook Content: Writing a DaemonSet SpecRequired FieldsAs with all other Kubernetes config, a DaemonSet needs `apiVersion`, `kind`, and `metadata` fields.  For\ngeneral information about working with config files, see\n[running stateless applications](/docs/tasks/run-application/run-stateless-application-deployment/)\nand [object management using kubectl](/docs/concepts/overview/working-with-objects/object-management/).  \nThe name of a DaemonSet object must be a valid\n[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).  \nA DaemonSet also needs a\n[`.spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)\nsection.\n"
  },
  {
    "question": "Can I set a restart policy for sidecar containers in Kubernetes?",
    "answer": "Yes, you can specify a `restartPolicy` for containers listed in a Pod's `initContainers` field, provided the `SidecarContainers` feature gate is enabled.",
    "uuid": "9a5aaef9-f68d-484f-8e63-618c55ebb58f",
    "question_with_context": "A user asked the following question:\nQuestion: Can I set a restart policy for sidecar containers in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Sidecar containers in Kubernetes {#pod-sidecar-containers}\nRunbook Content: Sidecar containers in Kubernetes {#pod-sidecar-containers}Kubernetes implements sidecar containers as a special case of\n[init containers](/docs/concepts/workloads/pods/init-containers/); sidecar containers remain\nrunning after Pod startup. This document uses the term _regular init containers_ to clearly\nrefer to containers that only run during Pod startup.  \nProvided that your cluster has the `SidecarContainers`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) enabled\n(the feature is active by default since Kubernetes v1.29), you can specify a `restartPolicy`\nfor containers listed in a Pod's `initContainers` field.\nThese restartable _sidecar_ containers are independent from other init containers and from\nthe main application container(s) within the same pod.\nThese can be started, stopped, or restarted without effecting the main application container\nand other init containers.  \nYou can also run a Pod with multiple containers that are not marked as init or sidecar\ncontainers. This is appropriate if the containers within the Pod are required for the\nPod to work overall, but you don't need to control which containers start or stop first.\nYou could also do this if you need to support older versions of Kubernetes that don't\nsupport a container-level `restartPolicy` field.\n"
  },
  {
    "question": "What happens to sidecar containers when the main application container stops?",
    "answer": "The kubelet postpones terminating sidecar containers until the main application container has fully stopped.",
    "uuid": "368d57e7-c672-4efa-baef-63b4d894534b",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to sidecar containers when the main application container stops?\nThis is about the following runbook:\nRunbook Title: Sidecar containers and Pod lifecycle\nRunbook Content: Sidecar containers and Pod lifecycleIf an init container is created with its `restartPolicy` set to `Always`, it will\nstart and remain running during the entire life of the Pod. This can be helpful for\nrunning supporting services separated from the main application containers.  \nIf a `readinessProbe` is specified for this init container, its result will be used\nto determine the `ready` state of the Pod.  \nSince these containers are defined as init containers, they benefit from the same\nordering and sequential guarantees as regular init containers, allowing you to mix\nsidecar containers with regular init containers for complex Pod initialization flows.  \nCompared to regular init containers, sidecars defined within `initContainers` continue to\nrun after they have started. This is important when there is more than one entry inside\n`.spec.initContainers` for a Pod. After a sidecar-style init container is running (the kubelet\nhas set the `started` status for that init container to true), the kubelet then starts the\nnext init container from the ordered `.spec.initContainers` list.\nThat status either becomes true because there is a process running in the\ncontainer and no startup probe defined, or as a result of its `startupProbe` succeeding.  \nUpon Pod [termination](/docs/concepts/workloads/pods/pod-lifecycle/#termination-with-sidecars),\nthe kubelet postpones terminating sidecar containers until the main application container has fully stopped.\nThe sidecar containers are then shut down in the opposite order of their appearance in the Pod specification.\nThis approach ensures that the sidecars remain operational, supporting other containers within the Pod,\nuntil their service is no longer required.\n"
  },
  {
    "question": "How should my application handle cases when it is restarted in a new Pod?",
    "answer": "Your application needs to handle the case when it is restarted in a new Pod by managing temporary files, locks, incomplete output, and similar issues caused by previous runs.",
    "uuid": "9fd32aa6-b077-489a-9327-2b1a468478e8",
    "question_with_context": "A user asked the following question:\nQuestion: How should my application handle cases when it is restarted in a new Pod?\nThis is about the following runbook:\nRunbook Title: Handling Pod and container failures\nRunbook Content: Handling Pod and container failuresA container in a Pod may fail for a number of reasons, such as because the process in it exited with\na non-zero exit code, or the container was killed for exceeding a memory limit, etc. If this\nhappens, and the `.spec.template.spec.restartPolicy = \"OnFailure\"`, then the Pod stays\non the node, but the container is re-run. Therefore, your program needs to handle the case when it is\nrestarted locally, or else specify `.spec.template.spec.restartPolicy = \"Never\"`.\nSee [pod lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/#example-states) for more information on `restartPolicy`.  \nAn entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node\n(node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the\n`.spec.template.spec.restartPolicy = \"Never\"`. When a Pod fails, then the Job controller\nstarts a new Pod. This means that your application needs to handle the case when it is restarted in a new\npod. In particular, it needs to handle temporary files, locks, incomplete output and the like\ncaused by previous runs.  \nBy default, each pod failure is counted towards the `.spec.backoffLimit` limit,\nsee [pod backoff failure policy](#pod-backoff-failure-policy). However, you can\ncustomize handling of pod failures by setting the Job's [pod failure policy](#pod-failure-policy).  \nAdditionally, you can choose to count the pod failures independently for each\nindex of an [Indexed](#completion-mode) Job by setting the `.spec.backoffLimitPerIndex` field\n(for more information, see [backoff limit per index](#backoff-limit-per-index)).  \nNote that even if you specify `.spec.parallelism = 1` and `.spec.completions = 1` and\n`.spec.template.spec.restartPolicy = \"Never\"`, the same program may\nsometimes be started twice.  \nIf you do specify `.spec.parallelism` and `.spec.completions` both greater than 1, then there may be\nmultiple pods running at once. Therefore, your pods must also be tolerant of concurrency.  \nIf you specify the `.spec.podFailurePolicy` field, the Job controller does not consider a terminating\nPod (a pod that has a `.metadata.deletionTimestamp` field set) as a failure until that Pod is\nterminal (its `.status.phase` is `Failed` or `Succeeded`). However, the Job controller\ncreates a replacement Pod as soon as the termination becomes apparent. Once the\npod terminates, the Job controller evaluates `.backoffLimit` and `.podFailurePolicy`\nfor the relevant Job, taking this now-terminated Pod into consideration.  \nIf either of these requirements is not satisfied, the Job controller counts\na terminating Pod as an immediate failure, even if that Pod later terminates\nwith `phase: \"Succeeded\"`.\n"
  },
  {
    "question": "How does Kubernetes handle container faults within a Pod?",
    "answer": "While a Pod is running, the kubelet is able to restart containers to handle some kind of faults.",
    "uuid": "11910d67-4cf2-447c-8dc9-b8cfee385745",
    "question_with_context": "A user asked the following question:\nQuestion: How does Kubernetes handle container faults within a Pod?\nThis is about the following runbook:\nRunbook Title: Pod lifetime\nRunbook Content: Pod lifetimeWhilst a Pod is running, the kubelet is able to restart containers to handle some\nkind of faults. Within a Pod, Kubernetes tracks different container\n[states](#container-states) and determines what action to take to make the Pod\nhealthy again.  \nIn the Kubernetes API, Pods have both a specification and an actual status. The\nstatus for a Pod object consists of a set of [Pod conditions](#pod-conditions).\nYou can also inject [custom readiness information](#pod-readiness-gate) into the\ncondition data for a Pod, if that is useful to your application.  \nPods are only [scheduled](/docs/concepts/scheduling-eviction/) once in their lifetime;\nassigning a Pod to a specific node is called _binding_, and the process of selecting\nwhich node to use is called _scheduling_.\nOnce a Pod has been scheduled and is bound to a node, Kubernetes tries\nto run that Pod on the node. The Pod runs on that node until it stops, or until the Pod\nis [terminated](#pod-termination); if Kubernetes isn't able to start the Pod on the selected\nnode (for example, if the node crashes before the Pod starts), then that particular Pod\nnever starts.  \nYou can use [Pod Scheduling Readiness](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/)\nto delay scheduling for a Pod until all its _scheduling gates_ are removed. For example,\nyou might want to define a set of Pods but only trigger scheduling once all the Pods\nhave been created.\n"
  },
  {
    "question": "What happens if I don't specify the number of replicas in my ReplicationController?",
    "answer": "If you do not specify `.spec.replicas`, it defaults to 1.",
    "uuid": "3bf22c2a-7ed8-4ab0-9407-21cf9e46f3a7",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I don't specify the number of replicas in my ReplicationController?\nThis is about the following runbook:\nRunbook Title: Multiple Replicas\nRunbook Content: Writing a ReplicationController ManifestMultiple ReplicasYou can specify how many pods should run concurrently by setting `.spec.replicas` to the number\nof pods you would like to have running concurrently.  The number running at any time may be higher\nor lower, such as if the replicas were just increased or decreased, or if a pod is gracefully\nshutdown, and a replacement starts early.  \nIf you do not specify `.spec.replicas`, then it defaults to 1.\n"
  },
  {
    "question": "How do I set the number of desired Pods in a Deployment?",
    "answer": "You can set the number of desired Pods in a Deployment by specifying the `.spec.replicas` field in your Deployment spec. This field is optional and defaults to 1.",
    "uuid": "ecce3ac6-3320-4697-8171-28d4924fcd03",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set the number of desired Pods in a Deployment?\nThis is about the following runbook:\nRunbook Title: Replicas\nRunbook Content: Writing a Deployment SpecReplicas`.spec.replicas` is an optional field that specifies the number of desired Pods. It defaults to 1.  \nShould you manually scale a Deployment, example via `kubectl scale deployment\ndeployment --replicas=X`, and then you update that Deployment based on a manifest\n(for example: by running `kubectl apply -f deployment.yaml`),\nthen applying that manifest overwrites the manual scaling that you previously did.  \nIf a [HorizontalPodAutoscaler](/docs/tasks/run-application/horizontal-pod-autoscale/) (or any\nsimilar API for horizontal scaling) is managing scaling for a Deployment, don't set `.spec.replicas`.  \nInstead, allow the Kubernetes\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} to manage the\n`.spec.replicas` field automatically.\n"
  },
  {
    "question": "How do I specify init containers in a Pod?",
    "answer": "You can specify init containers in the Pod specification alongside the containers array, which describes app containers.",
    "uuid": "79fec00c-2e5c-42af-b625-214a7e775a38",
    "question_with_context": "A user asked the following question:\nQuestion: How do I specify init containers in a Pod?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- erictune\ntitle: Init Containers\ncontent_type: concept\nweight: 40\n---  \n<!-- overview -->\nThis page provides an overview of init containers: specialized containers that run\nbefore app containers in a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}.\nInit containers can contain utilities or setup scripts not present in an app image.  \nYou can specify init containers in the Pod specification alongside the `containers`\narray (which describes app containers).  \nIn Kubernetes, a [sidecar container](/docs/concepts/workloads/pods/sidecar-containers/) is a container that\nstarts before the main application container and _continues to run_. This document is about init containers:\ncontainers that run to completion during Pod initialization.  \n<!-- body -->\n"
  },
  {
    "question": "Does a preStop hook affect the Terminated state of a container?",
    "answer": "Yes, if a container has a preStop hook configured, this hook runs before the container enters the Terminated state.",
    "uuid": "069f7e19-e31a-478d-875b-af2597dbd283",
    "question_with_context": "A user asked the following question:\nQuestion: Does a preStop hook affect the Terminated state of a container?\nThis is about the following runbook:\nRunbook Title: `Terminated` {#container-state-terminated}\nRunbook Content: Container states`Terminated` {#container-state-terminated}A container in the `Terminated` state began execution and then either ran to\ncompletion or failed for some reason. When you use `kubectl` to query a Pod with\na container that is `Terminated`, you see a reason, an exit code, and the start and\nfinish time for that container's period of execution.  \nIf a container has a `preStop` hook configured, this hook runs before the container enters\nthe `Terminated` state.\n"
  },
  {
    "question": "How do user namespaces affect container privileges on the host?",
    "answer": "User namespaces isolate the users in the container from the users in the node, allowing containers to run as root while being mapped to a non-root user on the host. This means that even if a container thinks it is running as root, it doesn't have root privileges on the host.",
    "uuid": "4e589473-3f56-40c7-9dc2-7a4048076453",
    "question_with_context": "A user asked the following question:\nQuestion: How do user namespaces affect container privileges on the host?\nThis is about the following runbook:\nRunbook Title: Understanding user namespaces for pods {#pods-and-userns}\nRunbook Content: Understanding user namespaces for pods {#pods-and-userns}Several container runtimes with their default configuration (like Docker Engine,\ncontainerd, CRI-O) use Linux namespaces for isolation. Other technologies exist\nand can be used with those runtimes too (e.g. Kata Containers uses VMs instead of\nLinux namespaces). This page is applicable for container runtimes using Linux\nnamespaces for isolation.  \nWhen creating a pod, by default, several new namespaces are used for isolation:\na network namespace to isolate the network of the container, a PID namespace to\nisolate the view of processes, etc. If a user namespace is used, this will\nisolate the users in the container from the users in the node.  \nThis means containers can run as root and be mapped to a non-root user on the\nhost. Inside the container the process will think it is running as root (and\ntherefore tools like `apt`, `yum`, etc. work fine), while in reality the process\ndoesn't have privileges on the host. You can verify this, for example, if you\ncheck which user the container process is running by executing `ps aux` from\nthe host. The user `ps` shows is not the same as the user you see if you\nexecute inside the container the command `id`.  \nThis abstraction limits what can happen, for example, if the container manages\nto escape to the host. Given that the container is running as a non-privileged\nuser on the host, it is limited what it can do to the host.  \nFurthermore, as users on each pod will be mapped to different non-overlapping\nusers in the host, it is limited what they can do to other pods too.  \nCapabilities granted to a pod are also limited to the pod user namespace and\nmostly invalid out of it, some are even completely void. Here are two examples:\n- `CAP_SYS_MODULE` does not have any effect if granted to a pod using user\nnamespaces, the pod isn't able to load kernel modules.\n- `CAP_SYS_ADMIN` is limited to the pod's user namespace and invalid outside\nof it.  \nWithout using a user namespace a container running as root, in the case of a\ncontainer breakout, has root privileges on the node. And if some capability were\ngranted to the container, the capabilities are valid on the host too. None of\nthis is true when we use user namespaces.  \nIf you want to know more details about what changes when user namespaces are in\nuse, see `man 7 user_namespaces`.\n"
  },
  {
    "question": "What happens if I try to drain a node when only 2 pods are available?",
    "answer": "If you try to drain a node when there are only 2 available pods for a deployment that requires at least 2 pods to be available, the drain command will block. This is because evicting a pod would violate the PodDisruptionBudget.",
    "uuid": "0728de72-ccc8-44d0-a4da-83396979b51b",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I try to drain a node when only 2 pods are available?\nThis is about the following runbook:\nRunbook Title: PodDisruptionBudget example {#pdb-example}\nRunbook Content: PodDisruptionBudget example {#pdb-example}Consider a cluster with 3 nodes, `node-1` through `node-3`.\nThe cluster is running several applications.  One of them has 3 replicas initially called\n`pod-a`, `pod-b`, and `pod-c`.  Another, unrelated pod without a PDB, called `pod-x`, is also shown.\nInitially, the pods are laid out as follows:  \n|       node-1         |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *available*   | pod-b *available*   | pod-c *available*  |\n| pod-x  *available*   |                     |                    |  \nAll 3 pods are part of a deployment, and they collectively have a PDB which requires\nthere be at least 2 of the 3 pods to be available at all times.  \nFor example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.\nThe cluster administrator first tries to drain `node-1` using the `kubectl drain` command.\nThat tool tries to evict `pod-a` and `pod-x`.  This succeeds immediately.\nBoth pods go into the `terminating` state at the same time.\nThis puts the cluster in this state:  \n|   node-1 *draining*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |\n| pod-x  *terminating* |                     |                    |  \nThe deployment notices that one of the pods is terminating, so it creates a replacement\ncalled `pod-d`.  Since `node-1` is cordoned, it lands on another node.  Something has\nalso created `pod-y` as a replacement for `pod-x`.  \n(Note: for a StatefulSet, `pod-a`, which would be called something like `pod-0`, would need\nto terminate completely before its replacement, which is also called `pod-0` but has a\ndifferent UID, could be created.  Otherwise, the example applies to a StatefulSet as well.)  \nNow the cluster is in this state:  \n|   node-1 *draining*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |\n| pod-x  *terminating* | pod-d *starting*    | pod-y              |  \nAt some point, the pods terminate, and the cluster looks like this:  \n|    node-1 *drained*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n|                      | pod-b *available*   | pod-c *available*  |\n|                      | pod-d *starting*    | pod-y              |  \nAt this point, if an impatient cluster administrator tries to drain `node-2` or\n`node-3`, the drain command will block, because there are only 2 available\npods for the deployment, and its PDB requires at least 2.  After some time passes, `pod-d` becomes available.  \nThe cluster state now looks like this:  \n|    node-1 *drained*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n|                      | pod-b *available*   | pod-c *available*  |\n|                      | pod-d *available*   | pod-y              |  \nNow, the cluster administrator tries to drain `node-2`.\nThe drain command will try to evict the two pods in some order, say\n`pod-b` first and then `pod-d`.  It will succeed at evicting `pod-b`.\nBut, when it tries to evict `pod-d`, it will be refused because that would leave only\none pod available for the deployment.  \nThe deployment creates a replacement for `pod-b` called `pod-e`.\nBecause there are not enough resources in the cluster to schedule\n`pod-e` the drain will again block.  The cluster may end up in this\nstate:  \n|    node-1 *drained*  |       node-2        |       node-3       | *no node*          |\n|:--------------------:|:-------------------:|:------------------:|:------------------:|\n|                      | pod-b *terminating* | pod-c *available*  | pod-e *pending*    |\n|                      | pod-d *available*   | pod-y              |                    |  \nAt this point, the cluster administrator needs to\nadd a node back to the cluster to proceed with the upgrade.  \nYou can see how Kubernetes varies the rate at which disruptions\ncan happen, according to:  \n- how many replicas an application needs\n- how long it takes to gracefully shutdown an instance\n- how long it takes a new instance to start up\n- the type of controller\n- the cluster's resource capacity\n"
  },
  {
    "question": "How can I set custom conditions for Pod readiness?",
    "answer": "You can use a Kubernetes client library to write code that sets custom Pod conditions for Pod readiness.",
    "uuid": "514f835b-89c9-43d0-b173-ae64991608ed",
    "question_with_context": "A user asked the following question:\nQuestion: How can I set custom conditions for Pod readiness?\nThis is about the following runbook:\nRunbook Title: Status for Pod readiness {#pod-readiness-status}\nRunbook Content: Pod conditionsStatus for Pod readiness {#pod-readiness-status}The `kubectl patch` command does not support patching object status.\nTo set these `status.conditions` for the Pod, applications and\n{{< glossary_tooltip term_id=\"operator-pattern\" text=\"operators\">}} should use\nthe `PATCH` action.\nYou can use a [Kubernetes client library](/docs/reference/using-api/client-libraries/) to\nwrite code that sets custom Pod conditions for Pod readiness.  \nFor a Pod that uses custom conditions, that Pod is evaluated to be ready **only**\nwhen both the following statements apply:  \n* All containers in the Pod are ready.\n* All conditions specified in `readinessGates` are `True`.  \nWhen a Pod's containers are Ready but at least one custom condition is missing or\n`False`, the kubelet sets the Pod's [condition](#pod-conditions) to `ContainersReady`.\n"
  },
  {
    "question": "Is there a connection between Init containers and sidecar containers in terms of resource management?",
    "answer": "Yes, the behavior of Init containers is related to sidecar containers, as both utilize Pod level control groups (cgroups) for resource management.",
    "uuid": "0f4f5e09-e821-48f7-8fd6-1a167a74805e",
    "question_with_context": "A user asked the following question:\nQuestion: Is there a connection between Init containers and sidecar containers in terms of resource management?\nThis is about the following runbook:\nRunbook Title: Init containers and Linux cgroups {#cgroups}\nRunbook Content: Detailed behaviorInit containers and Linux cgroups {#cgroups}On Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod\nrequest and limit, the same as the scheduler.  \n{{< comment >}}\nThis section also present under [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/) page.\nIf you're editing this section, change both places.\n{{< /comment >}}\n"
  },
  {
    "question": "What are the essential fields required in a Deployment spec?",
    "answer": ".apiVersion, .kind, and .metadata fields are essential in a Deployment spec.",
    "uuid": "dee3bb7a-10d0-4d7f-9209-74c578904d5e",
    "question_with_context": "A user asked the following question:\nQuestion: What are the essential fields required in a Deployment spec?\nThis is about the following runbook:\nRunbook Title: Writing a Deployment Spec\nRunbook Content: Writing a Deployment SpecAs with all other Kubernetes configs, a Deployment needs `.apiVersion`, `.kind`, and `.metadata` fields.\nFor general information about working with config files, see\n[deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/),\nconfiguring containers, and [using kubectl to manage resources](/docs/concepts/overview/working-with-objects/object-management/) documents.  \nWhen the control plane creates new Pods for a Deployment, the `.metadata.name` of the\nDeployment is part of the basis for naming those Pods. The name of a Deployment must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).  \nA Deployment also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\n"
  },
  {
    "question": "What happens if I leave .spec.completions unset for a work queue Job?",
    "answer": "If you leave `.spec.completions` unset for a work queue Job, it will default to `.spec.parallelism`. The Pods must coordinate among themselves or with an external service to determine their tasks. Once any Pod from the Job terminates successfully, no new Pods are created, and the Job is completed when at least one Pod has succeeded and all Pods are terminated.",
    "uuid": "5f7bb8dd-10a3-435d-8622-46f6870b83f2",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I leave .spec.completions unset for a work queue Job?\nThis is about the following runbook:\nRunbook Title: Parallel execution for Jobs {#parallel-jobs}\nRunbook Content: Writing a Job specParallel execution for Jobs {#parallel-jobs}There are three main types of task suitable to run as a Job:  \n1. Non-parallel Jobs\n- normally, only one Pod is started, unless the Pod fails.\n- the Job is complete as soon as its Pod terminates successfully.\n1. Parallel Jobs with a *fixed completion count*:\n- specify a non-zero positive value for `.spec.completions`.\n- the Job represents the overall task, and is complete when there are `.spec.completions` successful Pods.\n- when using `.spec.completionMode=\"Indexed\"`, each Pod gets a different index in the range 0 to `.spec.completions-1`.\n1. Parallel Jobs with a *work queue*:\n- do not specify `.spec.completions`, default to `.spec.parallelism`.\n- the Pods must coordinate amongst themselves or an external service to determine\nwhat each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.\n- each Pod is independently capable of determining whether or not all its peers are done,\nand thus that the entire Job is done.\n- when _any_ Pod from the Job terminates with success, no new Pods are created.\n- once at least one Pod has terminated with success and all Pods are terminated,\nthen the Job is completed with success.\n- once any Pod has exited with success, no other Pod should still be doing any work\nfor this task or writing any output. They should all be in the process of exiting.  \nFor a _non-parallel_ Job, you can leave both `.spec.completions` and `.spec.parallelism` unset.\nWhen both are unset, both are defaulted to 1.  \nFor a _fixed completion count_ Job, you should set `.spec.completions` to the number of completions needed.\nYou can set `.spec.parallelism`, or leave it unset and it will default to 1.  \nFor a _work queue_ Job, you must leave `.spec.completions` unset, and set `.spec.parallelism` to\na non-negative integer.  \nFor more information about how to make use of the different types of job,\nsee the [job patterns](#job-patterns) section.  \n#### Controlling parallelism  \nThe requested parallelism (`.spec.parallelism`) can be set to any non-negative value.\nIf it is unspecified, it defaults to 1.\nIf it is specified as 0, then the Job is effectively paused until it is increased.  \nActual parallelism (number of pods running at any instant) may be more or less than requested\nparallelism, for a variety of reasons:  \n- For _fixed completion count_ Jobs, the actual number of pods running in parallel will not exceed the number of\nremaining completions. Higher values of `.spec.parallelism` are effectively ignored.\n- For _work queue_ Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete, however.\n- If the Job {{< glossary_tooltip term_id=\"controller\" >}} has not had time to react.\n- If the Job controller failed to create Pods for any reason (lack of `ResourceQuota`, lack of permission, etc.),\nthen there may be fewer pods than requested.\n- The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.\n- When a Pod is gracefully shut down, it takes time to stop.\n"
  },
  {
    "question": "How can I enable sidecar containers in my Kubernetes cluster?",
    "answer": "Ensure that the `SidecarContainers` feature gate is enabled, which is active by default since Kubernetes v1.29.",
    "uuid": "9a5aaef9-f68d-484f-8e63-618c55ebb58f",
    "question_with_context": "A user asked the following question:\nQuestion: How can I enable sidecar containers in my Kubernetes cluster?\nThis is about the following runbook:\nRunbook Title: Sidecar containers in Kubernetes {#pod-sidecar-containers}\nRunbook Content: Sidecar containers in Kubernetes {#pod-sidecar-containers}Kubernetes implements sidecar containers as a special case of\n[init containers](/docs/concepts/workloads/pods/init-containers/); sidecar containers remain\nrunning after Pod startup. This document uses the term _regular init containers_ to clearly\nrefer to containers that only run during Pod startup.  \nProvided that your cluster has the `SidecarContainers`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) enabled\n(the feature is active by default since Kubernetes v1.29), you can specify a `restartPolicy`\nfor containers listed in a Pod's `initContainers` field.\nThese restartable _sidecar_ containers are independent from other init containers and from\nthe main application container(s) within the same pod.\nThese can be started, stopped, or restarted without effecting the main application container\nand other init containers.  \nYou can also run a Pod with multiple containers that are not marked as init or sidecar\ncontainers. This is appropriate if the containers within the Pod are required for the\nPod to work overall, but you don't need to control which containers start or stop first.\nYou could also do this if you need to support older versions of Kubernetes that don't\nsupport a container-level `restartPolicy` field.\n"
  },
  {
    "question": "What do I need to ensure for dynamic provisioning with Volume Claim Templates?",
    "answer": "The StorageClass specified for the volume claim must be set up to use dynamic provisioning.",
    "uuid": "d8e5a620-bc12-4764-83a5-c62b0dbe00e5",
    "question_with_context": "A user asked the following question:\nQuestion: What do I need to ensure for dynamic provisioning with Volume Claim Templates?\nThis is about the following runbook:\nRunbook Title: Volume Claim Templates\nRunbook Content: ComponentsVolume Claim TemplatesYou can set the `.spec.volumeClaimTemplates` field to create a\n[PersistentVolumeClaim](/docs/concepts/storage/persistent-volumes/).\nThis will provide stable storage to the StatefulSet if either  \n* The StorageClass specified for the volume claim is set up to use [dynamic\nprovisioning](/docs/concepts/storage/dynamic-provisioning/), or\n* The cluster already contains a PersistentVolume with the correct StorageClass\nand sufficient available storage space.\n"
  },
  {
    "question": "What happens when I set the update strategy to RollingUpdate?",
    "answer": "When you set the update strategy to `RollingUpdate`, it implements automated, rolling updates for the Pods in your StatefulSet. This is the default update strategy.",
    "uuid": "7c5a40d2-5293-42d3-a2bf-d3524bdb7fd8",
    "question_with_context": "A user asked the following question:\nQuestion: What happens when I set the update strategy to RollingUpdate?\nThis is about the following runbook:\nRunbook Title: Update strategies\nRunbook Content: Update strategiesA StatefulSet's `.spec.updateStrategy` field allows you to configure\nand disable automated rolling updates for containers, labels, resource request/limits, and\nannotations for the Pods in a StatefulSet. There are two possible values:  \n`OnDelete`\n: When a StatefulSet's `.spec.updateStrategy.type` is set to `OnDelete`,\nthe StatefulSet controller will not automatically update the Pods in a\nStatefulSet. Users must manually delete Pods to cause the controller to\ncreate new Pods that reflect modifications made to a StatefulSet's `.spec.template`.  \n`RollingUpdate`\n: The `RollingUpdate` update strategy implements automated, rolling updates for the Pods in a\nStatefulSet. This is the default update strategy.\n"
  },
  {
    "question": "How does a CronJob work in Kubernetes?",
    "answer": "A CronJob creates Jobs on a repeating schedule, similar to a line in a crontab file on a Unix system, and runs a Job periodically based on a schedule written in Cron format.",
    "uuid": "7995e834-2b63-4c6f-b712-7883db9f1e94",
    "question_with_context": "A user asked the following question:\nQuestion: How does a CronJob work in Kubernetes?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- erictune\n- soltysh\n- janetkuo\ntitle: CronJob\napi_metadata:\n- apiVersion: \"batch/v1\"\nkind: \"CronJob\"\ncontent_type: concept\ndescription: >-\nA CronJob starts one-time Jobs on a repeating schedule.\nweight: 80\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \n{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}  \nA _CronJob_ creates {{< glossary_tooltip term_id=\"job\" text=\"Jobs\" >}} on a repeating schedule.  \nCronJob is meant for performing regular scheduled actions such as backups, report generation,\nand so on. One CronJob object is like one line of a _crontab_ (cron table) file on a\nUnix system. It runs a Job periodically on a given schedule, written in\n[Cron](https://en.wikipedia.org/wiki/Cron) format.  \nCronJobs have limitations and idiosyncrasies.\nFor example, in certain circumstances, a single CronJob can create multiple concurrent Jobs. See the [limitations](#cron-job-limitations) below.  \nWhen the control plane creates new Jobs and (indirectly) Pods for a CronJob, the `.metadata.name`\nof the CronJob is part of the basis for naming those Pods.  The name of a CronJob must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).\nEven when the name is a DNS subdomain, the name must be no longer than 52\ncharacters.  This is because the CronJob controller will automatically append\n11 characters to the name you provide and there is a constraint that the\nlength of a Job name is no more than 63 characters.  \n<!-- body -->\n"
  },
  {
    "question": "Where do I find the labels that need to match for the Pod Selector?",
    "answer": "The labels that need to match are found in the `.spec.template.metadata.labels` of the StatefulSet.",
    "uuid": "5ad2f4f1-9448-402f-8c0d-7bbb9c95db82",
    "question_with_context": "A user asked the following question:\nQuestion: Where do I find the labels that need to match for the Pod Selector?\nThis is about the following runbook:\nRunbook Title: Pod Selector\nRunbook Content: ComponentsPod SelectorYou must set the `.spec.selector` field of a StatefulSet to match the labels of its\n`.spec.template.metadata.labels`. Failing to specify a matching Pod Selector will result in a\nvalidation error during StatefulSet creation.\n"
  },
  {
    "question": "How does Kubernetes handle repeated crashes of a container?",
    "answer": "After the initial crash, Kubernetes applies an exponential backoff delay for subsequent restarts to prevent rapid, repeated attempts from overloading the system.",
    "uuid": "1c1fb0a0-20b3-42b2-a6ae-6e80cddd5a2b",
    "question_with_context": "A user asked the following question:\nQuestion: How does Kubernetes handle repeated crashes of a container?\nThis is about the following runbook:\nRunbook Title: How Pods handle problems with containers {#container-restarts}\nRunbook Content: How Pods handle problems with containers {#container-restarts}Kubernetes manages container failures within Pods using a [`restartPolicy`](#restart-policy) defined in the Pod `spec`. This policy determines how Kubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:  \n1. **Initial crash**: Kubernetes attempts an immediate restart based on the Pod `restartPolicy`.\n1. **Repeated crashes**: After the initial crash Kubernetes applies an exponential\nbackoff delay for subsequent restarts, described in [`restartPolicy`](#restart-policy).\nThis prevents rapid, repeated restart attempts from overloading the system.\n1. **CrashLoopBackOff state**: This indicates that the backoff delay mechanism is currently\nin effect for a given container that is in a crash loop, failing and restarting repeatedly.\n1. **Backoff reset**: If a container runs successfully for a certain duration\n(e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash\nas the first one.  \nIn practice, a `CrashLoopBackOff` is a condition or event that might be seen as output\nfrom the `kubectl` command, while describing or listing Pods, when a container in the Pod\nfails to start properly and then continually tries and fails in a loop.  \nIn other words, when a container enters the crash loop, Kubernetes applies the\nexponential backoff delay mentioned in the [Container restart policy](#restart-policy).\nThis mechanism prevents a faulty container from overwhelming the system with continuous\nfailed start attempts.  \nThe `CrashLoopBackOff` can be caused by issues like the following:  \n* Application errors that cause the container to exit.\n* Configuration errors, such as incorrect environment variables or missing\nconfiguration files.\n* Resource constraints, where the container might not have enough memory or CPU\nto start properly.\n* Health checks failing if the application doesn't start serving within the\nexpected time.\n* Container liveness probes or startup probes returning a `Failure` result\nas mentioned in the [probes section](#container-probes).  \nTo investigate the root cause of a `CrashLoopBackOff` issue, a user can:  \n1. **Check logs**: Use `kubectl logs <name-of-pod>` to check the logs of the container.\nThis is often the most direct way to diagnose the issue causing the crashes.\n1. **Inspect events**: Use `kubectl describe pod <name-of-pod>` to see events\nfor the Pod, which can provide hints about configuration or resource issues.\n1. **Review configuration**: Ensure that the Pod configuration, including\nenvironment variables and mounted volumes, is correct and that all required\nexternal resources are available.\n1. **Check resource limits**: Make sure that the container has enough CPU\nand memory allocated. Sometimes, increasing the resources in the Pod definition\ncan resolve the issue.\n1. **Debug application**: There might exist bugs or misconfigurations in the\napplication code. Running this container image locally or in a development\nenvironment can help diagnose application specific issues.\n"
  },
  {
    "question": "How can I set the pod replacement policy for a Job in Kubernetes?",
    "answer": "You can set the pod replacement policy for a Job by including the `.spec.podReplacementPolicy` field in the Job specification. For example, to create replacement Pods only when the terminating Pod is fully terminal, set it to `Failed` like this: `podReplacementPolicy: Failed`.",
    "uuid": "c52c1311-e9a1-48eb-80a2-53a64fef65d7",
    "question_with_context": "A user asked the following question:\nQuestion: How can I set the pod replacement policy for a Job in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Delayed creation of replacement pods {#pod-replacement-policy}\nRunbook Content: Advanced usageDelayed creation of replacement pods {#pod-replacement-policy}{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \n{{< note >}}\nYou can only set `podReplacementPolicy` on Jobs if you enable the `JobPodReplacementPolicy`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n(enabled by default).\n{{< /note >}}  \nBy default, the Job controller recreates Pods as soon they either fail or are terminating (have a deletion timestamp).\nThis means that, at a given time, when some of the Pods are terminating, the number of running Pods for a Job\ncan be greater than `parallelism` or greater than one Pod per index (if you are using an Indexed Job).  \nYou may choose to create replacement Pods only when the terminating Pod is fully terminal (has `status.phase: Failed`).\nTo do this, set the `.spec.podReplacementPolicy: Failed`.\nThe default replacement policy depends on whether the Job has a `podFailurePolicy` set.\nWith no Pod failure policy defined for a Job, omitting the `podReplacementPolicy` field selects the\n`TerminatingOrFailed` replacement policy:\nthe control plane creates replacement Pods immediately upon Pod deletion\n(as soon as the control plane sees that a Pod for this Job has `deletionTimestamp` set).\nFor Jobs with a Pod failure policy set, the default  `podReplacementPolicy` is `Failed`, and no other\nvalue is permitted.\nSee [Pod failure policy](#pod-failure-policy) to learn more about Pod failure policies for Jobs.  \n```yaml\nkind: Job\nmetadata:\nname: new\n...\nspec:\npodReplacementPolicy: Failed\n...\n```  \nProvided your cluster has the feature gate enabled, you can inspect the `.status.terminating` field of a Job.\nThe value of the field is the number of Pods owned by the Job that are currently terminating.  \n```shell\nkubectl get jobs/myjob -o yaml\n```  \n```yaml\napiVersion: batch/v1\nkind: Job\n# .metadata and .spec omitted\nstatus:\nterminating: 3 # three Pods are terminating and have not yet reached the Failed phase\n```\n"
  },
  {
    "question": "How can I tell if my Job has failed or succeeded without waiting for the terminal condition?",
    "answer": "You can use the `FailureTarget` or the `SuccessCriteriaMet` condition to evaluate whether the Job has failed or succeeded.",
    "uuid": "2cfcb889-88f4-4f15-9129-08622e77a807",
    "question_with_context": "A user asked the following question:\nQuestion: How can I tell if my Job has failed or succeeded without waiting for the terminal condition?\nThis is about the following runbook:\nRunbook Title: Termination of Job pods\nRunbook Content: Job termination and cleanupTermination of Job podsThe Job controller adds the `FailureTarget` condition or the `SuccessCriteriaMet`\ncondition to the Job to trigger Pod termination after a Job meets either the\nsuccess or failure criteria.  \nFactors like `terminationGracePeriodSeconds` might increase the amount of time\nfrom the moment that the Job controller adds the `FailureTarget` condition or the\n`SuccessCriteriaMet` condition to the moment that all of the Job Pods terminate\nand the Job controller adds a [terminal condition](#terminal-job-conditions)\n(`Failed` or `Complete`).  \nYou can use the `FailureTarget` or the `SuccessCriteriaMet` condition to evaluate\nwhether the Job has failed or succeeded without having to wait for the controller\nto add a terminal condition.  \nFor example, you might want to decide when to create a replacement Job\nthat replaces a failed Job. If you replace the failed Job when the `FailureTarget`\ncondition appears, your replacement Job runs sooner, but could result in Pods\nfrom the failed and the replacement Job running at the same time, using\nextra compute resources.  \nAlternatively, if your cluster has limited resource capacity, you could choose to\nwait until the `Failed` condition appears on the Job, which would delay your\nreplacement Job but would ensure that you conserve resources by waiting\nuntil all of the failed Pods are removed.\n"
  },
  {
    "question": "What are init containers and how do they relate to Pods?",
    "answer": "Init containers are special containers that run during Pod startup and can be included in a Pod alongside application containers.",
    "uuid": "a39c0c31-b140-44c3-b649-76bed84a0747",
    "question_with_context": "A user asked the following question:\nQuestion: What are init containers and how do they relate to Pods?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- erictune\ntitle: Pods\napi_metadata:\n- apiVersion: \"v1\"\nkind: \"Pod\"\ncontent_type: concept\nweight: 10\nno_list: true\n---  \n<!-- overview -->  \n_Pods_ are the smallest deployable units of computing that you can create and manage in Kubernetes.  \nA _Pod_ (as in a pod of whales or pea pod) is a group of one or more\n{{< glossary_tooltip text=\"containers\" term_id=\"container\" >}}, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and\nco-scheduled, and run in a shared context. A Pod models an\napplication-specific \"logical host\": it contains one or more application\ncontainers which are relatively tightly coupled.\nIn non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.  \nAs well as application containers, a Pod can contain\n{{< glossary_tooltip text=\"init containers\" term_id=\"init-container\" >}} that run\nduring Pod startup. You can also inject\n{{< glossary_tooltip text=\"ephemeral containers\" term_id=\"ephemeral-container\" >}}\nfor debugging a running Pod.  \n<!-- body -->\n"
  },
  {
    "question": "Can I run daemons without using DaemonSetInit scripts? How?",
    "answer": "Yes, you can run daemons directly on a node using init, upstartd, or systemd.",
    "uuid": "9c4927a8-7570-4c94-83e7-de1c4ce2ae7f",
    "question_with_context": "A user asked the following question:\nQuestion: Can I run daemons without using DaemonSetInit scripts? How?\nThis is about the following runbook:\nRunbook Title: Init scripts\nRunbook Content: Alternatives to DaemonSetInit scriptsIt is certainly possible to run daemon processes by directly starting them on a node (e.g. using\n`init`, `upstartd`, or `systemd`).  This is perfectly fine.  However, there are several advantages to\nrunning such processes via a DaemonSet:  \n- Ability to monitor and manage logs for daemons in the same way as applications.\n- Same config language and tools (e.g. Pod templates, `kubectl`) for daemons and applications.\n- Running daemons in containers with resource limits increases isolation between daemons from app\ncontainers.  However, this can also be accomplished by running the daemons in a container but not in a Pod.\n"
  },
  {
    "question": "Where can I find more info on using Pods with workload resources?",
    "answer": "You can find more information on how Pods are used with workload resources in the section titled 'Working with Pods'.",
    "uuid": "4a243158-d8d1-4d20-8059-b539c8def2ec",
    "question_with_context": "A user asked the following question:\nQuestion: Where can I find more info on using Pods with workload resources?\nThis is about the following runbook:\nRunbook Title: Using Pods\nRunbook Content: Using PodsThe following is an example of a Pod which consists of a container running the image `nginx:1.14.2`.  \n{{% code_sample file=\"pods/simple-pod.yaml\" %}}  \nTo create the Pod shown above, run the following command:\n```shell\nkubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml\n```  \nPods are generally not created directly and are created using workload resources.\nSee [Working with Pods](#working-with-pods) for more information on how Pods are used\nwith workload resources.\n"
  },
  {
    "question": "How are Pods structured in terms of containers?",
    "answer": "A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.",
    "uuid": "fd5321e5-73a8-403a-a163-7b1ae4c03016",
    "question_with_context": "A user asked the following question:\nQuestion: How are Pods structured in terms of containers?\nThis is about the following runbook:\nRunbook Title: What is a Pod?\nRunbook Content: What is a Pod?{{< note >}}\nYou need to install a [container runtime](/docs/setup/production-environment/container-runtimes/)\ninto each node in the cluster so that Pods can run there.\n{{< /note >}}  \nThe shared context of a Pod is a set of Linux namespaces, cgroups, and\npotentially other facets of isolation - the same things that isolate a {{< glossary_tooltip text=\"container\" term_id=\"container\" >}}. Within a Pod's context, the individual applications may have\nfurther sub-isolations applied.  \nA Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.  \nPods in a Kubernetes cluster are used in two main ways:  \n* **Pods that run a single container**. The \"one-container-per-Pod\" model is the\nmost common Kubernetes use case; in this case, you can think of a Pod as a\nwrapper around a single container; Kubernetes manages Pods rather than managing\nthe containers directly.\n* **Pods that run multiple containers that need to work together**. A Pod can\nencapsulate an application composed of\n[multiple co-located containers](#how-pods-manage-multiple-containers) that are\ntightly coupled and need to share resources. These co-located containers\nform a single cohesive unit.  \nGrouping multiple co-located and co-managed containers in a single Pod is a\nrelatively advanced use case. You should use this pattern only in specific\ninstances in which your containers are tightly coupled.  \nYou don't need to run multiple containers to provide replication (for resilience\nor capacity); if you need multiple replicas, see\n[Workload management](/docs/concepts/workloads/controllers/).\n"
  },
  {
    "question": "How do I drain a node in a Kubernetes cluster with a PodDisruptionBudget?",
    "answer": "You can drain a node using the `kubectl drain` command. For example, to drain `node-1`, you would run `kubectl drain node-1`. This command will attempt to evict the pods running on that node, but it will respect the PodDisruptionBudget, ensuring that the required number of pods remain available.",
    "uuid": "0728de72-ccc8-44d0-a4da-83396979b51b",
    "question_with_context": "A user asked the following question:\nQuestion: How do I drain a node in a Kubernetes cluster with a PodDisruptionBudget?\nThis is about the following runbook:\nRunbook Title: PodDisruptionBudget example {#pdb-example}\nRunbook Content: PodDisruptionBudget example {#pdb-example}Consider a cluster with 3 nodes, `node-1` through `node-3`.\nThe cluster is running several applications.  One of them has 3 replicas initially called\n`pod-a`, `pod-b`, and `pod-c`.  Another, unrelated pod without a PDB, called `pod-x`, is also shown.\nInitially, the pods are laid out as follows:  \n|       node-1         |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *available*   | pod-b *available*   | pod-c *available*  |\n| pod-x  *available*   |                     |                    |  \nAll 3 pods are part of a deployment, and they collectively have a PDB which requires\nthere be at least 2 of the 3 pods to be available at all times.  \nFor example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.\nThe cluster administrator first tries to drain `node-1` using the `kubectl drain` command.\nThat tool tries to evict `pod-a` and `pod-x`.  This succeeds immediately.\nBoth pods go into the `terminating` state at the same time.\nThis puts the cluster in this state:  \n|   node-1 *draining*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |\n| pod-x  *terminating* |                     |                    |  \nThe deployment notices that one of the pods is terminating, so it creates a replacement\ncalled `pod-d`.  Since `node-1` is cordoned, it lands on another node.  Something has\nalso created `pod-y` as a replacement for `pod-x`.  \n(Note: for a StatefulSet, `pod-a`, which would be called something like `pod-0`, would need\nto terminate completely before its replacement, which is also called `pod-0` but has a\ndifferent UID, could be created.  Otherwise, the example applies to a StatefulSet as well.)  \nNow the cluster is in this state:  \n|   node-1 *draining*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |\n| pod-x  *terminating* | pod-d *starting*    | pod-y              |  \nAt some point, the pods terminate, and the cluster looks like this:  \n|    node-1 *drained*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n|                      | pod-b *available*   | pod-c *available*  |\n|                      | pod-d *starting*    | pod-y              |  \nAt this point, if an impatient cluster administrator tries to drain `node-2` or\n`node-3`, the drain command will block, because there are only 2 available\npods for the deployment, and its PDB requires at least 2.  After some time passes, `pod-d` becomes available.  \nThe cluster state now looks like this:  \n|    node-1 *drained*  |       node-2        |       node-3       |\n|:--------------------:|:-------------------:|:------------------:|\n|                      | pod-b *available*   | pod-c *available*  |\n|                      | pod-d *available*   | pod-y              |  \nNow, the cluster administrator tries to drain `node-2`.\nThe drain command will try to evict the two pods in some order, say\n`pod-b` first and then `pod-d`.  It will succeed at evicting `pod-b`.\nBut, when it tries to evict `pod-d`, it will be refused because that would leave only\none pod available for the deployment.  \nThe deployment creates a replacement for `pod-b` called `pod-e`.\nBecause there are not enough resources in the cluster to schedule\n`pod-e` the drain will again block.  The cluster may end up in this\nstate:  \n|    node-1 *drained*  |       node-2        |       node-3       | *no node*          |\n|:--------------------:|:-------------------:|:------------------:|:------------------:|\n|                      | pod-b *terminating* | pod-c *available*  | pod-e *pending*    |\n|                      | pod-d *available*   | pod-y              |                    |  \nAt this point, the cluster administrator needs to\nadd a node back to the cluster to proceed with the upgrade.  \nYou can see how Kubernetes varies the rate at which disruptions\ncan happen, according to:  \n- how many replicas an application needs\n- how long it takes to gracefully shutdown an instance\n- how long it takes a new instance to start up\n- the type of controller\n- the cluster's resource capacity\n"
  },
  {
    "question": "What is the default RestartPolicy if it's not set for a Pod?",
    "answer": "If the RestartPolicy is not set, the default value is Always.",
    "uuid": "8e8361be-2ec5-43d2-991d-11726e6ad39e",
    "question_with_context": "A user asked the following question:\nQuestion: What is the default RestartPolicy if it's not set for a Pod?\nThis is about the following runbook:\nRunbook Title: Replication Controller\nRunbook Content: AlternativesReplication ControllerJobs are complementary to [Replication Controllers](/docs/concepts/workloads/controllers/replicationcontroller/).\nA Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Job\nmanages Pods that are expected to terminate (e.g. batch tasks).  \nAs discussed in [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/), `Job` is *only* appropriate\nfor pods with `RestartPolicy` equal to `OnFailure` or `Never`.\n(Note: If `RestartPolicy` is not set, the default value is `Always`.)\n"
  },
  {
    "question": "How do I find the DNS name for a newly created Pod in a StatefulSet?",
    "answer": "You can find the DNS name for a newly created Pod by querying the Kubernetes API directly, rather than relying on DNS lookups, as DNS may not immediately reflect the new Pod due to negative caching.",
    "uuid": "c62daafb-5e74-4722-ad2c-6e5b31d438a0",
    "question_with_context": "A user asked the following question:\nQuestion: How do I find the DNS name for a newly created Pod in a StatefulSet?\nThis is about the following runbook:\nRunbook Title: Stable Network ID\nRunbook Content: Pod IdentityStable Network IDEach Pod in a StatefulSet derives its hostname from the name of the StatefulSet\nand the ordinal of the Pod. The pattern for the constructed hostname\nis `$(statefulset name)-$(ordinal)`. The example above will create three Pods\nnamed `web-0,web-1,web-2`.\nA StatefulSet can use a [Headless Service](/docs/concepts/services-networking/service/#headless-services)\nto control the domain of its Pods. The domain managed by this Service takes the form:\n`$(service name).$(namespace).svc.cluster.local`, where \"cluster.local\" is the\ncluster domain.\nAs each Pod is created, it gets a matching DNS subdomain, taking the form:\n`$(podname).$(governing service domain)`, where the governing service is defined\nby the `serviceName` field on the StatefulSet.  \nDepending on how DNS is configured in your cluster, you may not be able to look up the DNS\nname for a newly-run Pod immediately. This behavior can occur when other clients in the\ncluster have already sent queries for the hostname of the Pod before it was created.\nNegative caching (normal in DNS) means that the results of previous failed lookups are\nremembered and reused, even after the Pod is running, for at least a few seconds.  \nIf you need to discover Pods promptly after they are created, you have a few options:  \n- Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.\n- Decrease the time of caching in your Kubernetes DNS provider (typically this means editing the\nconfig map for CoreDNS, which currently caches for 30 seconds).  \nAs mentioned in the [limitations](#limitations) section, you are responsible for\ncreating the [Headless Service](/docs/concepts/services-networking/service/#headless-services)\nresponsible for the network identity of the pods.  \nHere are some examples of choices for Cluster Domain, Service name,\nStatefulSet name, and how that affects the DNS names for the StatefulSet's Pods.  \nCluster Domain | Service (ns/name) | StatefulSet (ns/name)  | StatefulSet Domain  | Pod DNS | Pod Hostname |\n-------------- | ----------------- | ----------------- | -------------- | ------- | ------------ |\ncluster.local | default/nginx     | default/web       | nginx.default.svc.cluster.local | web-{0..N-1}.nginx.default.svc.cluster.local | web-{0..N-1} |\ncluster.local | foo/nginx         | foo/web           | nginx.foo.svc.cluster.local     | web-{0..N-1}.nginx.foo.svc.cluster.local     | web-{0..N-1} |\nkube.local    | foo/nginx         | foo/web           | nginx.foo.svc.kube.local        | web-{0..N-1}.nginx.foo.svc.kube.local        | web-{0..N-1} |  \n{{< note >}}\nCluster Domain will be set to `cluster.local` unless\n[otherwise configured](/docs/concepts/services-networking/dns-pod-service/).\n{{< /note >}}\n"
  },
  {
    "question": "How can I learn about Pods in Kubernetes?",
    "answer": "You can learn about Pods by visiting the [Pods documentation](/docs/concepts/workloads/pods).",
    "uuid": "35b4d1bc-77fd-4259-a174-980eae0bd273",
    "question_with_context": "A user asked the following question:\nQuestion: How can I learn about Pods in Kubernetes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Read about different ways of running Jobs:\n* [Coarse Parallel Processing Using a Work Queue](/docs/tasks/job/coarse-parallel-processing-work-queue/)\n* [Fine Parallel Processing Using a Work Queue](/docs/tasks/job/fine-parallel-processing-work-queue/)\n* Use an [indexed Job for parallel processing with static work assignment](/docs/tasks/job/indexed-parallel-processing-static/)\n* Create multiple Jobs based on a template: [Parallel Processing using Expansions](/docs/tasks/job/parallel-processing-expansion/)\n* Follow the links within [Clean up finished jobs automatically](#clean-up-finished-jobs-automatically)\nto learn more about how your cluster can clean up completed and / or failed tasks.\n* `Job` is part of the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/job-v1\" >}}\nobject definition to understand the API for jobs.\n* Read about [`CronJob`](/docs/concepts/workloads/controllers/cron-jobs/), which you\ncan use to define a series of Jobs that will run based on a schedule, similar to\nthe UNIX tool `cron`.\n* Practice how to configure handling of retriable and non-retriable pod failures\nusing `podFailurePolicy`, based on the step-by-step [examples](/docs/tasks/job/pod-failure-policy/).\n"
  },
  {
    "question": "Is it possible to perform disruptive actions without downtime?",
    "answer": "Yes, it is possible to have no downtime, but it may be costly for duplicated nodes and require significant human effort to orchestrate the switchover.",
    "uuid": "fe8649e7-e004-4f33-9d8f-23909af228f7",
    "question_with_context": "A user asked the following question:\nQuestion: Is it possible to perform disruptive actions without downtime?\nThis is about the following runbook:\nRunbook Title: How to perform Disruptive Actions on your Cluster\nRunbook Content: How to perform Disruptive Actions on your ClusterIf you are a Cluster Administrator, and you need to perform a disruptive action on all\nthe nodes in your cluster, such as a node or system software upgrade, here are some options:  \n- Accept downtime during the upgrade.\n- Failover to another complete replica cluster.\n-  No downtime, but may be costly both for the duplicated nodes\nand for human effort to orchestrate the switchover.\n- Write disruption tolerant applications and use PDBs.\n- No downtime.\n- Minimal resource duplication.\n- Allows more automation of cluster administration.\n- Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary\ndisruptions largely overlaps with work to support autoscaling and tolerating\ninvoluntary disruptions.\n"
  },
  {
    "question": "Can I set a different restart policy in the pod template?",
    "answer": "No, only a `.spec.template.spec.restartPolicy` equal to `Always` is allowed, which is the default if not specified.",
    "uuid": "dcd5e71d-468b-41bb-a7df-933f79da1357",
    "question_with_context": "A user asked the following question:\nQuestion: Can I set a different restart policy in the pod template?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a ReplicationController ManifestPod TemplateThe `.spec.template` is the only required field of the `.spec`.  \nThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}, except it is nested and does not have an `apiVersion` or `kind`.  \nIn addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate\nlabels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [pod selector](#pod-selector).  \nOnly a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is allowed, which is the default if not specified.  \nFor local container restarts, ReplicationControllers delegate to an agent on the node,\nfor example the [Kubelet](/docs/reference/command-line-tools-reference/kubelet/).\n"
  },
  {
    "question": "How do I control the number of old ReplicaSets retained in a Deployment?",
    "answer": "You can set the `.spec.revisionHistoryLimit` field in a Deployment to specify how many old ReplicaSets you want to retain.",
    "uuid": "deb7b431-a24b-4111-9b57-b24abfff80b8",
    "question_with_context": "A user asked the following question:\nQuestion: How do I control the number of old ReplicaSets retained in a Deployment?\nThis is about the following runbook:\nRunbook Title: Clean up Policy\nRunbook Content: Clean up PolicyYou can set `.spec.revisionHistoryLimit` field in a Deployment to specify how many old ReplicaSets for\nthis Deployment you want to retain. The rest will be garbage-collected in the background. By default,\nit is 10.  \n{{< note >}}\nExplicitly setting this field to 0, will result in cleaning up all the history of your Deployment\nthus that Deployment will not be able to roll back.\n{{< /note >}}\n"
  },
  {
    "question": "How can I share data between containers in a Pod?",
    "answer": "You can share data between containers in a Pod by specifying a set of shared storage volumes. All containers in the Pod can access these shared volumes.",
    "uuid": "8a8c0205-7fc5-4de3-b354-ac9ba710a732",
    "question_with_context": "A user asked the following question:\nQuestion: How can I share data between containers in a Pod?\nThis is about the following runbook:\nRunbook Title: Storage in Pods {#pod-storage}\nRunbook Content: Resource sharing and communicationStorage in Pods {#pod-storage}A Pod can specify a set of shared storage\n{{< glossary_tooltip text=\"volumes\" term_id=\"volume\" >}}. All containers\nin the Pod can access the shared volumes, allowing those containers to\nshare data. Volumes also allow persistent data in a Pod to survive\nin case one of the containers within needs to be restarted. See\n[Storage](/docs/concepts/storage/) for more information on how\nKubernetes implements shared storage and makes it available to Pods.\n"
  },
  {
    "question": "Is there a default value for progress deadline seconds?",
    "answer": "Yes, the default value for `.spec.progressDeadlineSeconds` is 600 seconds.",
    "uuid": "1378b07a-4204-46a6-975f-2038e6cf7a93",
    "question_with_context": "A user asked the following question:\nQuestion: Is there a default value for progress deadline seconds?\nThis is about the following runbook:\nRunbook Title: Progress Deadline Seconds\nRunbook Content: Writing a Deployment SpecProgress Deadline Seconds`.spec.progressDeadlineSeconds` is an optional field that specifies the number of seconds you want\nto wait for your Deployment to progress before the system reports back that the Deployment has\n[failed progressing](#failed-deployment) - surfaced as a condition with `type: Progressing`, `status: \"False\"`.\nand `reason: ProgressDeadlineExceeded` in the status of the resource. The Deployment controller will keep\nretrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment\ncontroller will roll back a Deployment as soon as it observes such a condition.  \nIf specified, this field needs to be greater than `.spec.minReadySeconds`.\n"
  },
  {
    "question": "Can I use UTC for my CronJob schedule?",
    "answer": "Yes, you can set `.spec.timeZone: \"Etc/UTC\"` to instruct Kubernetes to interpret the schedule relative to Coordinated Universal Time.",
    "uuid": "d75c492d-0e14-4cb0-b7d0-47797c58d264",
    "question_with_context": "A user asked the following question:\nQuestion: Can I use UTC for my CronJob schedule?\nThis is about the following runbook:\nRunbook Title: Time zones\nRunbook Content: Writing a CronJob specTime zones{{< feature-state for_k8s_version=\"v1.27\" state=\"stable\" >}}  \nFor CronJobs with no time zone specified, the {{< glossary_tooltip term_id=\"kube-controller-manager\" text=\"kube-controller-manager\" >}}\ninterprets schedules relative to its local time zone.  \nYou can specify a time zone for a CronJob by setting `.spec.timeZone` to the name\nof a valid [time zone](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).\nFor example, setting `.spec.timeZone: \"Etc/UTC\"` instructs Kubernetes to interpret\nthe schedule relative to Coordinated Universal Time.  \nA time zone database from the Go standard library is included in the binaries and used as a fallback in case an external database is not available on the system.\n"
  },
  {
    "question": "What should I do if the probe outcome is 'Unknown'?",
    "answer": "No action should be taken, and the kubelet will make further checks.",
    "uuid": "236a7d87-6ea9-4a87-a5be-32713738afa1",
    "question_with_context": "A user asked the following question:\nQuestion: What should I do if the probe outcome is 'Unknown'?\nThis is about the following runbook:\nRunbook Title: Probe outcome\nRunbook Content: Container probesProbe outcomeEach probe has one of three results:  \n`Success`\n: The container passed the diagnostic.  \n`Failure`\n: The container failed the diagnostic.  \n`Unknown`\n: The diagnostic failed (no action should be taken, and the kubelet\nwill make further checks).\n"
  },
  {
    "question": "How can I automatically clean up finished Jobs in Kubernetes?",
    "answer": "You can automatically clean up finished Jobs by using a TTL mechanism provided by a TTL controller. This is done by specifying the `.spec.ttlSecondsAfterFinished` field of the Job.",
    "uuid": "1370f2a3-1802-4f1f-a7df-809251a61e7b",
    "question_with_context": "A user asked the following question:\nQuestion: How can I automatically clean up finished Jobs in Kubernetes?\nThis is about the following runbook:\nRunbook Title: TTL mechanism for finished Jobs\nRunbook Content: Clean up finished jobs automaticallyTTL mechanism for finished Jobs{{< feature-state for_k8s_version=\"v1.23\" state=\"stable\" >}}  \nAnother way to clean up finished Jobs (either `Complete` or `Failed`)\nautomatically is to use a TTL mechanism provided by a\n[TTL controller](/docs/concepts/workloads/controllers/ttlafterfinished/) for\nfinished resources, by specifying the `.spec.ttlSecondsAfterFinished` field of\nthe Job.  \nWhen the TTL controller cleans up the Job, it will delete the Job cascadingly,\ni.e. delete its dependent objects, such as Pods, together with the Job. Note\nthat when the Job is deleted, its lifecycle guarantees, such as finalizers, will\nbe honored.  \nFor example:  \n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: pi-with-ttl\nspec:\nttlSecondsAfterFinished: 100\ntemplate:\nspec:\ncontainers:\n- name: pi\nimage: perl:5.34.0\ncommand: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\nrestartPolicy: Never\n```  \nThe Job `pi-with-ttl` will be eligible to be automatically deleted, `100`\nseconds after it finishes.  \nIf the field is set to `0`, the Job will be eligible to be automatically deleted\nimmediately after it finishes. If the field is unset, this Job won't be cleaned\nup by the TTL controller after it finishes.  \n{{< note >}}\nIt is recommended to set `ttlSecondsAfterFinished` field because unmanaged jobs\n(Jobs that you created directly, and not indirectly through other workload APIs\nsuch as CronJob) have a default deletion\npolicy of `orphanDependents` causing Pods created by an unmanaged Job to be left around\nafter that Job is fully deleted.\nEven though the {{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} eventually\n[garbage collects](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)\nthe Pods from a deleted Job after they either fail or complete, sometimes those\nlingering pods may cause cluster performance degradation or in worst case cause the\ncluster to go offline due to this degradation.  \nYou can use [LimitRanges](/docs/concepts/policy/limit-range/) and\n[ResourceQuotas](/docs/concepts/policy/resource-quotas/) to place a\ncap on the amount of resources that a particular namespace can\nconsume.\n{{< /note >}}\n"
  },
  {
    "question": "Can I identify a Pod in a StatefulSet by its name?",
    "answer": "Yes, you can identify a Pod in a StatefulSet by its name using the label `statefulset.kubernetes.io/pod-name`.",
    "uuid": "6d4be5d1-5504-4838-a321-1aa5fa50c911",
    "question_with_context": "A user asked the following question:\nQuestion: Can I identify a Pod in a StatefulSet by its name?\nThis is about the following runbook:\nRunbook Title: Pod Name Label\nRunbook Content: Pod IdentityPod Name LabelWhen the StatefulSet {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} creates a Pod,\nit adds a label, `statefulset.kubernetes.io/pod-name`, that is set to the name of\nthe Pod. This label allows you to attach a Service to a specific Pod in\nthe StatefulSet.\n"
  },
  {
    "question": "How do I pause a Deployment in Kubernetes?",
    "answer": "To pause a Deployment, you can set the `.spec.paused` field to `true`.",
    "uuid": "d1f97aa8-56e3-40a5-b73b-f81f7fbe5860",
    "question_with_context": "A user asked the following question:\nQuestion: How do I pause a Deployment in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Paused\nRunbook Content: Writing a Deployment SpecPaused`.spec.paused` is an optional boolean field for pausing and resuming a Deployment. The only difference between\na paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused\nDeployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when\nit is created.\n"
  },
  {
    "question": "What's the best way to scale my application with Pods?",
    "answer": "To scale your application horizontally, you should use multiple Pods, one for each instance, typically managed by a workload resource and its controller.",
    "uuid": "70f9ac51-d732-4f73-bb9d-e6ce85a78e89",
    "question_with_context": "A user asked the following question:\nQuestion: What's the best way to scale my application with Pods?\nThis is about the following runbook:\nRunbook Title: Workload resources for managing pods\nRunbook Content: Using PodsWorkload resources for managing podsUsually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as {{< glossary_tooltip text=\"Deployment\"\nterm_id=\"deployment\" >}} or {{< glossary_tooltip text=\"Job\" term_id=\"job\" >}}.\nIf your Pods need to track state, consider the\n{{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}} resource.  \nEach Pod is meant to run a single instance of a given application. If you want to\nscale your application horizontally (to provide more overall resources by running\nmore instances), you should use multiple Pods, one for each instance. In\nKubernetes, this is typically referred to as _replication_.\nReplicated Pods are usually created and managed as a group by a workload resource\nand its {{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}.  \nSee [Pods and controllers](#pods-and-controllers) for more information on how\nKubernetes uses workload resources, and their controllers, to implement application\nscaling and auto-healing.  \nPods natively provide two kinds of shared resources for their constituent containers:\n[networking](#pod-networking) and [storage](#pod-storage).\n"
  },
  {
    "question": "What fields are included in the .spec.selector object?",
    "answer": ".spec.selector consists of two fields: matchLabels and matchExpressions.",
    "uuid": "f7bd6825-b5db-4b2c-8afb-ee34971960a9",
    "question_with_context": "A user asked the following question:\nQuestion: What fields are included in the .spec.selector object?\nThis is about the following runbook:\nRunbook Title: Pod Selector\nRunbook Content: Writing a DaemonSet SpecPod SelectorThe `.spec.selector` field is a pod selector.  It works the same as the `.spec.selector` of\na [Job](/docs/concepts/workloads/controllers/job/).  \nYou must specify a pod selector that matches the labels of the\n`.spec.template`.\nAlso, once a DaemonSet is created,\nits `.spec.selector` can not be mutated. Mutating the pod selector can lead to the\nunintentional orphaning of Pods, and it was found to be confusing to users.  \nThe `.spec.selector` is an object consisting of two fields:  \n* `matchLabels` - works the same as the `.spec.selector` of a\n[ReplicationController](/docs/concepts/workloads/controllers/replicationcontroller/).\n* `matchExpressions` - allows to build more sophisticated selectors by specifying key,\nlist of values and an operator that relates the key and values.  \nWhen the two are specified the result is ANDed.  \nThe `.spec.selector` must match the `.spec.template.metadata.labels`.\nConfig with these two not matching will be rejected by the API.\n"
  },
  {
    "question": "What do I need to install for Pods to run in a Kubernetes cluster?",
    "answer": "You need to install a container runtime into each node in the cluster so that Pods can run there.",
    "uuid": "fd5321e5-73a8-403a-a163-7b1ae4c03016",
    "question_with_context": "A user asked the following question:\nQuestion: What do I need to install for Pods to run in a Kubernetes cluster?\nThis is about the following runbook:\nRunbook Title: What is a Pod?\nRunbook Content: What is a Pod?{{< note >}}\nYou need to install a [container runtime](/docs/setup/production-environment/container-runtimes/)\ninto each node in the cluster so that Pods can run there.\n{{< /note >}}  \nThe shared context of a Pod is a set of Linux namespaces, cgroups, and\npotentially other facets of isolation - the same things that isolate a {{< glossary_tooltip text=\"container\" term_id=\"container\" >}}. Within a Pod's context, the individual applications may have\nfurther sub-isolations applied.  \nA Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.  \nPods in a Kubernetes cluster are used in two main ways:  \n* **Pods that run a single container**. The \"one-container-per-Pod\" model is the\nmost common Kubernetes use case; in this case, you can think of a Pod as a\nwrapper around a single container; Kubernetes manages Pods rather than managing\nthe containers directly.\n* **Pods that run multiple containers that need to work together**. A Pod can\nencapsulate an application composed of\n[multiple co-located containers](#how-pods-manage-multiple-containers) that are\ntightly coupled and need to share resources. These co-located containers\nform a single cohesive unit.  \nGrouping multiple co-located and co-managed containers in a single Pod is a\nrelatively advanced use case. You should use this pattern only in specific\ninstances in which your containers are tightly coupled.  \nYou don't need to run multiple containers to provide replication (for resilience\nor capacity); if you need multiple replicas, see\n[Workload management](/docs/concepts/workloads/controllers/).\n"
  },
  {
    "question": "How do I scale an Elastic Indexed Job?",
    "answer": "You can scale Indexed Jobs by mutating both `.spec.parallelism` and `.spec.completions` together such that `.spec.parallelism == .spec.completions`.",
    "uuid": "7356f2d2-c66e-4493-b43e-6b35e036c44d",
    "question_with_context": "A user asked the following question:\nQuestion: How do I scale an Elastic Indexed Job?\nThis is about the following runbook:\nRunbook Title: Elastic Indexed Jobs\nRunbook Content: Advanced usageElastic Indexed Jobs{{< feature-state feature_gate_name=\"ElasticIndexedJob\" >}}  \nYou can scale Indexed Jobs up or down by mutating both `.spec.parallelism`\nand `.spec.completions` together such that `.spec.parallelism == .spec.completions`.\nWhen scaling down, Kubernetes removes the Pods with higher indexes.  \nUse cases for elastic Indexed Jobs include batch workloads which require\nscaling an indexed Job, such as MPI, Horovod, Ray, and PyTorch training jobs.\n"
  },
  {
    "question": "Is it possible to update the TTL for a Job that is still running?",
    "answer": "The content does not specify about updating the TTL for a running Job, it only mentions modifying the TTL for finished Jobs.",
    "uuid": "ccf5d38f-25cc-445c-9b6f-511202e0067d",
    "question_with_context": "A user asked the following question:\nQuestion: Is it possible to update the TTL for a Job that is still running?\nThis is about the following runbook:\nRunbook Title: Updating TTL for finished Jobs\nRunbook Content: CaveatsUpdating TTL for finished JobsYou can modify the TTL period, e.g. `.spec.ttlSecondsAfterFinished` field of Jobs,\nafter the job is created or has finished. If you extend the TTL period after the\nexisting `ttlSecondsAfterFinished` period has expired, Kubernetes doesn't guarantee\nto retain that Job, even if an update to extend the TTL returns a successful API\nresponse.\n"
  },
  {
    "question": "Is there a way to further enhance availability when running replicated applications?",
    "answer": "Yes, you can spread applications across racks using anti-affinity or across zones if using a multi-zone cluster.",
    "uuid": "67957798-f2b6-4314-80d8-0225f9593c3c",
    "question_with_context": "A user asked the following question:\nQuestion: Is there a way to further enhance availability when running replicated applications?\nThis is about the following runbook:\nRunbook Title: Dealing with disruptions\nRunbook Content: Dealing with disruptionsHere are some ways to mitigate involuntary disruptions:  \n- Ensure your pod [requests the resources](/docs/tasks/configure-pod-container/assign-memory-resource) it needs.\n- Replicate your application if you need higher availability.  (Learn about running replicated\n[stateless](/docs/tasks/run-application/run-stateless-application-deployment/)\nand [stateful](/docs/tasks/run-application/run-replicated-stateful-application/) applications.)\n- For even higher availability when running replicated applications,\nspread applications across racks (using\n[anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity))\nor across zones (if using a\n[multi-zone cluster](/docs/setup/multiple-zones).)  \nThe frequency of voluntary disruptions varies.  On a basic Kubernetes cluster, there are\nno automated voluntary disruptions (only user-triggered ones).  However, your cluster administrator or hosting provider\nmay run some additional services which cause voluntary disruptions. For example,\nrolling out node software updates can cause voluntary disruptions. Also, some implementations\nof cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.\nYour cluster administrator or hosting provider should have documented what level of voluntary\ndisruptions, if any, to expect. Certain configuration options, such as\n[using PriorityClasses](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\nin your pod spec can also cause voluntary (and involuntary) disruptions.\n"
  },
  {
    "question": "How can I resume a paused Deployment rollout?",
    "answer": "To resume a paused Deployment rollout, use the command: `kubectl rollout resume deployment/nginx-deployment`. The output will confirm that the deployment has been resumed.",
    "uuid": "90ea9916-2644-45dc-91aa-e401a8fd8985",
    "question_with_context": "A user asked the following question:\nQuestion: How can I resume a paused Deployment rollout?\nThis is about the following runbook:\nRunbook Title: Pausing and Resuming a rollout of a Deployment {#pausing-and-resuming-a-deployment}\nRunbook Content: Pausing and Resuming a rollout of a Deployment {#pausing-and-resuming-a-deployment}When you update a Deployment, or plan to, you can pause rollouts\nfor that Deployment before you trigger one or more updates. When\nyou're ready to apply those changes, you resume rollouts for the\nDeployment. This approach allows you to\napply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.  \n* For example, with a Deployment that was created:  \nGet the Deployment details:\n```shell\nkubectl get deploy\n```\nThe output is similar to this:\n```\nNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx     3         3         3            3           1m\n```\nGet the rollout status:\n```shell\nkubectl get rs\n```\nThe output is similar to this:\n```\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   3         3         3         1m\n```  \n* Pause by running the following command:\n```shell\nkubectl rollout pause deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment paused\n```  \n* Then update the image of the Deployment:\n```shell\nkubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment image updated\n```  \n* Notice that no new rollout started:\n```shell\nkubectl rollout history deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\ndeployments \"nginx\"\nREVISION  CHANGE-CAUSE\n1   <none>\n```\n* Get the rollout status to verify that the existing ReplicaSet has not changed:\n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   3         3         3         2m\n```  \n* You can make as many updates as you wish, for example, update the resources that will be used:\n```shell\nkubectl set resources deployment/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment resource requirements updated\n```  \nThe initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to\nthe Deployment will not have any effect as long as the Deployment rollout is paused.  \n* Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:\n```shell\nkubectl rollout resume deployment/nginx-deployment\n```  \nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment resumed\n```\n* {{< glossary_tooltip text=\"Watch\" term_id=\"watch\" >}} the status of the rollout until it's done.\n```shell\nkubectl get rs --watch\n```  \nThe output is similar to this:\n```\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   2         2         2         2m\nnginx-3926361531   2         2         0         6s\nnginx-3926361531   2         2         1         18s\nnginx-2142116321   1         2         2         2m\nnginx-2142116321   1         2         2         2m\nnginx-3926361531   3         2         1         18s\nnginx-3926361531   3         2         1         18s\nnginx-2142116321   1         1         1         2m\nnginx-3926361531   3         3         1         18s\nnginx-3926361531   3         3         2         19s\nnginx-2142116321   0         1         1         2m\nnginx-2142116321   0         1         1         2m\nnginx-2142116321   0         0         0         2m\nnginx-3926361531   3         3         3         20s\n```\n* Get the status of the latest rollout:\n```shell\nkubectl get rs\n```  \nThe output is similar to this:\n```\nNAME               DESIRED   CURRENT   READY     AGE\nnginx-2142116321   0         0         0         2m\nnginx-3926361531   3         3         3         28s\n```\n{{< note >}}\nYou cannot rollback a paused Deployment until you resume it.\n{{< /note >}}\n"
  },
  {
    "question": "Is there a way to see processes in other containers while using ephemeral containers?",
    "answer": "Yes, it's helpful to enable process namespace sharing to view processes in other containers when using ephemeral containers.",
    "uuid": "63f63340-1e9b-4798-a2f9-c4bc89519c5a",
    "question_with_context": "A user asked the following question:\nQuestion: Is there a way to see processes in other containers while using ephemeral containers?\nThis is about the following runbook:\nRunbook Title: Uses for ephemeral containers\nRunbook Content: Uses for ephemeral containersEphemeral containers are useful for interactive troubleshooting when `kubectl\nexec` is insufficient because a container has crashed or a container image\ndoesn't include debugging utilities.  \nIn particular, [distroless images](https://github.com/GoogleContainerTools/distroless)\nenable you to deploy minimal container images that reduce attack surface\nand exposure to bugs and vulnerabilities. Since distroless images do not include a\nshell or any debugging utilities, it's difficult to troubleshoot distroless\nimages using `kubectl exec` alone.  \nWhen using ephemeral containers, it's helpful to enable [process namespace\nsharing](/docs/tasks/configure-pod-container/share-process-namespace/) so\nyou can view processes in other containers.\n"
  },
  {
    "question": "What happens if the labels in the ReplicaSet template don't match the selector?",
    "answer": "If the labels in the ReplicaSet template do not match the `.spec.selector`, the ReplicaSet will be rejected by the API.",
    "uuid": "cfa73a27-18df-42d8-ab40-6d33b3c3d057",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if the labels in the ReplicaSet template don't match the selector?\nThis is about the following runbook:\nRunbook Title: Pod Selector\nRunbook Content: Writing a ReplicaSet manifestPod SelectorThe `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/). As discussed\n[earlier](#how-a-replicaset-works) these are the labels used to identify potential Pods to acquire. In our\n`frontend.yaml` example, the selector was:  \n```yaml\nmatchLabels:\ntier: frontend\n```  \nIn the ReplicaSet, `.spec.template.metadata.labels` must match `spec.selector`, or it will\nbe rejected by the API.  \n{{< note >}}\nFor 2 ReplicaSets specifying the same `.spec.selector` but different\n`.spec.template.metadata.labels` and `.spec.template.spec` fields, each ReplicaSet ignores the\nPods created by the other ReplicaSet.\n{{< /note >}}\n"
  },
  {
    "question": "What should I do to configure Liveness, Readiness, and Startup Probes?",
    "answer": "You can configure Liveness, Readiness, and Startup Probes by following the guidelines provided in the documentation.",
    "uuid": "b1c33315-7646-41a5-a3e5-086205c697eb",
    "question_with_context": "A user asked the following question:\nQuestion: What should I do to configure Liveness, Readiness, and Startup Probes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Get hands-on experience\n[attaching handlers to container lifecycle events](/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/).  \n* Get hands-on experience\n[configuring Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).  \n* Learn more about [container lifecycle hooks](/docs/concepts/containers/container-lifecycle-hooks/).  \n* Learn more about [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/).  \n* For detailed information about Pod and container status in the API, see\nthe API reference documentation covering\n[`status`](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus) for Pod.\n"
  },
  {
    "question": "Can I run multiple tasks at the same time using a Job?",
    "answer": "Yes, you can use a Job to run multiple Pods in parallel.",
    "uuid": "5f453391-6a37-415b-bcdc-6952bcbf1f85",
    "question_with_context": "A user asked the following question:\nQuestion: Can I run multiple tasks at the same time using a Job?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- alculquicondor\n- erictune\n- soltysh\ntitle: Jobs\napi_metadata:\n- apiVersion: \"batch/v1\"\nkind: \"Job\"\ncontent_type: concept\ndescription: >-\nJobs represent one-off tasks that run to completion and then stop.\nfeature:\ntitle: Batch execution\ndescription: >\nIn addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.\nweight: 50\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nA Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate.\nAs pods successfully complete, the Job tracks the successful completions. When a specified number\nof successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up\nthe Pods it created. Suspending a Job will delete its active Pods until the Job\nis resumed again.  \nA simple case is to create one Job object in order to reliably run one Pod to completion.\nThe Job object will start a new Pod if the first Pod fails or is deleted (for example\ndue to a node hardware failure or a node reboot).  \nYou can also use a Job to run multiple Pods in parallel.  \nIf you want to run a Job (either a single task, or several in parallel) on a schedule,\nsee [CronJob](/docs/concepts/workloads/controllers/cron-jobs/).  \n<!-- body -->\n"
  },
  {
    "question": "What is the role of .spec.minReadySeconds in a Rolling Update?",
    "answer": ".spec.minReadySeconds specifies a duration that the control plane waits after a Pod turns ready before moving on to update the next Pod.",
    "uuid": "a6f6fd19-524f-4209-a8f3-e75ec5d0a322",
    "question_with_context": "A user asked the following question:\nQuestion: What is the role of .spec.minReadySeconds in a Rolling Update?\nThis is about the following runbook:\nRunbook Title: Rolling Updates\nRunbook Content: Rolling UpdatesWhen a StatefulSet's `.spec.updateStrategy.type` is set to `RollingUpdate`, the\nStatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed\nin the same order as Pod termination (from the largest ordinal to the smallest), updating\neach Pod one at a time.  \nThe Kubernetes control plane waits until an updated Pod is Running and Ready prior\nto updating its predecessor. If you have set `.spec.minReadySeconds` (see\n[Minimum Ready Seconds](#minimum-ready-seconds)), the control plane additionally waits that\namount of time after the Pod turns ready, before moving on.\n"
  },
  {
    "question": "What does the ReplicaSet controller do when scaling down?",
    "answer": "The ReplicaSet controller ensures that a desired number of Pods with a matching label selector are available and operational, and it chooses which pods to delete based on a specific algorithm.",
    "uuid": "5ca7f1f4-c5ef-4236-86ce-d61258621a62",
    "question_with_context": "A user asked the following question:\nQuestion: What does the ReplicaSet controller do when scaling down?\nThis is about the following runbook:\nRunbook Title: Scaling a ReplicaSet\nRunbook Content: Working with ReplicaSetsScaling a ReplicaSetA ReplicaSet can be easily scaled up or down by simply updating the `.spec.replicas` field. The ReplicaSet controller\nensures that a desired number of Pods with a matching label selector are available and operational.  \nWhen scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods to\nprioritize scaling down pods based on the following general algorithm:  \n1. Pending (and unschedulable) pods are scaled down first\n1. If `controller.kubernetes.io/pod-deletion-cost` annotation is set, then\nthe pod with the lower value will come first.\n1. Pods on nodes with more replicas come before pods on nodes with fewer replicas.\n1. If the pods' creation times differ, the pod that was created more recently\ncomes before the older pod (the creation times are bucketed on an integer log scale).  \nIf all of the above match, then selection is random.\n"
  },
  {
    "question": "What does a ReplicaSet do?",
    "answer": "A ReplicaSet ensures that a specified number of pod replicas are running at any given time.",
    "uuid": "fc0d9609-0ccb-4237-9c10-570ce16e53c1",
    "question_with_context": "A user asked the following question:\nQuestion: What does a ReplicaSet do?\nThis is about the following runbook:\nRunbook Title: When to use a ReplicaSet\nRunbook Content: When to use a ReplicaSetA ReplicaSet ensures that a specified number of pod replicas are running at any given\ntime. However, a Deployment is a higher-level concept that manages ReplicaSets and\nprovides declarative updates to Pods along with a lot of other useful features.\nTherefore, we recommend using Deployments instead of directly using ReplicaSets, unless\nyou require custom update orchestration or don't require updates at all.  \nThis actually means that you may never need to manipulate ReplicaSet objects:\nuse a Deployment instead, and define your application in the spec section.\n"
  },
  {
    "question": "How does Memory QoS determine which settings to apply for memory management?",
    "answer": "Memory QoS relies on the QoS class to determine which settings to apply, although it uses different mechanisms to provide controls over quality of service.",
    "uuid": "1c557ec4-1ee2-4d85-b0ed-2c405ce09422",
    "question_with_context": "A user asked the following question:\nQuestion: How does Memory QoS determine which settings to apply for memory management?\nThis is about the following runbook:\nRunbook Title: Memory QoS with cgroup v2\nRunbook Content: Memory QoS with cgroup v2{{< feature-state feature_gate_name=\"MemoryQoS\" >}}  \nMemory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes.\nMemory requests and limits of containers in pod are used to set specific interfaces `memory.min`\nand `memory.high` provided by the memory controller. When `memory.min` is set to memory requests,\nmemory resources are reserved and never reclaimed by the kernel; this is how Memory QoS ensures\nmemory availability for Kubernetes pods. And if memory limits are set in the container,\nthis means that the system needs to limit container memory usage; Memory QoS uses `memory.high`\nto throttle workload approaching its memory limit, ensuring that the system is not overwhelmed\nby instantaneous memory allocation.  \nMemory QoS relies on QoS class to determine which settings to apply; however, these are different\nmechanisms that both provide controls over quality of service.\n"
  },
  {
    "question": "What happens if a node is marked as unschedulable?",
    "answer": "Kubernetes can still run DaemonSet Pods on nodes that are marked as unschedulable because the DaemonSet controller automatically sets the `node.kubernetes.io/unschedulable:NoSchedule` toleration.",
    "uuid": "a0400e16-55db-4c60-9c2c-ad0a7dc9d940",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if a node is marked as unschedulable?\nThis is about the following runbook:\nRunbook Title: Taints and tolerations\nRunbook Content: How Daemon Pods are scheduledTaints and tolerationsThe DaemonSet controller automatically adds a set of {{< glossary_tooltip\ntext=\"tolerations\" term_id=\"toleration\" >}} to DaemonSet Pods:  \n{{< table caption=\"Tolerations for DaemonSet pods\" >}}  \n| Toleration key                                                                                                        | Effect       | Details                                                                                                                                       |\n| --------------------------------------------------------------------------------------------------------------------- | ------------ | --------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`node.kubernetes.io/not-ready`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-not-ready)             | `NoExecute`  | DaemonSet Pods can be scheduled onto nodes that are not healthy or ready to accept Pods. Any DaemonSet Pods running on such nodes will not be evicted. |\n| [`node.kubernetes.io/unreachable`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-unreachable)         | `NoExecute`  | DaemonSet Pods can be scheduled onto nodes that are unreachable from the node controller. Any DaemonSet Pods running on such nodes will not be evicted. |\n| [`node.kubernetes.io/disk-pressure`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-disk-pressure)     | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with disk pressure issues.                                                                         |\n| [`node.kubernetes.io/memory-pressure`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-memory-pressure) | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with memory pressure issues.                                                                        |\n| [`node.kubernetes.io/pid-pressure`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-pid-pressure) | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with process pressure issues.                                                                        |\n| [`node.kubernetes.io/unschedulable`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-unschedulable)   | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes that are unschedulable.                                                                            |\n| [`node.kubernetes.io/network-unavailable`](/docs/reference/labels-annotations-taints/#node-kubernetes-io-network-unavailable) | `NoSchedule` | **Only added for DaemonSet Pods that request host networking**, i.e., Pods having `spec.hostNetwork: true`. Such DaemonSet Pods can be scheduled onto nodes with unavailable network.|  \n{{< /table >}}  \nYou can add your own tolerations to the Pods of a DaemonSet as well, by\ndefining these in the Pod template of the DaemonSet.  \nBecause the DaemonSet controller sets the\n`node.kubernetes.io/unschedulable:NoSchedule` toleration automatically,\nKubernetes can run DaemonSet Pods on nodes that are marked as _unschedulable_.  \nIf you use a DaemonSet to provide an important node-level function, such as\n[cluster networking](/docs/concepts/cluster-administration/networking/), it is\nhelpful that Kubernetes places DaemonSet Pods on nodes before they are ready.\nFor example, without that special toleration, you could end up in a deadlock\nsituation where the node is not marked as ready because the network plugin is\nnot running there, and at the same time the network plugin is not running on\nthat node because the node is not yet ready.\n"
  },
  {
    "question": "How can I make sure my Pods only run on specific nodes using a DaemonSet?",
    "answer": "You can specify a `.spec.template.spec.nodeSelector` in your DaemonSet spec to create Pods on nodes that match the node selector.",
    "uuid": "9a98744a-3fa5-4f8c-a3a4-26604b893852",
    "question_with_context": "A user asked the following question:\nQuestion: How can I make sure my Pods only run on specific nodes using a DaemonSet?\nThis is about the following runbook:\nRunbook Title: Running Pods on select Nodes\nRunbook Content: Writing a DaemonSet SpecRunning Pods on select NodesIf you specify a `.spec.template.spec.nodeSelector`, then the DaemonSet controller will\ncreate Pods on nodes which match that [node selector](/docs/concepts/scheduling-eviction/assign-pod-node/).\nLikewise if you specify a `.spec.template.spec.affinity`,\nthen DaemonSet controller will create Pods on nodes which match that\n[node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/).\nIf you do not specify either, then the DaemonSet controller will create Pods on all nodes.\n"
  },
  {
    "question": "What naming rules should I follow for a ReplicationController to avoid issues with Pod hostnames?",
    "answer": "The name of a ReplicationController must be a valid DNS subdomain value, but for best compatibility, it should follow the more restrictive rules for a DNS label.",
    "uuid": "851d57d3-2b7b-435b-8b2a-5e8a979ebe1f",
    "question_with_context": "A user asked the following question:\nQuestion: What naming rules should I follow for a ReplicationController to avoid issues with Pod hostnames?\nThis is about the following runbook:\nRunbook Title: Writing a ReplicationController Manifest\nRunbook Content: Writing a ReplicationController ManifestAs with all other Kubernetes config, a ReplicationController needs `apiVersion`, `kind`, and `metadata` fields.  \nWhen the control plane creates new Pods for a ReplicationController, the `.metadata.name` of the\nReplicationController is part of the basis for naming those Pods.  The name of a ReplicationController must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames.  For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).  \nFor general information about working with configuration files, see [object management](/docs/concepts/overview/working-with-objects/object-management/).  \nA ReplicationController also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\n"
  },
  {
    "question": "What happens if I manually scale a Deployment and then apply a manifest?",
    "answer": "If you manually scale a Deployment using `kubectl scale deployment deployment --replicas=X` and then update that Deployment with a manifest (like running `kubectl apply -f deployment.yaml`), the manifest will overwrite your manual scaling.",
    "uuid": "ecce3ac6-3320-4697-8171-28d4924fcd03",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I manually scale a Deployment and then apply a manifest?\nThis is about the following runbook:\nRunbook Title: Replicas\nRunbook Content: Writing a Deployment SpecReplicas`.spec.replicas` is an optional field that specifies the number of desired Pods. It defaults to 1.  \nShould you manually scale a Deployment, example via `kubectl scale deployment\ndeployment --replicas=X`, and then you update that Deployment based on a manifest\n(for example: by running `kubectl apply -f deployment.yaml`),\nthen applying that manifest overwrites the manual scaling that you previously did.  \nIf a [HorizontalPodAutoscaler](/docs/tasks/run-application/horizontal-pod-autoscale/) (or any\nsimilar API for horizontal scaling) is managing scaling for a Deployment, don't set `.spec.replicas`.  \nInstead, allow the Kubernetes\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} to manage the\n`.spec.replicas` field automatically.\n"
  },
  {
    "question": "Can init containers run alongside the main containers like sidecar containers?",
    "answer": "Unlike sidecar containers, init containers are not continuously running alongside the main containers.",
    "uuid": "54fadf58-20c3-4458-a642-1125c157ce38",
    "question_with_context": "A user asked the following question:\nQuestion: Can init containers run alongside the main containers like sidecar containers?\nThis is about the following runbook:\nRunbook Title: Differences from sidecar containers\nRunbook Content: Understanding init containersDifferences from sidecar containersInit containers run and complete their tasks before the main application container starts.\nUnlike [sidecar containers](/docs/concepts/workloads/pods/sidecar-containers),\ninit containers are not continuously running alongside the main containers.  \nInit containers run to completion sequentially, and the main container does not start\nuntil all the init containers have successfully completed.  \ninit containers do not support `lifecycle`, `livenessProbe`, `readinessProbe`, or\n`startupProbe` whereas sidecar containers support all these [probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe) to control their lifecycle.  \nInit containers share the same resources (CPU, memory, network) with the main application\ncontainers but do not interact directly with them. They can, however, use shared volumes\nfor data exchange.\n"
  },
  {
    "question": "How does resource scheduling work with init containers?",
    "answer": "Scheduling is done based on effective requests/limits, allowing init containers to reserve resources for initialization that are not used during the life of the Pod.",
    "uuid": "58d748da-f188-47b4-b31e-a6b6ccd809b8",
    "question_with_context": "A user asked the following question:\nQuestion: How does resource scheduling work with init containers?\nThis is about the following runbook:\nRunbook Title: Resource sharing within containers\nRunbook Content: Resource sharing within containers{{< comment >}}\nThis section is also present in the [init containers](/docs/concepts/workloads/pods/init-containers/) page.\nIf you're editing this section, change both places.\n{{< /comment >}}  \nGiven the order of execution for init, sidecar and app containers, the following rules\nfor resource usage apply:  \n* The highest of any particular resource request or limit defined on all init\ncontainers is the *effective init request/limit*. If any resource has no\nresource limit specified this is considered as the highest limit.\n* The Pod's *effective request/limit* for a resource is the sum of\n[pod overhead](/docs/concepts/scheduling-eviction/pod-overhead/) and the higher of:\n* the sum of all non-init containers(app and sidecar containers) request/limit for a\nresource\n* the effective init request/limit for a resource\n* Scheduling is done based on effective requests/limits, which means\ninit containers can reserve resources for initialization that are not used\nduring the life of the Pod.\n* The QoS (quality of service) tier of the Pod's *effective QoS tier* is the\nQoS tier for all init, sidecar and app containers alike.  \nQuota and limits are applied based on the effective Pod request and\nlimit.\n"
  },
  {
    "question": "How can I remove Pods from a ReplicaSet for debugging purposes?",
    "answer": "You can remove Pods from a ReplicaSet by changing their labels.",
    "uuid": "4bb0841c-9aaa-4613-8b54-ae0f90a0d5fc",
    "question_with_context": "A user asked the following question:\nQuestion: How can I remove Pods from a ReplicaSet for debugging purposes?\nThis is about the following runbook:\nRunbook Title: Isolating Pods from a ReplicaSet\nRunbook Content: Working with ReplicaSetsIsolating Pods from a ReplicaSetYou can remove Pods from a ReplicaSet by changing their labels. This technique may be used to remove Pods\nfrom service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (\nassuming that the number of replicas is not also changed).\n"
  },
  {
    "question": "How can I check the status of a Terminated container using kubectl?",
    "answer": "You can use `kubectl` to query a Pod with a Terminated container to see the reason, exit code, and the start and finish time for that container's execution.",
    "uuid": "069f7e19-e31a-478d-875b-af2597dbd283",
    "question_with_context": "A user asked the following question:\nQuestion: How can I check the status of a Terminated container using kubectl?\nThis is about the following runbook:\nRunbook Title: `Terminated` {#container-state-terminated}\nRunbook Content: Container states`Terminated` {#container-state-terminated}A container in the `Terminated` state began execution and then either ran to\ncompletion or failed for some reason. When you use `kubectl` to query a Pod with\na container that is `Terminated`, you see a reason, an exit code, and the start and\nfinish time for that container's period of execution.  \nIf a container has a `preStop` hook configured, this hook runs before the container enters\nthe `Terminated` state.\n"
  },
  {
    "question": "What happens if I update the label selector in a Deployment?",
    "answer": "Updating a Deployment's label selector is generally discouraged and should be done with caution. If you do update it, the new selector will not select existing ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and creating a new ReplicaSet.",
    "uuid": "c1e9adb7-e706-4369-84eb-4e82b8b7280f",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I update the label selector in a Deployment?\nThis is about the following runbook:\nRunbook Title: Label selector updates\nRunbook Content: Updating a DeploymentLabel selector updatesIt is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.\nIn any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped\nall of the implications.  \n{{< note >}}\nIn API version `apps/v1`, a Deployment's label selector is immutable after it gets created.\n{{< /note >}}  \n* Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too,\notherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does\nnot select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and\ncreating a new ReplicaSet.\n* Selector updates changes the existing value in a selector key -- result in the same behavior as additions.\n* Selector removals removes an existing key from the Deployment selector -- do not require any changes in the\nPod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the\nremoved label still exists in any existing Pods and ReplicaSets.\n"
  },
  {
    "question": "What Kubernetes version introduced user namespaces as a beta feature?",
    "answer": "User namespaces were introduced as a beta feature in Kubernetes version v1.30.",
    "uuid": "6bf2c3e3-0d16-4007-b2e8-9b01b01b6b3a",
    "question_with_context": "A user asked the following question:\nQuestion: What Kubernetes version introduced user namespaces as a beta feature?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: User Namespaces\nreviewers:\ncontent_type: concept\nweight: 160\nmin-kubernetes-server-version: v1.25\n---  \n<!-- overview -->\n{{< feature-state for_k8s_version=\"v1.30\" state=\"beta\" >}}  \nThis page explains how user namespaces are used in Kubernetes pods. A user\nnamespace isolates the user running inside the container from the one\nin the host.  \nA process running as root in a container can run as a different (non-root) user\nin the host; in other words, the process has full privileges for operations\ninside the user namespace, but is unprivileged for operations outside the\nnamespace.  \nYou can use this feature to reduce the damage a compromised container can do to\nthe host or other pods in the same node. There are [several security\nvulnerabilities][KEP-vulns] rated either **HIGH** or **CRITICAL** that were not\nexploitable when user namespaces is active. It is expected user namespace will\nmitigate some future vulnerabilities too.  \n[KEP-vulns]: https://github.com/kubernetes/enhancements/tree/217d790720c5aef09b8bd4d6ca96284a0affe6c2/keps/sig-node/127-user-namespaces#motivation  \n<!-- body -->\n"
  },
  {
    "question": "Is the pod template schema the same as a regular Pod?",
    "answer": "Yes, the pod template has exactly the same schema as a Pod, but it is nested and does not include an apiVersion or kind.",
    "uuid": "ccc2497b-bfce-4537-87cd-e53a31bbc136",
    "question_with_context": "A user asked the following question:\nQuestion: Is the pod template schema the same as a regular Pod?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a Job specPod TemplateThe `.spec.template` is the only required field of the `.spec`.  \nThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates).\nIt has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}},\nexcept it is nested and does not have an `apiVersion` or `kind`.  \nIn addition to required fields for a Pod, a pod template in a Job must specify appropriate\nlabels (see [pod selector](#pod-selector)) and an appropriate restart policy.  \nOnly a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)\nequal to `Never` or `OnFailure` is allowed.\n"
  },
  {
    "question": "How do I label a job in Kubernetes?",
    "answer": "Job labels will have the `batch.kubernetes.io/` prefix for `job-name` and `controller-uid`.",
    "uuid": "13c6c55a-b9e2-4882-b8ba-820637b8834f",
    "question_with_context": "A user asked the following question:\nQuestion: How do I label a job in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Job Labels\nRunbook Content: Writing a Job specJob LabelsJob labels will have `batch.kubernetes.io/` prefix for `job-name` and `controller-uid`.\n"
  },
  {
    "question": "How long does a volume exist in relation to its Pod?",
    "answer": "A volume exists as long as the specific Pod (with that exact UID) exists.",
    "uuid": "26315af4-40de-4e77-ac03-5f1d35b9e624",
    "question_with_context": "A user asked the following question:\nQuestion: How long does a volume exist in relation to its Pod?\nThis is about the following runbook:\nRunbook Title: Associated lifetimes\nRunbook Content: Pod lifetimeAssociated lifetimesWhen something is said to have the same lifetime as a Pod, such as a\n{{< glossary_tooltip term_id=\"volume\" text=\"volume\" >}},\nthat means that the thing exists as long as that specific Pod (with that exact UID)\nexists. If that Pod is deleted for any reason, and even if an identical replacement\nis created, the related thing (a volume, in this example) is also destroyed and\ncreated anew.  \n{{< figure src=\"/images/docs/pod.svg\" title=\"Figure 1.\" class=\"diagram-medium\" caption=\"A multi-container Pod that contains a file puller [sidecar](/docs/concepts/workloads/pods/sidecar-containers/) and a web server. The Pod uses an [ephemeral `emptyDir` volume](/docs/concepts/storage/volumes/#emptydir) for shared storage between the containers.\" >}}\n"
  },
  {
    "question": "Is there a way to isolate Pods from a ReplicaSet without changing the number of replicas?",
    "answer": "Yes, you can isolate Pods by changing their labels without affecting the number of replicas.",
    "uuid": "4bb0841c-9aaa-4613-8b54-ae0f90a0d5fc",
    "question_with_context": "A user asked the following question:\nQuestion: Is there a way to isolate Pods from a ReplicaSet without changing the number of replicas?\nThis is about the following runbook:\nRunbook Title: Isolating Pods from a ReplicaSet\nRunbook Content: Working with ReplicaSetsIsolating Pods from a ReplicaSetYou can remove Pods from a ReplicaSet by changing their labels. This technique may be used to remove Pods\nfrom service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (\nassuming that the number of replicas is not also changed).\n"
  },
  {
    "question": "What could cause a Pod to restart unexpectedly?",
    "answer": "A Pod can restart if the Pod infrastructure container is restarted, which is uncommon and requires root access to nodes.",
    "uuid": "e83fd9c9-edbf-4bb7-9f52-872dafae4721",
    "question_with_context": "A user asked the following question:\nQuestion: What could cause a Pod to restart unexpectedly?\nThis is about the following runbook:\nRunbook Title: Pod restart reasons\nRunbook Content: Detailed behaviorPod restart reasonsA Pod can restart, causing re-execution of init containers, for the following\nreasons:  \n* The Pod infrastructure container is restarted. This is uncommon and would\nhave to be done by someone with root access to nodes.\n* All containers in a Pod are terminated while `restartPolicy` is set to Always,\nforcing a restart, and the init container completion record has been lost due\nto {{< glossary_tooltip text=\"garbage collection\" term_id=\"garbage-collection\" >}}.  \nThe Pod will not be restarted when the init container image is changed, or the\ninit container completion record has been lost due to garbage collection. This\napplies for Kubernetes v1.20 and later. If you are using an earlier version of\nKubernetes, consult the documentation for the version you are using.\n"
  },
  {
    "question": "How do I scale my nginx deployment to 10 replicas?",
    "answer": "You can scale your nginx deployment to 10 replicas using the command: `kubectl scale deployment/nginx-deployment --replicas=10`. The output will confirm that the deployment has been scaled.",
    "uuid": "b0b9f323-c01a-447a-b28b-21c481252eff",
    "question_with_context": "A user asked the following question:\nQuestion: How do I scale my nginx deployment to 10 replicas?\nThis is about the following runbook:\nRunbook Title: Scaling a Deployment\nRunbook Content: Scaling a DeploymentYou can scale a Deployment by using the following command:  \n```shell\nkubectl scale deployment/nginx-deployment --replicas=10\n```\nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment scaled\n```  \nAssuming [horizontal Pod autoscaling](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) is enabled\nin your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of\nPods you want to run based on the CPU utilization of your existing Pods.  \n```shell\nkubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80\n```\nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment scaled\n```\n"
  },
  {
    "question": "How does resource pressure affect Pods in Kubernetes?",
    "answer": "If a Container exceeds its resource request and the node it runs on faces resource pressure, the Pod becomes a candidate for eviction, leading to the termination of all Containers in that Pod.",
    "uuid": "90985d8c-a459-4ed0-8bb3-3eb837e6836b",
    "question_with_context": "A user asked the following question:\nQuestion: How does resource pressure affect Pods in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Some behavior is independent of QoS class {#class-independent-behavior}\nRunbook Content: Some behavior is independent of QoS class {#class-independent-behavior}Certain behavior is independent of the QoS class assigned by Kubernetes. For example:  \n* Any Container exceeding a resource limit will be killed and restarted by the kubelet without\naffecting other Containers in that Pod.  \n* If a Container exceeds its resource request and the node it runs on faces\nresource pressure, the Pod it is in becomes a candidate for [eviction](/docs/concepts/scheduling-eviction/node-pressure-eviction/).\nIf this occurs, all Containers in the Pod will be terminated. Kubernetes may create a\nreplacement Pod, usually on a different node.  \n* The resource request of a Pod is equal to the sum of the resource requests of\nits component Containers, and the resource limit of a Pod is equal to the sum of\nthe resource limits of its component Containers.  \n* The kube-scheduler does not consider QoS class when selecting which Pods to\n[preempt](/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption).\nPreemption can occur when a cluster does not have enough resources to run all the Pods\nyou defined.\n"
  },
  {
    "question": "Is there a way to disable the pod deletion cost feature?",
    "answer": "Yes, you can disable the pod deletion cost feature by using the `PodDeletionCost` feature gate in both kube-apiserver and kube-controller-manager.",
    "uuid": "fa3e9bee-939f-4b73-81a0-8fdc39ef990e",
    "question_with_context": "A user asked the following question:\nQuestion: Is there a way to disable the pod deletion cost feature?\nThis is about the following runbook:\nRunbook Title: Pod deletion cost\nRunbook Content: Working with ReplicaSetsPod deletion cost{{< feature-state for_k8s_version=\"v1.22\" state=\"beta\" >}}  \nUsing the [`controller.kubernetes.io/pod-deletion-cost`](/docs/reference/labels-annotations-taints/#pod-deletion-cost)\nannotation, users can set a preference regarding which pods to remove first when downscaling a ReplicaSet.  \nThe annotation should be set on the pod, the range is [-2147483648, 2147483647]. It represents the cost of\ndeleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletion\ncost are preferred to be deleted before pods with higher deletion cost.  \nThe implicit value for this annotation for pods that don't set it is 0; negative values are permitted.\nInvalid values will be rejected by the API server.  \nThis feature is beta and enabled by default. You can disable it using the\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n`PodDeletionCost` in both kube-apiserver and kube-controller-manager.  \n{{< note >}}\n- This is honored on a best-effort basis, so it does not offer any guarantees on pod deletion order.\n- Users should avoid updating the annotation frequently, such as updating it based on a metric value,\nbecause doing so will generate a significant number of pod updates on the apiserver.\n{{< /note >}}  \n#### Example Use Case  \nThe different pods of an application could have different utilization levels. On scale down, the application\nmay prefer to remove the pods with lower utilization. To avoid frequently updating the pods, the application\nshould update `controller.kubernetes.io/pod-deletion-cost` once before issuing a scale down (setting the\nannotation to a value proportional to pod utilization level). This works if the application itself controls\nthe down scaling; for example, the driver pod of a Spark deployment.\n"
  },
  {
    "question": "What is a probe in the context of container management?",
    "answer": "A probe is a diagnostic performed periodically by the kubelet on a container.",
    "uuid": "8dd1bdda-0450-4b32-94a1-aae45af7793a",
    "question_with_context": "A user asked the following question:\nQuestion: What is a probe in the context of container management?\nThis is about the following runbook:\nRunbook Title: Container probes\nRunbook Content: Container probesA _probe_ is a diagnostic performed periodically by the [kubelet](/docs/reference/command-line-tools-reference/kubelet/)\non a container. To perform a diagnostic, the kubelet either executes code within the container,\nor makes a network request.\n"
  },
  {
    "question": "Can init containers use lifecycle or probe fields like regular containers?",
    "answer": "No, regular init containers do not support the lifecycle, livenessProbe, readinessProbe, or startupProbe fields.",
    "uuid": "b5ace1a5-bc9c-417b-9a6e-e781dcd3659c",
    "question_with_context": "A user asked the following question:\nQuestion: Can init containers use lifecycle or probe fields like regular containers?\nThis is about the following runbook:\nRunbook Title: Differences from regular containers\nRunbook Content: Understanding init containersDifferences from regular containersInit containers support all the fields and features of app containers,\nincluding resource limits, [volumes](/docs/concepts/storage/volumes/), and security settings. However, the\nresource requests and limits for an init container are handled differently,\nas documented in [Resource sharing within containers](#resource-sharing-within-containers).  \nRegular init containers (in other words: excluding sidecar containers) do not support the\n`lifecycle`, `livenessProbe`, `readinessProbe`, or `startupProbe` fields. Init containers\nmust run to completion before the Pod can be ready; sidecar containers continue running\nduring a Pod's lifetime, and _do_ support some probes. See [sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nfor further details about sidecar containers.  \nIf you specify multiple init containers for a Pod, kubelet runs each init\ncontainer sequentially. Each init container must succeed before the next can run.\nWhen all of the init containers have run to completion, kubelet initializes\nthe application containers for the Pod and runs them as usual.\n"
  },
  {
    "question": "How does a ReplicaSet keep track of the Pods it manages?",
    "answer": "A ReplicaSet is linked to its Pods via the Pods' ownerReferences field, which specifies what resource the current object is owned by. This link allows the ReplicaSet to know the state of the Pods it is maintaining and plan accordingly.",
    "uuid": "3407e455-1281-4f38-bd92-ee35e294d0b5",
    "question_with_context": "A user asked the following question:\nQuestion: How does a ReplicaSet keep track of the Pods it manages?\nThis is about the following runbook:\nRunbook Title: How a ReplicaSet works\nRunbook Content: How a ReplicaSet worksA ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number\nof replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods\nit should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating\nand deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod\ntemplate.  \nA ReplicaSet is linked to its Pods via the Pods' [metadata.ownerReferences](/docs/concepts/architecture/garbage-collection/#owners-dependents)\nfield, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owning\nReplicaSet's identifying information within their ownerReferences field. It's through this link that the ReplicaSet\nknows of the state of the Pods it is maintaining and plans accordingly.  \nA ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no\nOwnerReference or the OwnerReference is not a {{< glossary_tooltip term_id=\"controller\" >}} and it\nmatches a ReplicaSet's selector, it will be immediately acquired by said ReplicaSet.\n"
  },
  {
    "question": "Is there a default value for maxUnavailable, and can it be set to 0?",
    "answer": "The default setting for maxUnavailable is 1, and it cannot be set to 0.",
    "uuid": "a3ea79ab-53cd-423b-9b9c-b3a792971e4b",
    "question_with_context": "A user asked the following question:\nQuestion: Is there a default value for maxUnavailable, and can it be set to 0?\nThis is about the following runbook:\nRunbook Title: Maximum unavailable Pods\nRunbook Content: Rolling UpdatesMaximum unavailable Pods{{< feature-state for_k8s_version=\"v1.24\" state=\"alpha\" >}}  \nYou can control the maximum number of Pods that can be unavailable during an update\nby specifying the `.spec.updateStrategy.rollingUpdate.maxUnavailable` field.\nThe value can be an absolute number (for example, `5`) or a percentage of desired\nPods (for example, `10%`). Absolute number is calculated from the percentage value\nby rounding it up. This field cannot be 0. The default setting is 1.  \nThis field applies to all Pods in the range `0` to `replicas - 1`. If there is any\nunavailable Pod in the range `0` to `replicas - 1`, it will be counted towards\n`maxUnavailable`.  \n{{< note >}}\nThe `maxUnavailable` field is in Alpha stage and it is honored only by API servers\nthat are running with the `MaxUnavailableStatefulSet`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\nenabled.\n{{< /note >}}\n"
  },
  {
    "question": "What container runtimes do I need to use user namespaces with Kubernetes?",
    "answer": "You need to use a CRI container runtime that supports user namespaces, such as containerd version 2.0 (and later) or CRI-O version 1.25 (and later).",
    "uuid": "6bd14f6e-6fb4-4077-b1ab-f5bafe4a6f3e",
    "question_with_context": "A user asked the following question:\nQuestion: What container runtimes do I need to use user namespaces with Kubernetes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"prerequisites\" %}}\nRunbook Content: {{% heading \"prerequisites\" %}}{{% thirdparty-content %}}  \nThis is a Linux-only feature and support is needed in Linux for idmap mounts on\nthe filesystems used. This means:  \n* On the node, the filesystem you use for `/var/lib/kubelet/pods/`, or the\ncustom directory you configure for this, needs idmap mount support.\n* All the filesystems used in the pod's volumes must support idmap mounts.  \nIn practice this means you need at least Linux 6.3, as tmpfs started supporting\nidmap mounts in that version. This is usually needed as several Kubernetes\nfeatures use tmpfs (the service account token that is mounted by default uses a\ntmpfs, Secrets use a tmpfs, etc.)  \nSome popular filesystems that support idmap mounts in Linux 6.3 are: btrfs,\next4, xfs, fat, tmpfs, overlayfs.  \nIn addition, the container runtime and its underlying OCI runtime must support\nuser namespaces. The following OCI runtimes offer support:  \n* [crun](https://github.com/containers/crun) version 1.9 or greater (it's recommend version 1.13+).\n* [runc](https://github.com/opencontainers/runc) version 1.2 or greater  \n{{< note >}}\nSome OCI runtimes do not include the support needed for using user namespaces in\nLinux pods. If you use a managed Kubernetes, or have downloaded it from packages\nand set it up, it's possible that nodes in your cluster use a runtime that doesn't\ninclude this support.\n{{< /note >}}  \nTo use user namespaces with Kubernetes, you also need to use a CRI\n{{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}\nto use this feature with Kubernetes pods:  \n* containerd: version 2.0 (and later) supports user namespaces for containers.\n* CRI-O: version 1.25 (and later) supports user namespaces for containers.  \nYou can see the status of user namespaces support in cri-dockerd tracked in an [issue][CRI-dockerd-issue]\non GitHub.  \n[CRI-dockerd-issue]: https://github.com/Mirantis/cri-dockerd/issues/74\n"
  },
  {
    "question": "How does the kubelet perform a diagnostic on a container?",
    "answer": "The kubelet performs a diagnostic on a container by invoking different actions such as `ExecAction`, `TCPSocketAction`, or `HTTPGetAction`.",
    "uuid": "e0e75efe-999e-4223-8685-43aa64af6fe9",
    "question_with_context": "A user asked the following question:\nQuestion: How does the kubelet perform a diagnostic on a container?\nThis is about the following runbook:\nRunbook Title: Container probes\nRunbook Content: Container probesA _probe_ is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet can invoke different actions:  \n- `ExecAction` (performed with the help of the container runtime)\n- `TCPSocketAction` (checked directly by the kubelet)\n- `HTTPGetAction` (checked directly by the kubelet)  \nYou can read more about [probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)\nin the Pod Lifecycle documentation.\n"
  },
  {
    "question": "What do I need to include in the pod template for a Job?",
    "answer": "In the pod template for a Job, you must include appropriate labels and specify a restart policy.",
    "uuid": "ccc2497b-bfce-4537-87cd-e53a31bbc136",
    "question_with_context": "A user asked the following question:\nQuestion: What do I need to include in the pod template for a Job?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a Job specPod TemplateThe `.spec.template` is the only required field of the `.spec`.  \nThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates).\nIt has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}},\nexcept it is nested and does not have an `apiVersion` or `kind`.  \nIn addition to required fields for a Pod, a pod template in a Job must specify appropriate\nlabels (see [pod selector](#pod-selector)) and an appropriate restart policy.  \nOnly a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)\nequal to `Never` or `OnFailure` is allowed.\n"
  },
  {
    "question": "Can I change the pod selector after creating a DaemonSet?",
    "answer": "No, once a DaemonSet is created, its .spec.selector cannot be mutated.",
    "uuid": "f7bd6825-b5db-4b2c-8afb-ee34971960a9",
    "question_with_context": "A user asked the following question:\nQuestion: Can I change the pod selector after creating a DaemonSet?\nThis is about the following runbook:\nRunbook Title: Pod Selector\nRunbook Content: Writing a DaemonSet SpecPod SelectorThe `.spec.selector` field is a pod selector.  It works the same as the `.spec.selector` of\na [Job](/docs/concepts/workloads/controllers/job/).  \nYou must specify a pod selector that matches the labels of the\n`.spec.template`.\nAlso, once a DaemonSet is created,\nits `.spec.selector` can not be mutated. Mutating the pod selector can lead to the\nunintentional orphaning of Pods, and it was found to be confusing to users.  \nThe `.spec.selector` is an object consisting of two fields:  \n* `matchLabels` - works the same as the `.spec.selector` of a\n[ReplicationController](/docs/concepts/workloads/controllers/replicationcontroller/).\n* `matchExpressions` - allows to build more sophisticated selectors by specifying key,\nlist of values and an operator that relates the key and values.  \nWhen the two are specified the result is ANDed.  \nThe `.spec.selector` must match the `.spec.template.metadata.labels`.\nConfig with these two not matching will be rejected by the API.\n"
  },
  {
    "question": "What are the requirements for a Pod to be classified as Guaranteed?",
    "answer": "For a Pod to be given a QoS class of Guaranteed, every Container in the Pod must have a memory limit and a memory request, with the memory limit equaling the memory request. Additionally, every Container must have a CPU limit and a CPU request, with the CPU limit also equaling the CPU request.",
    "uuid": "5fc3e6b9-e905-4fff-952b-2870c0a4b958",
    "question_with_context": "A user asked the following question:\nQuestion: What are the requirements for a Pod to be classified as Guaranteed?\nThis is about the following runbook:\nRunbook Title: Guaranteed\nRunbook Content: Quality of Service classesGuaranteedPods that are `Guaranteed` have the strictest resource limits and are least likely\nto face eviction. They are guaranteed not to be killed until they exceed their limits\nor there are no lower-priority Pods that can be preempted from the Node. They may\nnot acquire resources beyond their specified limits. These Pods can also make\nuse of exclusive CPUs using the\n[`static`](/docs/tasks/administer-cluster/cpu-management-policies/#static-policy) CPU management policy.  \n#### Criteria  \nFor a Pod to be given a QoS class of `Guaranteed`:  \n* Every Container in the Pod must have a memory limit and a memory request.\n* For every Container in the Pod, the memory limit must equal the memory request.\n* Every Container in the Pod must have a CPU limit and a CPU request.\n* For every Container in the Pod, the CPU limit must equal the CPU request.\n"
  },
  {
    "question": "Do I need to change Pod template labels when adding a new selector?",
    "answer": "Yes, when you add a new selector, you must also update the Pod template labels in the Deployment spec with the new label. If you don't, a validation error will be returned.",
    "uuid": "c1e9adb7-e706-4369-84eb-4e82b8b7280f",
    "question_with_context": "A user asked the following question:\nQuestion: Do I need to change Pod template labels when adding a new selector?\nThis is about the following runbook:\nRunbook Title: Label selector updates\nRunbook Content: Updating a DeploymentLabel selector updatesIt is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.\nIn any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped\nall of the implications.  \n{{< note >}}\nIn API version `apps/v1`, a Deployment's label selector is immutable after it gets created.\n{{< /note >}}  \n* Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too,\notherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does\nnot select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and\ncreating a new ReplicaSet.\n* Selector updates changes the existing value in a selector key -- result in the same behavior as additions.\n* Selector removals removes an existing key from the Deployment selector -- do not require any changes in the\nPod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the\nremoved label still exists in any existing Pods and ReplicaSets.\n"
  },
  {
    "question": "How does kubelet manage multiple init containers in a Pod?",
    "answer": "Kubelet runs each init container sequentially, and each must succeed before the next can run.",
    "uuid": "b5ace1a5-bc9c-417b-9a6e-e781dcd3659c",
    "question_with_context": "A user asked the following question:\nQuestion: How does kubelet manage multiple init containers in a Pod?\nThis is about the following runbook:\nRunbook Title: Differences from regular containers\nRunbook Content: Understanding init containersDifferences from regular containersInit containers support all the fields and features of app containers,\nincluding resource limits, [volumes](/docs/concepts/storage/volumes/), and security settings. However, the\nresource requests and limits for an init container are handled differently,\nas documented in [Resource sharing within containers](#resource-sharing-within-containers).  \nRegular init containers (in other words: excluding sidecar containers) do not support the\n`lifecycle`, `livenessProbe`, `readinessProbe`, or `startupProbe` fields. Init containers\nmust run to completion before the Pod can be ready; sidecar containers continue running\nduring a Pod's lifetime, and _do_ support some probes. See [sidecar container](/docs/concepts/workloads/pods/sidecar-containers/)\nfor further details about sidecar containers.  \nIf you specify multiple init containers for a Pod, kubelet runs each init\ncontainer sequentially. Each init container must succeed before the next can run.\nWhen all of the init containers have run to completion, kubelet initializes\nthe application containers for the Pod and runs them as usual.\n"
  },
  {
    "question": "How do I set up a node to support user namespaces for pods?",
    "answer": "To set up a node to support user namespaces for pods, ensure you have a user `kubelet` in the system, install the `getsubids` binary, and configure subordinate UIDs/GIDs for the `kubelet` user. Make sure the subordinate user ID starts at a multiple of 65536 and is at least 65536, with the count being a multiple of 65536 and at least `65536 x <maxPods>`.",
    "uuid": "2930fca9-8c14-475b-b6c2-66e40d0482ab",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set up a node to support user namespaces for pods?\nThis is about the following runbook:\nRunbook Title: Set up a node to support user namespaces\nRunbook Content: Set up a node to support user namespacesBy default, the kubelet assigns pods UIDs/GIDs above the range 0-65535, based on\nthe assumption that the host's files and processes use UIDs/GIDs within this\nrange, which is standard for most Linux distributions. This approach prevents\nany overlap between the UIDs/GIDs of the host and those of the pods.  \nAvoiding the overlap is important to mitigate the impact of vulnerabilities such\nas [CVE-2021-25741][CVE-2021-25741], where a pod can potentially read arbitrary\nfiles in the host. If the UIDs/GIDs of the pod and the host don't overlap, it is\nlimited what a pod would be able to do: the pod UID/GID won't match the host's\nfile owner/group.  \nThe kubelet can use a custom range for user IDs and group IDs for pods. To\nconfigure a custom range, the node needs to have:  \n* A user `kubelet` in the system (you cannot use any other username here)\n* The binary `getsubids` installed (part of [shadow-utils][shadow-utils]) and\nin the `PATH` for the kubelet binary.\n* A configuration of subordinate UIDs/GIDs for the `kubelet` user (see\n[`man 5 subuid`](https://man7.org/linux/man-pages/man5/subuid.5.html) and\n[`man 5 subgid`](https://man7.org/linux/man-pages/man5/subgid.5.html)).  \nThis setting only gathers the UID/GID range configuration and does not change\nthe user executing the `kubelet`.  \nYou must follow some constraints for the subordinate ID range that you assign\nto the `kubelet` user:  \n* The subordinate user ID, that starts the UID range for Pods, **must** be a\nmultiple of 65536 and must also be greater than or equal to 65536. In other\nwords, you cannot use any ID from the range 0-65535 for Pods; the kubelet\nimposes this restriction to make it difficult to create an accidentally insecure\nconfiguration.  \n* The subordinate ID count must be a multiple of 65536  \n* The subordinate ID count must be at least `65536 x <maxPods>` where `<maxPods>`\nis the maximum number of pods that can run on the node.  \n* You must assign the same range for both user IDs and for group IDs, It doesn't\nmatter if other users have user ID ranges that don't align with the group ID\nranges.  \n* None of the assigned ranges should overlap with any other assignment.  \n* The subordinate configuration must be only one line. In other words, you can't\nhave multiple ranges.  \nFor example, you could define `/etc/subuid` and `/etc/subgid` to both have\nthese entries for the `kubelet` user:  \n```\n# The format is\n#   name:firstID:count of IDs\n# where\n# - firstID is 65536 (the minimum value possible)\n# - count of IDs is 110 (default limit for number of) * 65536\nkubelet:65536:7208960\n```  \n[CVE-2021-25741]: https://github.com/kubernetes/kubernetes/issues/104980\n[shadow-utils]: https://github.com/shadow-maint/shadow\n"
  },
  {
    "question": "When does the kubelet set the `Initialized` condition to True for a Pod?",
    "answer": "For a Pod with init containers, the kubelet sets the `Initialized` condition to `True` after the init containers have successfully completed, which happens after successful sandbox creation and network configuration. For a Pod without init containers, it is set to `True` before sandbox creation and network configuration starts.",
    "uuid": "c54024f5-54fb-483d-a18f-3396e704c97f",
    "question_with_context": "A user asked the following question:\nQuestion: When does the kubelet set the `Initialized` condition to True for a Pod?\nThis is about the following runbook:\nRunbook Title: Pod network readiness {#pod-has-network}\nRunbook Content: Pod conditionsPod network readiness {#pod-has-network}{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \n{{< note >}}\nDuring its early development, this condition was named `PodHasNetwork`.\n{{< /note >}}  \nAfter a Pod gets scheduled on a node, it needs to be admitted by the kubelet and\nto have any required storage volumes mounted. Once these phases are complete,\nthe kubelet works with\na container runtime (using {{< glossary_tooltip term_id=\"cri\" >}}) to set up a\nruntime sandbox and configure networking for the Pod. If the\n`PodReadyToStartContainersCondition`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled\n(it is enabled by default for Kubernetes {{< skew currentVersion >}}), the\n`PodReadyToStartContainers` condition will be added to the `status.conditions` field of a Pod.  \nThe `PodReadyToStartContainers` condition is set to `False` by the Kubelet when it detects a\nPod does not have a runtime sandbox with networking configured. This occurs in\nthe following scenarios:  \n- Early in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox for\nthe Pod using the container runtime.\n- Later in the lifecycle of the Pod, when the Pod sandbox has been destroyed due to either:\n- the node rebooting, without the Pod getting evicted\n- for container runtimes that use virtual machines for isolation, the Pod\nsandbox virtual machine rebooting, which then requires creating a new sandbox and\nfresh container network configuration.  \nThe `PodReadyToStartContainers` condition is set to `True` by the kubelet after the\nsuccessful completion of sandbox creation and network configuration for the Pod\nby the runtime plugin. The kubelet can start pulling container images and create\ncontainers after `PodReadyToStartContainers` condition has been set to `True`.  \nFor a Pod with init containers, the kubelet sets the `Initialized` condition to\n`True` after the init containers have successfully completed (which happens\nafter successful sandbox creation and network configuration by the runtime\nplugin). For a Pod without init containers, the kubelet sets the `Initialized`\ncondition to `True` before sandbox creation and network configuration starts.\n"
  },
  {
    "question": "What does a ReplicationController do in Kubernetes?",
    "answer": "A ReplicationController ensures that a specified number of pod replicas are running at any one time, making sure that a pod or a homogeneous set of pods is always up and available.",
    "uuid": "7e24bd6f-2b68-4c2a-9c56-0786f9738fc0",
    "question_with_context": "A user asked the following question:\nQuestion: What does a ReplicationController do in Kubernetes?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- bprashanth\n- janetkuo\ntitle: ReplicationController\napi_metadata:\n- apiVersion: \"v1\"\nkind: \"ReplicationController\"\ncontent_type: concept\nweight: 90\ndescription: >-\nLegacy API for managing workloads that can scale horizontally.\nSuperseded by the Deployment and ReplicaSet APIs.\n---  \n<!-- overview -->  \n{{< note >}}\nA [`Deployment`](/docs/concepts/workloads/controllers/deployment/) that configures a [`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is now the recommended way to set up replication.\n{{< /note >}}  \nA _ReplicationController_ ensures that a specified number of pod replicas are running at any one\ntime. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is\nalways up and available.  \n<!-- body -->\n"
  },
  {
    "question": "What steps do I need to take to run a stateless application using a Deployment?",
    "answer": "To run a stateless application using a Deployment, you can follow the guide available at [Run a stateless application using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/).",
    "uuid": "533ba39c-41c7-40d7-9ccb-874a5b50b3c6",
    "question_with_context": "A user asked the following question:\nQuestion: What steps do I need to take to run a stateless application using a Deployment?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn more about [Pods](/docs/concepts/workloads/pods).\n* [Run a stateless application using a Deployment](/docs/tasks/run-application/run-stateless-application-deployment/).\n* Read the {{< api-reference page=\"workload-resources/deployment-v1\" >}} to understand the Deployment API.\n* Read about [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) and how\nyou can use it to manage application availability during disruptions.\n* Use kubectl to [create a Deployment](/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/).\n"
  },
  {
    "question": "How should I configure ReplicationPods for my application?",
    "answer": "ReplicationPods created by a ReplicationController should be fungible and semantically identical, though their configurations may become heterogeneous over time.",
    "uuid": "774fd40c-9a28-4f8d-8358-3dd7aacbfe1e",
    "question_with_context": "A user asked the following question:\nQuestion: How should I configure ReplicationPods for my application?\nThis is about the following runbook:\nRunbook Title: Writing programs for Replication\nRunbook Content: Writing programs for ReplicationPods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the [RabbitMQ work queues](https://www.rabbitmq.com/tutorials/tutorial-two-python.html), as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.\n"
  },
  {
    "question": "How can I access Pod-level fields in Kubernetes using the downward API?",
    "answer": "You can pass information from available Pod-level fields using `fieldRef`.",
    "uuid": "6c1fc433-91dd-4560-b46a-714611e8c6df",
    "question_with_context": "A user asked the following question:\nQuestion: How can I access Pod-level fields in Kubernetes using the downward API?\nThis is about the following runbook:\nRunbook Title: Available fields\nRunbook Content: Available fieldsOnly some Kubernetes API fields are available through the downward API. This\nsection lists which fields you can make available.  \nYou can pass information from available Pod-level fields using `fieldRef`.\nAt the API level, the `spec` for a Pod always defines at least one\n[Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container).\nYou can pass information from available Container-level fields using\n`resourceFieldRef`.\n"
  },
  {
    "question": "Can I use any username other than kubelet for setting up user namespaces?",
    "answer": "No, you must use the username `kubelet` for setting up user namespaces; no other username is allowed.",
    "uuid": "2930fca9-8c14-475b-b6c2-66e40d0482ab",
    "question_with_context": "A user asked the following question:\nQuestion: Can I use any username other than kubelet for setting up user namespaces?\nThis is about the following runbook:\nRunbook Title: Set up a node to support user namespaces\nRunbook Content: Set up a node to support user namespacesBy default, the kubelet assigns pods UIDs/GIDs above the range 0-65535, based on\nthe assumption that the host's files and processes use UIDs/GIDs within this\nrange, which is standard for most Linux distributions. This approach prevents\nany overlap between the UIDs/GIDs of the host and those of the pods.  \nAvoiding the overlap is important to mitigate the impact of vulnerabilities such\nas [CVE-2021-25741][CVE-2021-25741], where a pod can potentially read arbitrary\nfiles in the host. If the UIDs/GIDs of the pod and the host don't overlap, it is\nlimited what a pod would be able to do: the pod UID/GID won't match the host's\nfile owner/group.  \nThe kubelet can use a custom range for user IDs and group IDs for pods. To\nconfigure a custom range, the node needs to have:  \n* A user `kubelet` in the system (you cannot use any other username here)\n* The binary `getsubids` installed (part of [shadow-utils][shadow-utils]) and\nin the `PATH` for the kubelet binary.\n* A configuration of subordinate UIDs/GIDs for the `kubelet` user (see\n[`man 5 subuid`](https://man7.org/linux/man-pages/man5/subuid.5.html) and\n[`man 5 subgid`](https://man7.org/linux/man-pages/man5/subgid.5.html)).  \nThis setting only gathers the UID/GID range configuration and does not change\nthe user executing the `kubelet`.  \nYou must follow some constraints for the subordinate ID range that you assign\nto the `kubelet` user:  \n* The subordinate user ID, that starts the UID range for Pods, **must** be a\nmultiple of 65536 and must also be greater than or equal to 65536. In other\nwords, you cannot use any ID from the range 0-65535 for Pods; the kubelet\nimposes this restriction to make it difficult to create an accidentally insecure\nconfiguration.  \n* The subordinate ID count must be a multiple of 65536  \n* The subordinate ID count must be at least `65536 x <maxPods>` where `<maxPods>`\nis the maximum number of pods that can run on the node.  \n* You must assign the same range for both user IDs and for group IDs, It doesn't\nmatter if other users have user ID ranges that don't align with the group ID\nranges.  \n* None of the assigned ranges should overlap with any other assignment.  \n* The subordinate configuration must be only one line. In other words, you can't\nhave multiple ranges.  \nFor example, you could define `/etc/subuid` and `/etc/subgid` to both have\nthese entries for the `kubelet` user:  \n```\n# The format is\n#   name:firstID:count of IDs\n# where\n# - firstID is 65536 (the minimum value possible)\n# - count of IDs is 110 (default limit for number of) * 65536\nkubelet:65536:7208960\n```  \n[CVE-2021-25741]: https://github.com/kubernetes/kubernetes/issues/104980\n[shadow-utils]: https://github.com/shadow-maint/shadow\n"
  },
  {
    "question": "How do I define a job template for a CronJob?",
    "answer": "You define a job template for a CronJob using the `.spec.jobTemplate`, which is required and has the same schema as a Job, but is nested and does not include an `apiVersion` or `kind`.",
    "uuid": "32104032-2881-477e-9ae8-0ce7e581115a",
    "question_with_context": "A user asked the following question:\nQuestion: How do I define a job template for a CronJob?\nThis is about the following runbook:\nRunbook Title: Job template\nRunbook Content: Writing a CronJob specJob templateThe `.spec.jobTemplate` defines a template for the Jobs that the CronJob creates, and it is required.\nIt has exactly the same schema as a [Job](/docs/concepts/workloads/controllers/job/), except that\nit is nested and does not have an `apiVersion` or `kind`.\nYou can specify common metadata for the templated Jobs, such as\n{{< glossary_tooltip text=\"labels\" term_id=\"label\" >}} or\n{{< glossary_tooltip text=\"annotations\" term_id=\"annotation\" >}}.\nFor information about writing a Job `.spec`, see [Writing a Job Spec](/docs/concepts/workloads/controllers/job/#writing-a-job-spec).\n"
  },
  {
    "question": "What is the difference between init containers and sidecar containers?",
    "answer": "Init containers run to completion during Pod initialization, while sidecar containers start before the main application container and continue to run.",
    "uuid": "79fec00c-2e5c-42af-b625-214a7e775a38",
    "question_with_context": "A user asked the following question:\nQuestion: What is the difference between init containers and sidecar containers?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- erictune\ntitle: Init Containers\ncontent_type: concept\nweight: 40\n---  \n<!-- overview -->\nThis page provides an overview of init containers: specialized containers that run\nbefore app containers in a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}.\nInit containers can contain utilities or setup scripts not present in an app image.  \nYou can specify init containers in the Pod specification alongside the `containers`\narray (which describes app containers).  \nIn Kubernetes, a [sidecar container](/docs/concepts/workloads/pods/sidecar-containers/) is a container that\nstarts before the main application container and _continues to run_. This document is about init containers:\ncontainers that run to completion during Pod initialization.  \n<!-- body -->\n"
  },
  {
    "question": "How do I update a Pod in Kubernetes without replacing it?",
    "answer": "You can update some fields of a running Pod in place, but most metadata about a Pod is immutable. You can only change fields like `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds`, or add new entries to `spec.tolerations`.",
    "uuid": "0992d191-58e7-4970-91a0-890b5b2cb360",
    "question_with_context": "A user asked the following question:\nQuestion: How do I update a Pod in Kubernetes without replacing it?\nThis is about the following runbook:\nRunbook Title: Pod update and replacement\nRunbook Content: Pod update and replacementAs mentioned in the previous section, when the Pod template for a workload\nresource is changed, the controller creates new Pods based on the updated\ntemplate instead of updating or patching the existing Pods.  \nKubernetes doesn't prevent you from managing Pods directly. It is possible to\nupdate some fields of a running Pod, in place. However, Pod update operations\nlike\n[`patch`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#patch-pod-v1-core), and\n[`replace`](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#replace-pod-v1-core)\nhave some limitations:  \n- Most of the metadata about a Pod is immutable. For example, you cannot\nchange the `namespace`, `name`, `uid`, or `creationTimestamp` fields;\nthe `generation` field is unique. It only accepts updates that increment the\nfield's current value.\n- If the `metadata.deletionTimestamp` is set, no new entry can be added to the\n`metadata.finalizers` list.\n- Pod updates may not change fields other than `spec.containers[*].image`,\n`spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or\n`spec.tolerations`. For `spec.tolerations`, you can only add new entries.\n- When updating the `spec.activeDeadlineSeconds` field, two types of updates\nare allowed:  \n1. setting the unassigned field to a positive number;\n1. updating the field from a positive number to a smaller, non-negative\nnumber.\n"
  },
  {
    "question": "What occurs if the termination grace period expires before the Pod is fully terminated?",
    "answer": "If the grace period expires, the Pod may enter forced termination, where all remaining containers are terminated simultaneously with a short grace period.",
    "uuid": "fbad93ee-5ef6-4ebf-9ec8-27d1c54dbd1a",
    "question_with_context": "A user asked the following question:\nQuestion: What occurs if the termination grace period expires before the Pod is fully terminated?\nThis is about the following runbook:\nRunbook Title: Pod shutdown and sidecar containers {##termination-with-sidecars}\nRunbook Content: Termination of Pods {#pod-termination}Pod shutdown and sidecar containers {##termination-with-sidecars}If your Pod includes one or more\n[sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\n(init containers with an Always restart policy), the kubelet will delay sending\nthe TERM signal to these sidecar containers until the last main container has fully terminated.\nThe sidecar containers will be terminated in the reverse order they are defined in the Pod spec.\nThis ensures that sidecar containers continue serving the other containers in the Pod until they\nare no longer needed.  \nThis means that slow termination of a main container will also delay the termination of the sidecar containers.\nIf the grace period expires before the termination process is complete, the Pod may enter [forced termination](#pod-termination-beyond-grace-period).\nIn this case, all remaining containers in the Pod will be terminated simultaneously with a short grace period.  \nSimilarly, if the Pod has a `preStop` hook that exceeds the termination grace period, emergency termination may occur.\nIn general, if you have used `preStop` hooks to control the termination order without sidecar containers, you can now\nremove them and allow the kubelet to manage sidecar termination automatically.\n"
  },
  {
    "question": "How do Pod Disruption Budgets help in separating roles?",
    "answer": "Pod Disruption Budgets support the separation of roles by providing an interface between the Cluster Owner and Application Owner roles.",
    "uuid": "09bf4d01-d0fc-43ec-bbf5-1007b6d6c2e5",
    "question_with_context": "A user asked the following question:\nQuestion: How do Pod Disruption Budgets help in separating roles?\nThis is about the following runbook:\nRunbook Title: Separating Cluster Owner and Application Owner Roles\nRunbook Content: Separating Cluster Owner and Application Owner RolesOften, it is useful to think of the Cluster Manager\nand Application Owner as separate roles with limited knowledge\nof each other.   This separation of responsibilities\nmay make sense in these scenarios:  \n- when there are many application teams sharing a Kubernetes cluster, and\nthere is natural specialization of roles\n- when third-party tools or services are used to automate cluster management  \nPod Disruption Budgets support this separation of roles by providing an\ninterface between the roles.  \nIf you do not have such a separation of responsibilities in your organization,\nyou may not need to use Pod Disruption Budgets.\n"
  },
  {
    "question": "How are the ordinal indices assigned to Pods in a StatefulSet?",
    "answer": "Pods in a StatefulSet are assigned unique integer ordinals starting from 0 up to N-1, where N is the number of replicas.",
    "uuid": "23e34f25-6f4e-4d1a-8bc5-f73c23995c0b",
    "question_with_context": "A user asked the following question:\nQuestion: How are the ordinal indices assigned to Pods in a StatefulSet?\nThis is about the following runbook:\nRunbook Title: Ordinal Index\nRunbook Content: Pod IdentityOrdinal IndexFor a StatefulSet with N [replicas](#replicas), each Pod in the StatefulSet\nwill be assigned an integer ordinal, that is unique over the Set. By default,\npods will be assigned ordinals from 0 up through N-1. The StatefulSet controller\nwill also add a pod label with this index: `apps.kubernetes.io/pod-index`.\n"
  },
  {
    "question": "What should I do if my Job Pods are stuck with the tracking finalizer?",
    "answer": "If you observe that Pods from a Job are stuck with the tracking finalizer, refer to the documentation on 'My pod stays terminating' for troubleshooting.",
    "uuid": "33524afd-44f1-48df-9dec-77417ccdfcc2",
    "question_with_context": "A user asked the following question:\nQuestion: What should I do if my Job Pods are stuck with the tracking finalizer?\nThis is about the following runbook:\nRunbook Title: Job tracking with finalizers\nRunbook Content: Advanced usageJob tracking with finalizers{{< feature-state for_k8s_version=\"v1.26\" state=\"stable\" >}}  \nThe control plane keeps track of the Pods that belong to any Job and notices if\nany such Pod is removed from the API server. To do that, the Job controller\ncreates Pods with the finalizer `batch.kubernetes.io/job-tracking`. The\ncontroller removes the finalizer only after the Pod has been accounted for in\nthe Job status, allowing the Pod to be removed by other controllers or users.  \n{{< note >}}\nSee [My pod stays terminating](/docs/tasks/debug/debug-application/debug-pods/) if you\nobserve that pods from a Job are stuck with the tracking finalizer.\n{{< /note >}}\n"
  },
  {
    "question": "What happens if I don't specify a time zone for my CronJob?",
    "answer": "If no time zone is specified, the kube-controller-manager interprets schedules relative to its local time zone.",
    "uuid": "d75c492d-0e14-4cb0-b7d0-47797c58d264",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I don't specify a time zone for my CronJob?\nThis is about the following runbook:\nRunbook Title: Time zones\nRunbook Content: Writing a CronJob specTime zones{{< feature-state for_k8s_version=\"v1.27\" state=\"stable\" >}}  \nFor CronJobs with no time zone specified, the {{< glossary_tooltip term_id=\"kube-controller-manager\" text=\"kube-controller-manager\" >}}\ninterprets schedules relative to its local time zone.  \nYou can specify a time zone for a CronJob by setting `.spec.timeZone` to the name\nof a valid [time zone](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).\nFor example, setting `.spec.timeZone: \"Etc/UTC\"` instructs Kubernetes to interpret\nthe schedule relative to Coordinated Universal Time.  \nA time zone database from the Go standard library is included in the binaries and used as a fallback in case an external database is not available on the system.\n"
  },
  {
    "question": "What happens when a ReplicaSet needs to create new Pods?",
    "answer": "When a ReplicaSet needs to create new Pods, it uses its Pod template to create the Pods necessary to meet the desired number of replicas.",
    "uuid": "3407e455-1281-4f38-bd92-ee35e294d0b5",
    "question_with_context": "A user asked the following question:\nQuestion: What happens when a ReplicaSet needs to create new Pods?\nThis is about the following runbook:\nRunbook Title: How a ReplicaSet works\nRunbook Content: How a ReplicaSet worksA ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number\nof replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods\nit should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating\nand deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod\ntemplate.  \nA ReplicaSet is linked to its Pods via the Pods' [metadata.ownerReferences](/docs/concepts/architecture/garbage-collection/#owners-dependents)\nfield, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owning\nReplicaSet's identifying information within their ownerReferences field. It's through this link that the ReplicaSet\nknows of the state of the Pods it is maintaining and plans accordingly.  \nA ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no\nOwnerReference or the OwnerReference is not a {{< glossary_tooltip term_id=\"controller\" >}} and it\nmatches a ReplicaSet's selector, it will be immediately acquired by said ReplicaSet.\n"
  },
  {
    "question": "What can I do if my deployment has failed?",
    "answer": "You can scale it up/down, roll back to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.",
    "uuid": "73d546f1-e7c9-4a97-824d-70bea90a3839",
    "question_with_context": "A user asked the following question:\nQuestion: What can I do if my deployment has failed?\nThis is about the following runbook:\nRunbook Title: Operating on a failed deployment\nRunbook Content: Deployment statusOperating on a failed deploymentAll actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back\nto a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.\n"
  },
  {
    "question": "What are some use cases for Elastic Indexed Jobs?",
    "answer": "Use cases for elastic Indexed Jobs include batch workloads which require scaling an indexed Job, such as MPI, Horovod, Ray, and PyTorch training jobs.",
    "uuid": "7356f2d2-c66e-4493-b43e-6b35e036c44d",
    "question_with_context": "A user asked the following question:\nQuestion: What are some use cases for Elastic Indexed Jobs?\nThis is about the following runbook:\nRunbook Title: Elastic Indexed Jobs\nRunbook Content: Advanced usageElastic Indexed Jobs{{< feature-state feature_gate_name=\"ElasticIndexedJob\" >}}  \nYou can scale Indexed Jobs up or down by mutating both `.spec.parallelism`\nand `.spec.completions` together such that `.spec.parallelism == .spec.completions`.\nWhen scaling down, Kubernetes removes the Pods with higher indexes.  \nUse cases for elastic Indexed Jobs include batch workloads which require\nscaling an indexed Job, such as MPI, Horovod, Ray, and PyTorch training jobs.\n"
  },
  {
    "question": "What happens if I interrupt the kubectl delete command for a ReplicationController?",
    "answer": "If the kubectl delete command is interrupted, it can be restarted without any issues.",
    "uuid": "97ceb753-fa5d-4a2b-abf4-b2646cacaba2",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I interrupt the kubectl delete command for a ReplicationController?\nThis is about the following runbook:\nRunbook Title: Deleting a ReplicationController and its Pods\nRunbook Content: Working with ReplicationControllersDeleting a ReplicationController and its PodsTo delete a ReplicationController and all its pods, use [`kubectl\ndelete`](/docs/reference/generated/kubectl/kubectl-commands#delete).  Kubectl will scale the ReplicationController to zero and wait\nfor it to delete each pod before deleting the ReplicationController itself.  If this kubectl\ncommand is interrupted, it can be restarted.  \nWhen using the REST API or [client library](/docs/reference/using-api/client-libraries), you need to do the steps explicitly (scale replicas to\n0, wait for pod deletions, then delete the ReplicationController).\n"
  },
  {
    "question": "What happens if the Pod doesn't have a runtime sandbox with networking configured?",
    "answer": "The `PodReadyToStartContainers` condition is set to `False` by the Kubelet when it detects that a Pod does not have a runtime sandbox with networking configured, which can occur early in the Pod's lifecycle or later if the sandbox has been destroyed.",
    "uuid": "c54024f5-54fb-483d-a18f-3396e704c97f",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if the Pod doesn't have a runtime sandbox with networking configured?\nThis is about the following runbook:\nRunbook Title: Pod network readiness {#pod-has-network}\nRunbook Content: Pod conditionsPod network readiness {#pod-has-network}{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \n{{< note >}}\nDuring its early development, this condition was named `PodHasNetwork`.\n{{< /note >}}  \nAfter a Pod gets scheduled on a node, it needs to be admitted by the kubelet and\nto have any required storage volumes mounted. Once these phases are complete,\nthe kubelet works with\na container runtime (using {{< glossary_tooltip term_id=\"cri\" >}}) to set up a\nruntime sandbox and configure networking for the Pod. If the\n`PodReadyToStartContainersCondition`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled\n(it is enabled by default for Kubernetes {{< skew currentVersion >}}), the\n`PodReadyToStartContainers` condition will be added to the `status.conditions` field of a Pod.  \nThe `PodReadyToStartContainers` condition is set to `False` by the Kubelet when it detects a\nPod does not have a runtime sandbox with networking configured. This occurs in\nthe following scenarios:  \n- Early in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox for\nthe Pod using the container runtime.\n- Later in the lifecycle of the Pod, when the Pod sandbox has been destroyed due to either:\n- the node rebooting, without the Pod getting evicted\n- for container runtimes that use virtual machines for isolation, the Pod\nsandbox virtual machine rebooting, which then requires creating a new sandbox and\nfresh container network configuration.  \nThe `PodReadyToStartContainers` condition is set to `True` by the kubelet after the\nsuccessful completion of sandbox creation and network configuration for the Pod\nby the runtime plugin. The kubelet can start pulling container images and create\ncontainers after `PodReadyToStartContainers` condition has been set to `True`.  \nFor a Pod with init containers, the kubelet sets the `Initialized` condition to\n`True` after the init containers have successfully completed (which happens\nafter successful sandbox creation and network configuration by the runtime\nplugin). For a Pod without init containers, the kubelet sets the `Initialized`\ncondition to `True` before sandbox creation and network configuration starts.\n"
  },
  {
    "question": "How do I create Pods without doing it directly?",
    "answer": "You usually don't need to create Pods directly. Instead, create them using workload resources such as Deployment or Job.",
    "uuid": "70f9ac51-d732-4f73-bb9d-e6ce85a78e89",
    "question_with_context": "A user asked the following question:\nQuestion: How do I create Pods without doing it directly?\nThis is about the following runbook:\nRunbook Title: Workload resources for managing pods\nRunbook Content: Using PodsWorkload resources for managing podsUsually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as {{< glossary_tooltip text=\"Deployment\"\nterm_id=\"deployment\" >}} or {{< glossary_tooltip text=\"Job\" term_id=\"job\" >}}.\nIf your Pods need to track state, consider the\n{{< glossary_tooltip text=\"StatefulSet\" term_id=\"statefulset\" >}} resource.  \nEach Pod is meant to run a single instance of a given application. If you want to\nscale your application horizontally (to provide more overall resources by running\nmore instances), you should use multiple Pods, one for each instance. In\nKubernetes, this is typically referred to as _replication_.\nReplicated Pods are usually created and managed as a group by a workload resource\nand its {{< glossary_tooltip text=\"controller\" term_id=\"controller\" >}}.  \nSee [Pods and controllers](#pods-and-controllers) for more information on how\nKubernetes uses workload resources, and their controllers, to implement application\nscaling and auto-healing.  \nPods natively provide two kinds of shared resources for their constituent containers:\n[networking](#pod-networking) and [storage](#pod-storage).\n"
  },
  {
    "question": "What happens if an init container fails to start?",
    "answer": "If an init container fails to start due to the runtime or exits with failure, it is retried according to the Pod's restartPolicy. If the restartPolicy is set to Always, the init containers use restartPolicy OnFailure.",
    "uuid": "d7c7f11c-7e7d-4b21-a0fa-d8e35b9d15e1",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if an init container fails to start?\nThis is about the following runbook:\nRunbook Title: Detailed behavior\nRunbook Content: Detailed behaviorDuring Pod startup, the kubelet delays running init containers until the networking\nand storage are ready. Then the kubelet runs the Pod's init containers in the order\nthey appear in the Pod's spec.  \nEach init container must exit successfully before\nthe next container starts. If a container fails to start due to the runtime or\nexits with failure, it is retried according to the Pod `restartPolicy`. However,\nif the Pod `restartPolicy` is set to Always, the init containers use\n`restartPolicy` OnFailure.  \nA Pod cannot be `Ready` until all init containers have succeeded. The ports on an\ninit container are not aggregated under a Service. A Pod that is initializing\nis in the `Pending` state but should have a condition `Initialized` set to false.  \nIf the Pod [restarts](#pod-restart-reasons), or is restarted, all init containers\nmust execute again.  \nChanges to the init container spec are limited to the container image field.\nDirectly altering the `image` field of  an init container does _not_ restart the\nPod or trigger its recreation. If the Pod has yet to start, that change may\nhave an effect on how the Pod boots up.  \nFor a [pod template](/docs/concepts/workloads/pods/#pod-templates)\nyou can typically change any field for an init container; the impact of making\nthat change depends on where the pod template is used.  \nBecause init containers can be restarted, retried, or re-executed, init container\ncode should be idempotent. In particular, code that writes into any `emptyDir` volume\nshould be prepared for the possibility that an output file already exists.  \nInit containers have all of the fields of an app container. However, Kubernetes\nprohibits `readinessProbe` from being used because init containers cannot\ndefine readiness distinct from completion. This is enforced during validation.  \nUse `activeDeadlineSeconds` on the Pod to prevent init containers from failing forever.\nThe active deadline includes init containers.\nHowever it is recommended to use `activeDeadlineSeconds` only if teams deploy their application\nas a Job, because `activeDeadlineSeconds` has an effect even after initContainer finished.\nThe Pod which is already running correctly would be killed by `activeDeadlineSeconds` if you set.  \nThe name of each app and init container in a Pod must be unique; a\nvalidation error is thrown for any container sharing a name with another.\n"
  },
  {
    "question": "How does a sidecar container relate to the main application container?",
    "answer": "A sidecar container runs alongside the main application container within the same Pod, providing additional functionality without altering the primary application code.",
    "uuid": "5330b7c1-3b64-45be-a905-eb63e7d6f1a0",
    "question_with_context": "A user asked the following question:\nQuestion: How does a sidecar container relate to the main application container?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: Sidecar Containers\ncontent_type: concept\nweight: 50\n---  \n<!-- overview -->\n{{< feature-state for_k8s_version=\"v1.29\" state=\"beta\" >}}  \nSidecar containers are the secondary containers that run along with the main\napplication container within the same {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}.\nThese containers are used to enhance or to extend the functionality of the primary _app\ncontainer_ by providing additional services, or functionality such as logging, monitoring,\nsecurity, or data synchronization, without directly altering the primary application code.  \nTypically, you only have one app container in a Pod. For example, if you have a web\napplication that requires a local webserver, the local webserver is a sidecar and the\nweb application itself is the app container.  \n<!-- body -->\n"
  },
  {
    "question": "What happens to existing Jobs when I modify a CronJob?",
    "answer": "Existing Jobs that have already started continue to run without changes when you modify a CronJob.",
    "uuid": "5a6fe8e3-cb0d-4820-801d-38af84d4820e",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to existing Jobs when I modify a CronJob?\nThis is about the following runbook:\nRunbook Title: Modifying a CronJob\nRunbook Content: CronJob limitations {#cron-job-limitations}Modifying a CronJobBy design, a CronJob contains a template for _new_ Jobs.\nIf you modify an existing CronJob, the changes you make will apply to new Jobs that\nstart to run after your modification is complete. Jobs (and their Pods) that have already\nstarted continue to run without changes.\nThat is, the CronJob does _not_ update existing Jobs, even if those remain running.\n"
  },
  {
    "question": "What do I need to check for a Pod to be considered ready?",
    "answer": "A Pod is evaluated to be ready only when all containers in the Pod are ready and all conditions specified in readinessGates are True.",
    "uuid": "514f835b-89c9-43d0-b173-ae64991608ed",
    "question_with_context": "A user asked the following question:\nQuestion: What do I need to check for a Pod to be considered ready?\nThis is about the following runbook:\nRunbook Title: Status for Pod readiness {#pod-readiness-status}\nRunbook Content: Pod conditionsStatus for Pod readiness {#pod-readiness-status}The `kubectl patch` command does not support patching object status.\nTo set these `status.conditions` for the Pod, applications and\n{{< glossary_tooltip term_id=\"operator-pattern\" text=\"operators\">}} should use\nthe `PATCH` action.\nYou can use a [Kubernetes client library](/docs/reference/using-api/client-libraries/) to\nwrite code that sets custom Pod conditions for Pod readiness.  \nFor a Pod that uses custom conditions, that Pod is evaluated to be ready **only**\nwhen both the following statements apply:  \n* All containers in the Pod are ready.\n* All conditions specified in `readinessGates` are `True`.  \nWhen a Pod's containers are Ready but at least one custom condition is missing or\n`False`, the kubelet sets the Pod's [condition](#pod-conditions) to `ContainersReady`.\n"
  },
  {
    "question": "How do I perform a rolling update using ReplicationController?",
    "answer": "To perform a rolling update, create a new ReplicationController with 1 replica, then scale the new controller up by 1 and the old controller down by 1, one at a time. Finally, delete the old controller after it reaches 0 replicas.",
    "uuid": "a9dd6f9c-b01d-4a1e-ae8a-92b26fa55d0e",
    "question_with_context": "A user asked the following question:\nQuestion: How do I perform a rolling update using ReplicationController?\nThis is about the following runbook:\nRunbook Title: Rolling updates\nRunbook Content: Common usage patternsRolling updatesThe ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.  \nAs explained in [#1353](https://issue.k8s.io/1353), the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.  \nIdeally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.  \nThe two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.\n"
  },
  {
    "question": "How do I set a deadline for my CronJob to start if it misses its scheduled time?",
    "answer": "You can set a deadline for your CronJob by using the `.spec.startingDeadlineSeconds` field, which defines the deadline in whole seconds for starting the Job after it misses its scheduled time.",
    "uuid": "1d2c4dc2-0936-4062-91b6-e04b90c9561b",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set a deadline for my CronJob to start if it misses its scheduled time?\nThis is about the following runbook:\nRunbook Title: Deadline for delayed Job start {#starting-deadline}\nRunbook Content: Writing a CronJob specDeadline for delayed Job start {#starting-deadline}The `.spec.startingDeadlineSeconds` field is optional.\nThis field defines a deadline (in whole seconds) for starting the Job, if that Job misses its scheduled time\nfor any reason.  \nAfter missing the deadline, the CronJob skips that instance of the Job (future occurrences are still scheduled).\nFor example, if you have a backup Job that runs twice a day, you might allow it to start up to 8 hours late,\nbut no later, because a backup taken any later wouldn't be useful: you would instead prefer to wait for\nthe next scheduled run.  \nFor Jobs that miss their configured deadline, Kubernetes treats them as failed Jobs.\nIf you don't specify `startingDeadlineSeconds` for a CronJob, the Job occurrences have no deadline.  \nIf the `.spec.startingDeadlineSeconds` field is set (not null), the CronJob\ncontroller measures the time between when a Job is expected to be created and\nnow. If the difference is higher than that limit, it will skip this execution.  \nFor example, if it is set to `200`, it allows a Job to be created for up to 200\nseconds after the actual schedule.\n"
  },
  {
    "question": "How can I enable user namespaces for Pods in Kubernetes?",
    "answer": "You can enable user namespaces for Pods by using the feature gate `UserNamespacesPodSecurityStandards`. Admins must ensure that user namespaces are enabled by all nodes within the cluster.",
    "uuid": "86053104-f55d-4170-8e02-d5279d907ed4",
    "question_with_context": "A user asked the following question:\nQuestion: How can I enable user namespaces for Pods in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Integration with Pod security admission checks\nRunbook Content: Integration with Pod security admission checks{{< feature-state state=\"alpha\" for_k8s_version=\"v1.29\" >}}  \nFor Linux Pods that enable user namespaces, Kubernetes relaxes the application of\n[Pod Security Standards](/docs/concepts/security/pod-security-standards) in a controlled way.\nThis behavior can be controlled by the [feature\ngate](/docs/reference/command-line-tools-reference/feature-gates/)\n`UserNamespacesPodSecurityStandards`, which allows an early opt-in for end\nusers. Admins have to ensure that user namespaces are enabled by all nodes\nwithin the cluster if using the feature gate.  \nIf you enable the associated feature gate and create a Pod that uses user\nnamespaces, the following fields won't be constrained even in contexts that enforce the\n_Baseline_ or _Restricted_ pod security standard. This behavior does not\npresent a security concern because `root` inside a Pod with user namespaces\nactually refers to the user inside the container, that is never mapped to a\nprivileged user on the host. Here's the list of fields that are **not** checks for Pods in those\ncircumstances:  \n- `spec.securityContext.runAsNonRoot`\n- `spec.containers[*].securityContext.runAsNonRoot`\n- `spec.initContainers[*].securityContext.runAsNonRoot`\n- `spec.ephemeralContainers[*].securityContext.runAsNonRoot`\n- `spec.securityContext.runAsUser`\n- `spec.containers[*].securityContext.runAsUser`\n- `spec.initContainers[*].securityContext.runAsUser`\n- `spec.ephemeralContainers[*].securityContext.runAsUser`\n"
  },
  {
    "question": "What\u2019s the main purpose of a DaemonSet in Kubernetes?",
    "answer": "Use a DaemonSet when it is important that a copy of a Pod always runs on all or certain hosts, especially if it provides node-level functionality that allows other Pods to run correctly on that particular node.",
    "uuid": "688caeb4-c255-463b-92f5-f68bfe6bb305",
    "question_with_context": "A user asked the following question:\nQuestion: What\u2019s the main purpose of a DaemonSet in Kubernetes?\nThis is about the following runbook:\nRunbook Title: Deployments\nRunbook Content: Alternatives to DaemonSetDeploymentsDaemonSets are similar to [Deployments](/docs/concepts/workloads/controllers/deployment/) in that\nthey both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,\nstorage servers).  \nUse a Deployment for stateless services, like frontends, where scaling up and down the\nnumber of replicas and rolling out updates are more important than controlling exactly which host\nthe Pod runs on.  Use a DaemonSet when it is important that a copy of a Pod always run on\nall or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that particular node.  \nFor example, [network plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/) often include a component that runs as a DaemonSet. The DaemonSet component makes sure that the node where it's running has working cluster networking.\n"
  },
  {
    "question": "What happens if a liveness probe fails?",
    "answer": "If the liveness probe fails, the kubelet kills the container, and the container is subjected to its restart policy.",
    "uuid": "d976ddc4-e932-4d9a-b691-089fcfdc239b",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if a liveness probe fails?\nThis is about the following runbook:\nRunbook Title: Types of probe\nRunbook Content: Container probesTypes of probeThe kubelet can optionally perform and react to three kinds of probes on running\ncontainers:  \n`livenessProbe`\n: Indicates whether the container is running. If\nthe liveness probe fails, the kubelet kills the container, and the container\nis subjected to its [restart policy](#restart-policy). If a container does not\nprovide a liveness probe, the default state is `Success`.  \n`readinessProbe`\n: Indicates whether the container is ready to respond to requests.\nIf the readiness probe fails, the endpoints controller removes the Pod's IP\naddress from the endpoints of all Services that match the Pod. The default\nstate of readiness before the initial delay is `Failure`. If a container does\nnot provide a readiness probe, the default state is `Success`.  \n`startupProbe`\n: Indicates whether the application within the container is started.\nAll other probes are disabled if a startup probe is provided, until it succeeds.\nIf the startup probe fails, the kubelet kills the container, and the container\nis subjected to its [restart policy](#restart-policy). If a container does not\nprovide a startup probe, the default state is `Success`.  \nFor more information about how to set up a liveness, readiness, or startup probe,\nsee [Configure Liveness, Readiness and Startup Probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).  \n#### When should you use a liveness probe?  \nIf the process in your container is able to crash on its own whenever it\nencounters an issue or becomes unhealthy, you do not necessarily need a liveness\nprobe; the kubelet will automatically perform the correct action in accordance\nwith the Pod's `restartPolicy`.  \nIf you'd like your container to be killed and restarted if a probe fails, then\nspecify a liveness probe, and specify a `restartPolicy` of Always or OnFailure.  \n#### When should you use a readiness probe?  \nIf you'd like to start sending traffic to a Pod only when a probe succeeds,\nspecify a readiness probe. In this case, the readiness probe might be the same\nas the liveness probe, but the existence of the readiness probe in the spec means\nthat the Pod will start without receiving any traffic and only start receiving\ntraffic after the probe starts succeeding.  \nIf you want your container to be able to take itself down for maintenance, you\ncan specify a readiness probe that checks an endpoint specific to readiness that\nis different from the liveness probe.  \nIf your app has a strict dependency on back-end services, you can implement both\na liveness and a readiness probe. The liveness probe passes when the app itself\nis healthy, but the readiness probe additionally checks that each required\nback-end service is available. This helps you avoid directing traffic to Pods\nthat can only respond with error messages.  \nIf your container needs to work on loading large data, configuration files, or\nmigrations during startup, you can use a\n[startup probe](#when-should-you-use-a-startup-probe). However, if you want to\ndetect the difference between an app that has failed and an app that is still\nprocessing its startup data, you might prefer a readiness probe.  \n{{< note >}}\nIf you want to be able to drain requests when the Pod is deleted, you do not\nnecessarily need a readiness probe; on deletion, the Pod automatically puts itself\ninto an unready state regardless of whether the readiness probe exists.\nThe Pod remains in the unready state while it waits for the containers in the Pod\nto stop.\n{{< /note >}}  \n#### When should you use a startup probe?  \nStartup probes are useful for Pods that have containers that take a long time to\ncome into service. Rather than set a long liveness interval, you can configure\na separate configuration for probing the container as it starts up, allowing\na time longer than the liveness interval would allow.  \nIf your container usually starts in more than\n`initialDelaySeconds + failureThreshold \u00d7 periodSeconds`, you should specify a\nstartup probe that checks the same endpoint as the liveness probe. The default for\n`periodSeconds` is 10s. You should then set its `failureThreshold` high enough to\nallow the container to start, without changing the default values of the liveness\nprobe. This helps to protect against deadlocks.\n"
  },
  {
    "question": "How are resource allocations managed for sidecar containers in Linux?",
    "answer": "Resource allocations for sidecar containers in Linux are based on the effective Pod request and limit, similar to how the scheduler operates.",
    "uuid": "7628a1f1-f2c6-4155-b996-04c612243bc8",
    "question_with_context": "A user asked the following question:\nQuestion: How are resource allocations managed for sidecar containers in Linux?\nThis is about the following runbook:\nRunbook Title: Sidecar containers and Linux cgroups {#cgroups}\nRunbook Content: Resource sharing within containersSidecar containers and Linux cgroups {#cgroups}On Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod\nrequest and limit, the same as the scheduler.\n"
  },
  {
    "question": "Can I use the Pod's name as an identifier in my application?",
    "answer": "Yes, you can use the Pod's name as an identifier by injecting it into a well-known environment variable.",
    "uuid": "63cd54b8-226c-45cd-9bcf-f57e428245f0",
    "question_with_context": "A user asked the following question:\nQuestion: Can I use the Pod's name as an identifier in my application?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\ntitle: Downward API\ncontent_type: concept\nweight: 170\ndescription: >\nThere are two ways to expose Pod and container fields to a running container:\nenvironment variables, and as files that are populated by a special volume type.\nTogether, these two ways of exposing Pod and container fields are called the downward API.\n---  \n<!-- overview -->  \nIt is sometimes useful for a container to have information about itself, without\nbeing overly coupled to Kubernetes. The _downward API_ allows containers to consume\ninformation about themselves or the cluster without using the Kubernetes client\nor API server.  \nAn example is an existing application that assumes a particular well-known\nenvironment variable holds a unique identifier. One possibility is to wrap the\napplication, but that is tedious and error-prone, and it violates the goal of low\ncoupling. A better option would be to use the Pod's name as an identifier, and\ninject the Pod's name into the well-known environment variable.  \nIn Kubernetes, there are two ways to expose Pod and container fields to a running container:  \n* as [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)\n* as [files in a `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)  \nTogether, these two ways of exposing Pod and container fields are called the\n_downward API_.  \n<!-- body -->\n"
  },
  {
    "question": "If a probe result is 'Failure', what does that indicate?",
    "answer": "The container failed the diagnostic.",
    "uuid": "236a7d87-6ea9-4a87-a5be-32713738afa1",
    "question_with_context": "A user asked the following question:\nQuestion: If a probe result is 'Failure', what does that indicate?\nThis is about the following runbook:\nRunbook Title: Probe outcome\nRunbook Content: Container probesProbe outcomeEach probe has one of three results:  \n`Success`\n: The container passed the diagnostic.  \n`Failure`\n: The container failed the diagnostic.  \n`Unknown`\n: The diagnostic failed (no action should be taken, and the kubelet\nwill make further checks).\n"
  },
  {
    "question": "What is the state of the TTL-after-finished feature in Kubernetes?",
    "answer": "The TTL-after-finished feature is stable for Kubernetes version v1.23.",
    "uuid": "3f4edcf6-c8f1-4a3e-a1ea-9c7e025f363e",
    "question_with_context": "A user asked the following question:\nQuestion: What is the state of the TTL-after-finished feature in Kubernetes?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- janetkuo\ntitle: Automatic Cleanup for Finished Jobs\ncontent_type: concept\nweight: 70\ndescription: >-\nA time-to-live mechanism to clean up old Jobs that have finished execution.\n---  \n<!-- overview -->  \n{{< feature-state for_k8s_version=\"v1.23\" state=\"stable\" >}}  \nWhen your Job has finished, it's useful to keep that Job in the API (and not immediately delete the Job)\nso that you can tell whether the Job succeeded or failed.  \nKubernetes' TTL-after-finished {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} provides a\nTTL (time to live) mechanism to limit the lifetime of Job objects that\nhave finished execution.  \n<!-- body -->\n"
  },
  {
    "question": "Can I use any restart policy in the Pod Template?",
    "answer": "No, only a `.spec.template.spec.restartPolicy` equal to `Always` is allowed in the Pod Template, which is also the default if not specified.",
    "uuid": "79d0fde0-3a05-4c97-b7bc-4789e1110272",
    "question_with_context": "A user asked the following question:\nQuestion: Can I use any restart policy in the Pod Template?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a Deployment SpecPod TemplateThe `.spec.template` and `.spec.selector` are the only required fields of the `.spec`.  \nThe `.spec.template` is a [Pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}, except it is nested and does not have an `apiVersion` or `kind`.  \nIn addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate\nlabels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [selector](#selector).  \nOnly a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is\nallowed, which is the default if not specified.\n"
  },
  {
    "question": "How can I learn about Pods in Kubernetes?",
    "answer": "You can learn about Pods by visiting the [Pods documentation](/docs/concepts/workloads/pods).",
    "uuid": "78872c54-c791-4133-a1cf-4eaaee6d6b91",
    "question_with_context": "A user asked the following question:\nQuestion: How can I learn about Pods in Kubernetes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Learn about [static Pods](#static-pods), which are useful for running Kubernetes\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}} components.\n* Find out how to use DaemonSets\n* [Perform a rolling update on a DaemonSet](/docs/tasks/manage-daemon/update-daemon-set/)\n* [Perform a rollback on a DaemonSet](/docs/tasks/manage-daemon/rollback-daemon-set/)\n(for example, if a roll out didn't work how you expected).\n* Understand [how Kubernetes assigns Pods to Nodes](/docs/concepts/scheduling-eviction/assign-pod-node/).\n* Learn about [device plugins](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) and\n[add ons](/docs/concepts/cluster-administration/addons/), which often run as DaemonSets.\n* `DaemonSet` is a top-level resource in the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/daemon-set-v1\" >}}\nobject definition to understand the API for daemon sets.\n"
  },
  {
    "question": "What should I do if my Deployment is stuck due to insufficient quota?",
    "answer": "To address an issue of insufficient quota, you can scale down your Deployment, scale down other controllers you may be running, or increase the quota in your namespace.",
    "uuid": "d55b3908-1a94-43fb-b51c-b44983848999",
    "question_with_context": "A user asked the following question:\nQuestion: What should I do if my Deployment is stuck due to insufficient quota?\nThis is about the following runbook:\nRunbook Title: Failed Deployment\nRunbook Content: Deployment statusFailed DeploymentYour Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur\ndue to some of the following factors:  \n* Insufficient quota\n* Readiness probe failures\n* Image pull errors\n* Insufficient permissions\n* Limit ranges\n* Application runtime misconfiguration  \nOne way you can detect this condition is to specify a deadline parameter in your Deployment spec:\n([`.spec.progressDeadlineSeconds`](#progress-deadline-seconds)). `.spec.progressDeadlineSeconds` denotes the\nnumber of seconds the Deployment controller waits before indicating (in the Deployment status) that the\nDeployment progress has stalled.  \nThe following `kubectl` command sets the spec with `progressDeadlineSeconds` to make the controller report\nlack of progress of a rollout for a Deployment after 10 minutes:  \n```shell\nkubectl patch deployment/nginx-deployment -p '{\"spec\":{\"progressDeadlineSeconds\":600}}'\n```\nThe output is similar to this:\n```\ndeployment.apps/nginx-deployment patched\n```\nOnce the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following\nattributes to the Deployment's `.status.conditions`:  \n* `type: Progressing`\n* `status: \"False\"`\n* `reason: ProgressDeadlineExceeded`  \nThis condition can also fail early and is then set to status value of `\"False\"` due to reasons as `ReplicaSetCreateError`.\nAlso, the deadline is not taken into account anymore once the Deployment rollout completes.  \nSee the [Kubernetes API conventions](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties) for more information on status conditions.  \n{{< note >}}\nKubernetes takes no action on a stalled Deployment other than to report a status condition with\n`reason: ProgressDeadlineExceeded`. Higher level orchestrators can take advantage of it and act accordingly, for\nexample, rollback the Deployment to its previous version.\n{{< /note >}}  \n{{< note >}}\nIf you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline.\nYou can safely pause a Deployment rollout in the middle of a rollout and resume without triggering\nthe condition for exceeding the deadline.\n{{< /note >}}  \nYou may experience transient errors with your Deployments, either due to a low timeout that you have set or\ndue to any other kind of error that can be treated as transient. For example, let's suppose you have\ninsufficient quota. If you describe the Deployment you will notice the following section:  \n```shell\nkubectl describe deployment nginx-deployment\n```\nThe output is similar to this:\n```\n<...>\nConditions:\nType            Status  Reason\n----            ------  ------\nAvailable       True    MinimumReplicasAvailable\nProgressing     True    ReplicaSetUpdated\nReplicaFailure  True    FailedCreate\n<...>\n```  \nIf you run `kubectl get deployment nginx-deployment -o yaml`, the Deployment status is similar to this:  \n```\nstatus:\navailableReplicas: 2\nconditions:\n- lastTransitionTime: 2016-10-04T12:25:39Z\nlastUpdateTime: 2016-10-04T12:25:39Z\nmessage: Replica set \"nginx-deployment-4262182780\" is progressing.\nreason: ReplicaSetUpdated\nstatus: \"True\"\ntype: Progressing\n- lastTransitionTime: 2016-10-04T12:25:42Z\nlastUpdateTime: 2016-10-04T12:25:42Z\nmessage: Deployment has minimum availability.\nreason: MinimumReplicasAvailable\nstatus: \"True\"\ntype: Available\n- lastTransitionTime: 2016-10-04T12:25:39Z\nlastUpdateTime: 2016-10-04T12:25:39Z\nmessage: 'Error creating: pods \"nginx-deployment-4262182780-\" is forbidden: exceeded quota:\nobject-counts, requested: pods=1, used: pods=3, limited: pods=2'\nreason: FailedCreate\nstatus: \"True\"\ntype: ReplicaFailure\nobservedGeneration: 3\nreplicas: 2\nunavailableReplicas: 2\n```  \nEventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the\nreason for the Progressing condition:  \n```\nConditions:\nType            Status  Reason\n----            ------  ------\nAvailable       True    MinimumReplicasAvailable\nProgressing     False   ProgressDeadlineExceeded\nReplicaFailure  True    FailedCreate\n```  \nYou can address an issue of insufficient quota by scaling down your Deployment, by scaling down other\ncontrollers you may be running, or by increasing quota in your namespace. If you satisfy the quota\nconditions and the Deployment controller then completes the Deployment rollout, you'll see the\nDeployment's status update with a successful condition (`status: \"True\"` and `reason: NewReplicaSetAvailable`).  \n```\nConditions:\nType          Status  Reason\n----          ------  ------\nAvailable     True    MinimumReplicasAvailable\nProgressing   True    NewReplicaSetAvailable\n```  \n`type: Available` with `status: \"True\"` means that your Deployment has minimum availability. Minimum availability is dictated\nby the parameters specified in the deployment strategy. `type: Progressing` with `status: \"True\"` means that your Deployment\nis either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum\nrequired new replicas are available (see the Reason of the condition for the particulars - in our case\n`reason: NewReplicaSetAvailable` means that the Deployment is complete).  \nYou can check if a Deployment has failed to progress by using `kubectl rollout status`. `kubectl rollout status`\nreturns a non-zero exit code if the Deployment has exceeded the progression deadline.  \n```shell\nkubectl rollout status deployment/nginx-deployment\n```\nThe output is similar to this:\n```\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nerror: deployment \"nginx\" exceeded its progress deadline\n```\nand the exit status from `kubectl rollout` is 1 (indicating an error):\n```shell\necho $?\n```\n```\n1\n```\n"
  },
  {
    "question": "How do I scale a ReplicaSet up or down?",
    "answer": "You can scale a ReplicaSet up or down by simply updating the `.spec.replicas` field.",
    "uuid": "5ca7f1f4-c5ef-4236-86ce-d61258621a62",
    "question_with_context": "A user asked the following question:\nQuestion: How do I scale a ReplicaSet up or down?\nThis is about the following runbook:\nRunbook Title: Scaling a ReplicaSet\nRunbook Content: Working with ReplicaSetsScaling a ReplicaSetA ReplicaSet can be easily scaled up or down by simply updating the `.spec.replicas` field. The ReplicaSet controller\nensures that a desired number of Pods with a matching label selector are available and operational.  \nWhen scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods to\nprioritize scaling down pods based on the following general algorithm:  \n1. Pending (and unschedulable) pods are scaled down first\n1. If `controller.kubernetes.io/pod-deletion-cost` annotation is set, then\nthe pod with the lower value will come first.\n1. Pods on nodes with more replicas come before pods on nodes with fewer replicas.\n1. If the pods' creation times differ, the pod that was created more recently\ncomes before the older pod (the creation times are bucketed on an integer log scale).  \nIf all of the above match, then selection is random.\n"
  },
  {
    "question": "How do I set memory limits for my container using resourceFieldRef?",
    "answer": "You can set memory limits using `resource: limits.memory` for the memory limit and `resource: requests.memory` for the memory request.",
    "uuid": "3680aa14-032e-40b8-95f6-e57cfc5b282b",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set memory limits for my container using resourceFieldRef?\nThis is about the following runbook:\nRunbook Title: Information available via `resourceFieldRef` {#downwardapi-resourceFieldRef}\nRunbook Content: Available fieldsInformation available via `resourceFieldRef` {#downwardapi-resourceFieldRef}These container-level fields allow you to provide information about\n[requests and limits](/docs/concepts/configuration/manage-resources-containers/#requests-and-limits)\nfor resources such as CPU and memory.  \n`resource: limits.cpu`\n: A container's CPU limit  \n`resource: requests.cpu`\n: A container's CPU request  \n`resource: limits.memory`\n: A container's memory limit  \n`resource: requests.memory`\n: A container's memory request  \n`resource: limits.hugepages-*`\n: A container's hugepages limit  \n`resource: requests.hugepages-*`\n: A container's hugepages request  \n`resource: limits.ephemeral-storage`\n: A container's ephemeral-storage limit  \n`resource: requests.ephemeral-storage`\n: A container's ephemeral-storage request  \n#### Fallback information for resource limits  \nIf CPU and memory limits are not specified for a container, and you use the\ndownward API to try to expose that information, then the\nkubelet defaults to exposing the maximum allocatable value for CPU and memory\nbased on the [node allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)\ncalculation.\n"
  },
  {
    "question": "Which filesystems support idmap mounts in Linux 6.3?",
    "answer": "Some popular filesystems that support idmap mounts in Linux 6.3 are btrfs, ext4, xfs, fat, tmpfs, and overlayfs.",
    "uuid": "6bd14f6e-6fb4-4077-b1ab-f5bafe4a6f3e",
    "question_with_context": "A user asked the following question:\nQuestion: Which filesystems support idmap mounts in Linux 6.3?\nThis is about the following runbook:\nRunbook Title: {{% heading \"prerequisites\" %}}\nRunbook Content: {{% heading \"prerequisites\" %}}{{% thirdparty-content %}}  \nThis is a Linux-only feature and support is needed in Linux for idmap mounts on\nthe filesystems used. This means:  \n* On the node, the filesystem you use for `/var/lib/kubelet/pods/`, or the\ncustom directory you configure for this, needs idmap mount support.\n* All the filesystems used in the pod's volumes must support idmap mounts.  \nIn practice this means you need at least Linux 6.3, as tmpfs started supporting\nidmap mounts in that version. This is usually needed as several Kubernetes\nfeatures use tmpfs (the service account token that is mounted by default uses a\ntmpfs, Secrets use a tmpfs, etc.)  \nSome popular filesystems that support idmap mounts in Linux 6.3 are: btrfs,\next4, xfs, fat, tmpfs, overlayfs.  \nIn addition, the container runtime and its underlying OCI runtime must support\nuser namespaces. The following OCI runtimes offer support:  \n* [crun](https://github.com/containers/crun) version 1.9 or greater (it's recommend version 1.13+).\n* [runc](https://github.com/opencontainers/runc) version 1.2 or greater  \n{{< note >}}\nSome OCI runtimes do not include the support needed for using user namespaces in\nLinux pods. If you use a managed Kubernetes, or have downloaded it from packages\nand set it up, it's possible that nodes in your cluster use a runtime that doesn't\ninclude this support.\n{{< /note >}}  \nTo use user namespaces with Kubernetes, you also need to use a CRI\n{{< glossary_tooltip text=\"container runtime\" term_id=\"container-runtime\" >}}\nto use this feature with Kubernetes pods:  \n* containerd: version 2.0 (and later) supports user namespaces for containers.\n* CRI-O: version 1.25 (and later) supports user namespaces for containers.  \nYou can see the status of user namespaces support in cri-dockerd tracked in an [issue][CRI-dockerd-issue]\non GitHub.  \n[CRI-dockerd-issue]: https://github.com/Mirantis/cri-dockerd/issues/74\n"
  },
  {
    "question": "Can I control the range of ordinals assigned to my Pods?",
    "answer": "Yes, you can control the range of ordinals assigned to your Pods by setting the `.spec.ordinals.start` field along with the number of replicas. Pods will be assigned ordinals from `.spec.ordinals.start` up to `.spec.ordinals.start + .spec.replicas - 1`.",
    "uuid": "8d7b0a8d-77a1-4ba1-9b6b-0137ac3f5447",
    "question_with_context": "A user asked the following question:\nQuestion: Can I control the range of ordinals assigned to my Pods?\nThis is about the following runbook:\nRunbook Title: Start ordinal\nRunbook Content: Pod IdentityStart ordinal{{< feature-state feature_gate_name=\"StatefulSetStartOrdinal\" >}}  \n`.spec.ordinals` is an optional field that allows you to configure the integer\nordinals assigned to each Pod. It defaults to nil. Within the field, you can\nconfigure the following options:  \n* `.spec.ordinals.start`: If the `.spec.ordinals.start` field is set, Pods will\nbe assigned ordinals from `.spec.ordinals.start` up through\n`.spec.ordinals.start + .spec.replicas - 1`.\n"
  },
  {
    "question": "How do I set up a Rolling Update strategy for my Deployment?",
    "answer": "To set up a Rolling Update strategy for your Deployment, specify `.spec.strategy.type` as `RollingUpdate` in your Deployment YAML. You can also define `maxUnavailable` and `maxSurge` to control the rolling update process.",
    "uuid": "70eae8f8-fae9-4520-8557-2a69b5bc576a",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set up a Rolling Update strategy for my Deployment?\nThis is about the following runbook:\nRunbook Title: Strategy\nRunbook Content: Writing a Deployment SpecStrategy`.spec.strategy` specifies the strategy used to replace old Pods by new ones.\n`.spec.strategy.type` can be \"Recreate\" or \"RollingUpdate\". \"RollingUpdate\" is\nthe default value.  \n#### Recreate Deployment  \nAll existing Pods are killed before new ones are created when `.spec.strategy.type==Recreate`.  \n{{< note >}}\nThis will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods\nof the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new\nrevision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the\nreplacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an\n\"at most\" guarantee for your Pods, you should consider using a\n[StatefulSet](/docs/concepts/workloads/controllers/statefulset/).\n{{< /note >}}  \n#### Rolling Update Deployment  \nThe Deployment updates Pods in a rolling update\nfashion when `.spec.strategy.type==RollingUpdate`. You can specify `maxUnavailable` and `maxSurge` to control\nthe rolling update process.  \n##### Max Unavailable  \n`.spec.strategy.rollingUpdate.maxUnavailable` is an optional field that specifies the maximum number\nof Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)\nor a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by\nrounding down. The value cannot be 0 if `.spec.strategy.rollingUpdate.maxSurge` is 0. The default value is 25%.  \nFor example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired\nPods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled\ndown further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available\nat all times during the update is at least 70% of the desired Pods.  \n##### Max Surge  \n`.spec.strategy.rollingUpdate.maxSurge` is an optional field that specifies the maximum number of Pods\nthat can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a\npercentage of desired Pods (for example, 10%). The value cannot be 0 if `MaxUnavailable` is 0. The absolute number\nis calculated from the percentage by rounding up. The default value is 25%.  \nFor example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the\nrolling update starts, such that the total number of old and new Pods does not exceed 130% of desired\nPods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the\ntotal number of Pods running at any time during the update is at most 130% of desired Pods.  \nHere are some Rolling Update Deployment examples that use the `maxUnavailable` and `maxSurge`:  \n{{< tabs name=\"tab_with_md\" >}}\n{{% tab name=\"Max Unavailable\" %}}  \n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nlabels:\napp: nginx\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\nstrategy:\ntype: RollingUpdate\nrollingUpdate:\nmaxUnavailable: 1\n```  \n{{% /tab %}}\n{{% tab name=\"Max Surge\" %}}  \n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nlabels:\napp: nginx\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\nstrategy:\ntype: RollingUpdate\nrollingUpdate:\nmaxSurge: 1\n```  \n{{% /tab %}}\n{{% tab name=\"Hybrid\" %}}  \n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nlabels:\napp: nginx\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\nstrategy:\ntype: RollingUpdate\nrollingUpdate:\nmaxSurge: 1\nmaxUnavailable: 1\n```  \n{{% /tab %}}\n{{< /tabs >}}\n"
  },
  {
    "question": "Is there a way to discover DaemonSets using DNS?",
    "answer": "Yes, you can create a headless service with the same pod selector and then discover DaemonSets using the endpoints resource or by retrieving multiple A records from DNS.",
    "uuid": "1a06bc4a-9ca2-445d-806e-f3de21a64d7c",
    "question_with_context": "A user asked the following question:\nQuestion: Is there a way to discover DaemonSets using DNS?\nThis is about the following runbook:\nRunbook Title: Communicating with Daemon Pods\nRunbook Content: Communicating with Daemon PodsSome possible patterns for communicating with Pods in a DaemonSet are:  \n- **Push**: Pods in the DaemonSet are configured to send updates to another service, such\nas a stats database.  They do not have clients.\n- **NodeIP and Known Port**: Pods in the DaemonSet can use a `hostPort`, so that the pods\nare reachable via the node IPs.\nClients know the list of node IPs somehow, and know the port by convention.\n- **DNS**: Create a [headless service](/docs/concepts/services-networking/service/#headless-services)\nwith the same pod selector, and then discover DaemonSets using the `endpoints`\nresource or retrieve multiple A records from DNS.\n- **Service**: Create a service with the same Pod selector, and use the service to reach a\ndaemon on a random node. (No way to reach specific node.)\n"
  },
  {
    "question": "How do I add init containers to my Pod?",
    "answer": "To specify an init container for a Pod, add the `initContainers` field into the Pod specification as an array of `container` items.",
    "uuid": "7f5730c2-3588-4ef1-a0c8-7361ca75747b",
    "question_with_context": "A user asked the following question:\nQuestion: How do I add init containers to my Pod?\nThis is about the following runbook:\nRunbook Title: Understanding init containers\nRunbook Content: Understanding init containersA {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}} can have multiple containers\nrunning apps within it, but it can also have one or more init containers, which are run\nbefore the app containers are started.  \nInit containers are exactly like regular containers, except:  \n* Init containers always run to completion.\n* Each init container must complete successfully before the next one starts.  \nIf a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.\nHowever, if the Pod has a `restartPolicy` of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.  \nTo specify an init container for a Pod, add the `initContainers` field into\nthe [Pod specification](/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec),\nas an array of `container` items (similar to the app `containers` field and its contents).\nSee [Container](/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container) in the\nAPI reference for more details.  \nThe status of the init containers is returned in `.status.initContainerStatuses`\nfield as an array of the container statuses (similar to the `.status.containerStatuses`\nfield).\n"
  },
  {
    "question": "Can I create Pods that run on specific nodes?",
    "answer": "Yes, it is possible to create Pods directly that specify a particular node to run on, but using a DaemonSet is recommended for better management.",
    "uuid": "09737193-e72c-4bb7-b2ba-0df8fb63e8f1",
    "question_with_context": "A user asked the following question:\nQuestion: Can I create Pods that run on specific nodes?\nThis is about the following runbook:\nRunbook Title: Bare Pods\nRunbook Content: Alternatives to DaemonSetBare PodsIt is possible to create Pods directly which specify a particular node to run on.  However,\na DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of\nnode failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should\nuse a DaemonSet rather than creating individual Pods.\n"
  },
  {
    "question": "What label does the StatefulSet controller add to each Pod?",
    "answer": "The StatefulSet controller adds a pod label with the index: `apps.kubernetes.io/pod-index`.",
    "uuid": "23e34f25-6f4e-4d1a-8bc5-f73c23995c0b",
    "question_with_context": "A user asked the following question:\nQuestion: What label does the StatefulSet controller add to each Pod?\nThis is about the following runbook:\nRunbook Title: Ordinal Index\nRunbook Content: Pod IdentityOrdinal IndexFor a StatefulSet with N [replicas](#replicas), each Pod in the StatefulSet\nwill be assigned an integer ordinal, that is unique over the Set. By default,\npods will be assigned ordinals from 0 up through N-1. The StatefulSet controller\nwill also add a pod label with this index: `apps.kubernetes.io/pod-index`.\n"
  },
  {
    "question": "How do I set the minimum ready seconds for a Pod?",
    "answer": "You can set the minimum ready seconds for a Pod by using the `.spec.minReadySeconds` field in your Deployment Spec.",
    "uuid": "0793bfd6-938a-433f-9da3-b0f4eb6550f0",
    "question_with_context": "A user asked the following question:\nQuestion: How do I set the minimum ready seconds for a Pod?\nThis is about the following runbook:\nRunbook Title: Min Ready Seconds\nRunbook Content: Writing a Deployment SpecMin Ready Seconds`.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly\ncreated Pod should be ready without any of its containers crashing, for it to be considered available.\nThis defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when\na Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).\n"
  },
  {
    "question": "Can two ReplicaSets use the same selector but have different labels?",
    "answer": "Yes, two ReplicaSets can specify the same `.spec.selector` but have different `.spec.template.metadata.labels` and `.spec.template.spec` fields. In this case, each ReplicaSet will ignore the Pods created by the other.",
    "uuid": "cfa73a27-18df-42d8-ab40-6d33b3c3d057",
    "question_with_context": "A user asked the following question:\nQuestion: Can two ReplicaSets use the same selector but have different labels?\nThis is about the following runbook:\nRunbook Title: Pod Selector\nRunbook Content: Writing a ReplicaSet manifestPod SelectorThe `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/). As discussed\n[earlier](#how-a-replicaset-works) these are the labels used to identify potential Pods to acquire. In our\n`frontend.yaml` example, the selector was:  \n```yaml\nmatchLabels:\ntier: frontend\n```  \nIn the ReplicaSet, `.spec.template.metadata.labels` must match `spec.selector`, or it will\nbe rejected by the API.  \n{{< note >}}\nFor 2 ReplicaSets specifying the same `.spec.selector` but different\n`.spec.template.metadata.labels` and `.spec.template.spec` fields, each ReplicaSet ignores the\nPods created by the other ReplicaSet.\n{{< /note >}}\n"
  },
  {
    "question": "What happens to a volume when its associated Pod is deleted?",
    "answer": "When a Pod is deleted, the associated volume is also destroyed and created anew, even if an identical replacement Pod is created.",
    "uuid": "26315af4-40de-4e77-ac03-5f1d35b9e624",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to a volume when its associated Pod is deleted?\nThis is about the following runbook:\nRunbook Title: Associated lifetimes\nRunbook Content: Pod lifetimeAssociated lifetimesWhen something is said to have the same lifetime as a Pod, such as a\n{{< glossary_tooltip term_id=\"volume\" text=\"volume\" >}},\nthat means that the thing exists as long as that specific Pod (with that exact UID)\nexists. If that Pod is deleted for any reason, and even if an identical replacement\nis created, the related thing (a volume, in this example) is also destroyed and\ncreated anew.  \n{{< figure src=\"/images/docs/pod.svg\" title=\"Figure 1.\" class=\"diagram-medium\" caption=\"A multi-container Pod that contains a file puller [sidecar](/docs/concepts/workloads/pods/sidecar-containers/) and a web server. The Pod uses an [ephemeral `emptyDir` volume](/docs/concepts/storage/volumes/#emptydir) for shared storage between the containers.\" >}}\n"
  },
  {
    "question": "What happens if I don't specify a matching Pod Selector in a StatefulSet?",
    "answer": "Failing to specify a matching Pod Selector will result in a validation error during StatefulSet creation.",
    "uuid": "5ad2f4f1-9448-402f-8c0d-7bbb9c95db82",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I don't specify a matching Pod Selector in a StatefulSet?\nThis is about the following runbook:\nRunbook Title: Pod Selector\nRunbook Content: ComponentsPod SelectorYou must set the `.spec.selector` field of a StatefulSet to match the labels of its\n`.spec.template.metadata.labels`. Failing to specify a matching Pod Selector will result in a\nvalidation error during StatefulSet creation.\n"
  },
  {
    "question": "What happens if a container in a Pod fails?",
    "answer": "If one of the containers in the Pod fails, Kubernetes may try to restart that specific container.",
    "uuid": "aeef62fc-6f26-4833-b926-8677af2fb8fc",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if a container in a Pod fails?\nThis is about the following runbook:\nRunbook Title: Pods and fault recovery {#pod-fault-recovery}\nRunbook Content: Pod lifetimePods and fault recovery {#pod-fault-recovery}If one of the containers in the Pod fails, then Kubernetes may try to restart that\nspecific container.\nRead [How Pods handle problems with containers](#container-restarts) to learn more.  \nPods can however fail in a way that the cluster cannot recover from, and in that case\nKubernetes does not attempt to heal the Pod further; instead, Kubernetes deletes the\nPod and relies on other components to provide automatic healing.  \nIf a Pod is scheduled to a {{< glossary_tooltip text=\"node\" term_id=\"node\" >}} and that\nnode then fails, the Pod is treated as unhealthy and Kubernetes eventually deletes the Pod.\nA Pod won't survive an {{< glossary_tooltip text=\"eviction\" term_id=\"eviction\" >}} due to\na lack of resources or Node maintenance.  \nKubernetes uses a higher-level abstraction, called a\n{{< glossary_tooltip term_id=\"controller\" text=\"controller\" >}}, that handles the work of\nmanaging the relatively disposable Pod instances.  \nA given Pod (as defined by a UID) is never \"rescheduled\" to a different node; instead,\nthat Pod can be replaced by a new, near-identical Pod. If you make a replacement Pod, it can\neven have same name (as in `.metadata.name`) that the old Pod had, but the replacement\nwould have a different `.metadata.uid` from the old Pod.  \nKubernetes does not guarantee that a replacement for an existing Pod would be scheduled to\nthe same node as the old Pod that was being replaced.\n"
  },
  {
    "question": "What are some examples of involuntary disruptions in a Kubernetes environment?",
    "answer": "Examples of involuntary disruptions include hardware failure of the physical machine, accidental deletion of a VM by a cluster administrator, cloud provider or hypervisor failure, kernel panic, node disappearance due to network partition, and eviction of a pod due to the node being out-of-resources.",
    "uuid": "bb0d0eb6-2a22-433f-9f88-1e135bbb3f35",
    "question_with_context": "A user asked the following question:\nQuestion: What are some examples of involuntary disruptions in a Kubernetes environment?\nThis is about the following runbook:\nRunbook Title: Voluntary and involuntary disruptions\nRunbook Content: Voluntary and involuntary disruptionsPods do not disappear until someone (a person or a controller) destroys them, or\nthere is an unavoidable hardware or system software error.  \nWe call these unavoidable cases *involuntary disruptions* to\nan application.  Examples are:  \n- a hardware failure of the physical machine backing the node\n- cluster administrator deletes VM (instance) by mistake\n- cloud provider or hypervisor failure makes VM disappear\n- a kernel panic\n- the node disappears from the cluster due to cluster network partition\n- eviction of a pod due to the node being [out-of-resources](/docs/concepts/scheduling-eviction/node-pressure-eviction/).  \nExcept for the out-of-resources condition, all these conditions\nshould be familiar to most users; they are not specific\nto Kubernetes.  \nWe call other cases *voluntary disruptions*.  These include both\nactions initiated by the application owner and those initiated by a Cluster\nAdministrator.  Typical application owner actions include:  \n- deleting the deployment or other controller that manages the pod\n- updating a deployment's pod template causing a restart\n- directly deleting a pod (e.g. by accident)  \nCluster administrator actions include:  \n- [Draining a node](/docs/tasks/administer-cluster/safely-drain-node/) for repair or upgrade.\n- Draining a node from a cluster to scale the cluster down (learn about\n[Cluster Autoscaling](https://github.com/kubernetes/autoscaler/#readme)).\n- Removing a pod from a node to permit something else to fit on that node.  \nThese actions might be taken directly by the cluster administrator, or by automation\nrun by the cluster administrator, or by your cluster hosting provider.  \nAsk your cluster administrator or consult your cloud provider or distribution documentation\nto determine if any sources of voluntary disruptions are enabled for your cluster.\nIf none are enabled, you can skip creating Pod Disruption Budgets.  \n{{< caution >}}\nNot all voluntary disruptions are constrained by Pod Disruption Budgets. For example,\ndeleting deployments or pods bypasses Pod Disruption Budgets.\n{{< /caution >}}\n"
  },
  {
    "question": "Where can I find information on creating a Pod with an init container?",
    "answer": "You can read about creating a Pod that has an init container by visiting the documentation on [creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).",
    "uuid": "97187912-7f92-4a08-8ac9-ee4f6f8614f1",
    "question_with_context": "A user asked the following question:\nQuestion: Where can I find information on creating a Pod with an init container?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn how to [Adopt Sidecar Containers](/docs/tutorials/configuration/pod-sidecar-containers/)\n* Read a blog post on [native sidecar containers](/blog/2023/08/25/native-sidecar-containers/).\n* Read about [creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container).\n* Learn about the [types of probes](/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe): liveness, readiness, startup probe.\n* Learn about [pod overhead](/docs/concepts/scheduling-eviction/pod-overhead/).\n"
  },
  {
    "question": "What should I check before disabling the JobManagedBy feature gate?",
    "answer": "Before disabling the JobManagedBy feature gate, check if there are any jobs with a custom value in the `spec.managedBy` field. If such jobs exist, they may be reconciled by both the built-in Job controller and the external controller, leading to potential conflicts.",
    "uuid": "3dcba3fe-f125-4e33-842a-1ff74e54a233",
    "question_with_context": "A user asked the following question:\nQuestion: What should I check before disabling the JobManagedBy feature gate?\nThis is about the following runbook:\nRunbook Title: Delegation of managing a Job object to external controller\nRunbook Content: Advanced usageDelegation of managing a Job object to external controller{{< feature-state feature_gate_name=\"JobManagedBy\" >}}  \n{{< note >}}\nYou can only set the `managedBy` field on Jobs if you enable the `JobManagedBy`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n(disabled by default).\n{{< /note >}}  \nThis feature allows you to disable the built-in Job controller, for a specific\nJob, and delegate reconciliation of the Job to an external controller.  \nYou indicate the controller that reconciles the Job by setting a custom value\nfor the `spec.managedBy` field - any value\nother than `kubernetes.io/job-controller`. The value of the field is immutable.  \n{{< note >}}\nWhen using this feature, make sure the controller indicated by the field is\ninstalled, otherwise the Job may not be reconciled at all.\n{{< /note >}}  \n{{< note >}}\nWhen developing an external Job controller be aware that your controller needs\nto operate in a fashion conformant with the definitions of the API spec and\nstatus fields of the Job object.  \nPlease review these in detail in the [Job API](/docs/reference/kubernetes-api/workload-resources/job-v1/).\nWe also recommend that you run the e2e conformance tests for the Job object to\nverify your implementation.  \nFinally, when developing an external Job controller make sure it does not use the\n`batch.kubernetes.io/job-tracking` finalizer, reserved for the built-in controller.\n{{< /note >}}  \n{{< warning >}}\nIf you are considering to disable the `JobManagedBy` feature gate, or to\ndowngrade the cluster to a version without the feature gate enabled, check if\nthere are jobs with a custom value of the `spec.managedBy` field. If there\nare such jobs, there is a risk that they might be reconciled by two controllers\nafter the operation: the built-in Job controller and the external controller\nindicated by the field value.\n{{< /warning >}}\n"
  },
  {
    "question": "What does a StatefulSet do in Kubernetes?",
    "answer": "A StatefulSet runs a group of Pods and maintains a sticky identity for each of those Pods.",
    "uuid": "0f0e7590-1bee-4c16-a55f-d65932f6278f",
    "question_with_context": "A user asked the following question:\nQuestion: What does a StatefulSet do in Kubernetes?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- enisoc\n- erictune\n- foxish\n- janetkuo\n- kow3ns\n- smarterclayton\ntitle: StatefulSets\napi_metadata:\n- apiVersion: \"apps/v1\"\nkind: \"StatefulSet\"\ncontent_type: concept\ndescription: >-\nA StatefulSet runs a group of Pods, and maintains a sticky identity for each of those Pods. This is useful for managing\napplications that need persistent storage or a stable, unique network identity.\nweight: 30\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nStatefulSet is the workload API object used to manage stateful applications.  \n{{< glossary_definition term_id=\"statefulset\" length=\"all\" >}}  \n<!-- body -->\n"
  },
  {
    "question": "What does Kubernetes do when a Deployment is marked as progressing?",
    "answer": "When a Deployment is marked as progressing, the Deployment controller adds a condition to the Deployment's .status.conditions with type 'Progressing', status 'True', and a reason such as 'NewReplicaSetCreated', 'FoundNewReplicaSet', or 'ReplicaSetUpdated'.",
    "uuid": "792a42a4-df8b-464e-a996-78dff23c1c9f",
    "question_with_context": "A user asked the following question:\nQuestion: What does Kubernetes do when a Deployment is marked as progressing?\nThis is about the following runbook:\nRunbook Title: Progressing Deployment\nRunbook Content: Deployment statusProgressing DeploymentKubernetes marks a Deployment as _progressing_ when one of the following tasks is performed:  \n* The Deployment creates a new ReplicaSet.\n* The Deployment is scaling up its newest ReplicaSet.\n* The Deployment is scaling down its older ReplicaSet(s).\n* New Pods become ready or available (ready for at least [MinReadySeconds](#min-ready-seconds)).  \nWhen the rollout becomes \u201cprogressing\u201d, the Deployment controller adds a condition with the following\nattributes to the Deployment's `.status.conditions`:  \n* `type: Progressing`\n* `status: \"True\"`\n* `reason: NewReplicaSetCreated` | `reason: FoundNewReplicaSet` | `reason: ReplicaSetUpdated`  \nYou can monitor the progress for a Deployment by using `kubectl rollout status`.\n"
  },
  {
    "question": "What does 'stable' mean in the context of StatefulSets?",
    "answer": "In the context of StatefulSets, 'stable' is synonymous with persistence across Pod (re)scheduling.",
    "uuid": "5ae3f1e9-3483-46e7-a507-7cdb5714be4f",
    "question_with_context": "A user asked the following question:\nQuestion: What does 'stable' mean in the context of StatefulSets?\nThis is about the following runbook:\nRunbook Title: Using StatefulSets\nRunbook Content: Using StatefulSetsStatefulSets are valuable for applications that require one or more of the\nfollowing.  \n* Stable, unique network identifiers.\n* Stable, persistent storage.\n* Ordered, graceful deployment and scaling.\n* Ordered, automated rolling updates.  \nIn the above, stable is synonymous with persistence across Pod (re)scheduling.\nIf an application doesn't require any stable identifiers or ordered deployment,\ndeletion, or scaling, you should deploy your application using a workload object\nthat provides a set of stateless replicas.\n[Deployment](/docs/concepts/workloads/controllers/deployment/) or\n[ReplicaSet](/docs/concepts/workloads/controllers/replicaset/) may be better suited to your stateless needs.\n"
  },
  {
    "question": "How does the termination of sidecar containers work when a Pod is shutting down?",
    "answer": "The kubelet will delay sending the TERM signal to sidecar containers until the last main container has fully terminated. Sidecar containers are terminated in the reverse order they are defined in the Pod spec.",
    "uuid": "fbad93ee-5ef6-4ebf-9ec8-27d1c54dbd1a",
    "question_with_context": "A user asked the following question:\nQuestion: How does the termination of sidecar containers work when a Pod is shutting down?\nThis is about the following runbook:\nRunbook Title: Pod shutdown and sidecar containers {##termination-with-sidecars}\nRunbook Content: Termination of Pods {#pod-termination}Pod shutdown and sidecar containers {##termination-with-sidecars}If your Pod includes one or more\n[sidecar containers](/docs/concepts/workloads/pods/sidecar-containers/)\n(init containers with an Always restart policy), the kubelet will delay sending\nthe TERM signal to these sidecar containers until the last main container has fully terminated.\nThe sidecar containers will be terminated in the reverse order they are defined in the Pod spec.\nThis ensures that sidecar containers continue serving the other containers in the Pod until they\nare no longer needed.  \nThis means that slow termination of a main container will also delay the termination of the sidecar containers.\nIf the grace period expires before the termination process is complete, the Pod may enter [forced termination](#pod-termination-beyond-grace-period).\nIn this case, all remaining containers in the Pod will be terminated simultaneously with a short grace period.  \nSimilarly, if the Pod has a `preStop` hook that exceeds the termination grace period, emergency termination may occur.\nIn general, if you have used `preStop` hooks to control the termination order without sidecar containers, you can now\nremove them and allow the kubelet to manage sidecar termination automatically.\n"
  },
  {
    "question": "Is there a way to set a seccomp profile for my containers?",
    "answer": "Yes, you can set a specific seccomp profile for your containers by configuring it in the `securityContext` field of the Pod specification.",
    "uuid": "941b4842-b6f3-420b-9b87-29458959f86c",
    "question_with_context": "A user asked the following question:\nQuestion: Is there a way to set a seccomp profile for my containers?\nThis is about the following runbook:\nRunbook Title: Pod security settings {#pod-security}\nRunbook Content: Pod security settings {#pod-security}To set security constraints on Pods and containers, you use the\n`securityContext` field in the Pod specification. This field gives you\ngranular control over what a Pod or individual containers can do. For example:  \n* Drop specific Linux capabilities to avoid the impact of a CVE.\n* Force all processes in the Pod to run as a non-root user or as a specific\nuser or group ID.\n* Set a specific seccomp profile.\n* Set Windows security options, such as whether containers run as HostProcess.  \n{{< caution >}}\nYou can also use the Pod securityContext to enable\n[_privileged mode_](/docs/concepts/security/linux-kernel-security-constraints/#privileged-containers)\nin Linux containers. Privileged mode overrides many of the other security\nsettings in the securityContext. Avoid using this setting unless you can't grant\nthe equivalent permissions by using other fields in the securityContext.\nIn Kubernetes 1.26 and later, you can run Windows containers in a similarly\nprivileged mode by setting the `windowsOptions.hostProcess` flag on the\nsecurity context of the Pod spec. For details and instructions, see\n[Create a Windows HostProcess Pod](/docs/tasks/configure-pod-container/create-hostprocess-pod/).\n{{< /caution >}}  \n* To learn about kernel-level security constraints that you can use,\nsee [Linux kernel security constraints for Pods and containers](/docs/concepts/security/linux-kernel-security-constraints).\n* To learn more about the Pod security context, see\n[Configure a Security Context for a Pod or Container](/docs/tasks/configure-pod-container/security-context/).\n"
  },
  {
    "question": "What is the purpose of the Pod Name Label in a StatefulSet?",
    "answer": "The Pod Name Label allows you to attach a Service to a specific Pod in the StatefulSet.",
    "uuid": "6d4be5d1-5504-4838-a321-1aa5fa50c911",
    "question_with_context": "A user asked the following question:\nQuestion: What is the purpose of the Pod Name Label in a StatefulSet?\nThis is about the following runbook:\nRunbook Title: Pod Name Label\nRunbook Content: Pod IdentityPod Name LabelWhen the StatefulSet {{<glossary_tooltip text=\"controller\" term_id=\"controller\">}} creates a Pod,\nit adds a label, `statefulset.kubernetes.io/pod-name`, that is set to the name of\nthe Pod. This label allows you to attach a Service to a specific Pod in\nthe StatefulSet.\n"
  },
  {
    "question": "What command do I use to check the rollout status of a specific deployment?",
    "answer": "You can use the command `kubectl rollout status deployment/nginx-deployment` to check the rollout status of a specific deployment.",
    "uuid": "ec777193-0c02-4298-af6c-786b43f49577",
    "question_with_context": "A user asked the following question:\nQuestion: What command do I use to check the rollout status of a specific deployment?\nThis is about the following runbook:\nRunbook Title: Complete Deployment\nRunbook Content: Deployment statusComplete DeploymentKubernetes marks a Deployment as _complete_ when it has the following characteristics:  \n* All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any\nupdates you've requested have been completed.\n* All of the replicas associated with the Deployment are available.\n* No old replicas for the Deployment are running.  \nWhen the rollout becomes \u201ccomplete\u201d, the Deployment controller sets a condition with the following\nattributes to the Deployment's `.status.conditions`:  \n* `type: Progressing`\n* `status: \"True\"`\n* `reason: NewReplicaSetAvailable`  \nThis `Progressing` condition will retain a status value of `\"True\"` until a new rollout\nis initiated. The condition holds even when availability of replicas changes (which\ndoes instead affect the `Available` condition).  \nYou can check if a Deployment has completed by using `kubectl rollout status`. If the rollout completed\nsuccessfully, `kubectl rollout status` returns a zero exit code.  \n```shell\nkubectl rollout status deployment/nginx-deployment\n```\nThe output is similar to this:\n```\nWaiting for rollout to finish: 2 of 3 updated replicas are available...\ndeployment \"nginx-deployment\" successfully rolled out\n```\nand the exit status from `kubectl rollout` is 0 (success):\n```shell\necho $?\n```\n```\n0\n```\n"
  },
  {
    "question": "What are the ways to expose container information using the downward API?",
    "answer": "You can expose container information using the downward API either as environment variables or as files in a downward API volume.",
    "uuid": "c454605b-ff93-4d73-819a-c40392a79685",
    "question_with_context": "A user asked the following question:\nQuestion: What are the ways to expose container information using the downward API?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}You can read about [`downwardAPI` volumes](/docs/concepts/storage/volumes/#downwardapi).  \nYou can try using the downward API to expose container- or Pod-level information:\n* as [environment variables](/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)\n* as [files in `downwardAPI` volume](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)\n"
  },
  {
    "question": "What happens to a paused Deployment when I change the PodTemplateSpec?",
    "answer": "If a Deployment is paused, any changes to the PodTemplateSpec will not trigger new rollouts.",
    "uuid": "d1f97aa8-56e3-40a5-b73b-f81f7fbe5860",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to a paused Deployment when I change the PodTemplateSpec?\nThis is about the following runbook:\nRunbook Title: Paused\nRunbook Content: Writing a Deployment SpecPaused`.spec.paused` is an optional boolean field for pausing and resuming a Deployment. The only difference between\na paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused\nDeployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when\nit is created.\n"
  },
  {
    "question": "How do I handle pod failures in Kubernetes?",
    "answer": "You can configure handling of retriable and non-retriable pod failures using `podFailurePolicy`, based on the step-by-step [examples](/docs/tasks/job/pod-failure-policy/).",
    "uuid": "35b4d1bc-77fd-4259-a174-980eae0bd273",
    "question_with_context": "A user asked the following question:\nQuestion: How do I handle pod failures in Kubernetes?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\n* Read about different ways of running Jobs:\n* [Coarse Parallel Processing Using a Work Queue](/docs/tasks/job/coarse-parallel-processing-work-queue/)\n* [Fine Parallel Processing Using a Work Queue](/docs/tasks/job/fine-parallel-processing-work-queue/)\n* Use an [indexed Job for parallel processing with static work assignment](/docs/tasks/job/indexed-parallel-processing-static/)\n* Create multiple Jobs based on a template: [Parallel Processing using Expansions](/docs/tasks/job/parallel-processing-expansion/)\n* Follow the links within [Clean up finished jobs automatically](#clean-up-finished-jobs-automatically)\nto learn more about how your cluster can clean up completed and / or failed tasks.\n* `Job` is part of the Kubernetes REST API.\nRead the {{< api-reference page=\"workload-resources/job-v1\" >}}\nobject definition to understand the API for jobs.\n* Read about [`CronJob`](/docs/concepts/workloads/controllers/cron-jobs/), which you\ncan use to define a series of Jobs that will run based on a schedule, similar to\nthe UNIX tool `cron`.\n* Practice how to configure handling of retriable and non-retriable pod failures\nusing `podFailurePolicy`, based on the step-by-step [examples](/docs/tasks/job/pod-failure-policy/).\n"
  },
  {
    "question": "What happens to the pods I remove from a ReplicationController's target set?",
    "answer": "Pods that are removed will be replaced automatically, assuming the number of replicas is not changed.",
    "uuid": "91779434-b52a-40d9-a29d-643d87afe40d",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to the pods I remove from a ReplicationController's target set?\nThis is about the following runbook:\nRunbook Title: Isolating pods from a ReplicationController\nRunbook Content: Working with ReplicationControllersIsolating pods from a ReplicationControllerPods may be removed from a ReplicationController's target set by changing their labels. This technique may be used to remove pods from service for debugging and data recovery. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).\n"
  },
  {
    "question": "How do I define labels in a Pod Template for a ReplicaSet?",
    "answer": "You need to include labels in the `.spec.template` section of your ReplicaSet manifest. For example, you can add a label like `tier: frontend`.",
    "uuid": "f5fef35e-c8c9-4faf-91b0-30b8c16870df",
    "question_with_context": "A user asked the following question:\nQuestion: How do I define labels in a Pod Template for a ReplicaSet?\nThis is about the following runbook:\nRunbook Title: Pod Template\nRunbook Content: Writing a ReplicaSet manifestPod TemplateThe `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates) which is also\nrequired to have labels in place. In our `frontend.yaml` example we had one label: `tier: frontend`.\nBe careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.  \nFor the template's [restart policy](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) field,\n`.spec.template.spec.restartPolicy`, the only allowed value is `Always`, which is the default.\n"
  },
  {
    "question": "How do I create stable storage for my StatefulSet using Volume Claim Templates?",
    "answer": "You can set the `.spec.volumeClaimTemplates` field to create a PersistentVolumeClaim, which will provide stable storage to the StatefulSet.",
    "uuid": "d8e5a620-bc12-4764-83a5-c62b0dbe00e5",
    "question_with_context": "A user asked the following question:\nQuestion: How do I create stable storage for my StatefulSet using Volume Claim Templates?\nThis is about the following runbook:\nRunbook Title: Volume Claim Templates\nRunbook Content: ComponentsVolume Claim TemplatesYou can set the `.spec.volumeClaimTemplates` field to create a\n[PersistentVolumeClaim](/docs/concepts/storage/persistent-volumes/).\nThis will provide stable storage to the StatefulSet if either  \n* The StorageClass specified for the volume claim is set up to use [dynamic\nprovisioning](/docs/concepts/storage/dynamic-provisioning/), or\n* The cluster already contains a PersistentVolume with the correct StorageClass\nand sufficient available storage space.\n"
  },
  {
    "question": "What happens if I update a Deployment while it's already rolling out?",
    "answer": "If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet for the update and starts scaling it up, while rolling over the previously scaling ReplicaSet by adding it to the list of old ReplicaSets and starting to scale it down.",
    "uuid": "e2035b3d-80a6-4c2c-8845-fb7362ed4185",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I update a Deployment while it's already rolling out?\nThis is about the following runbook:\nRunbook Title: Rollover (aka multiple updates in-flight)\nRunbook Content: Updating a DeploymentRollover (aka multiple updates in-flight)Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up\nthe desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels\nmatch `.spec.selector` but whose template does not match `.spec.template` are scaled down. Eventually, the new\nReplicaSet is scaled to `.spec.replicas` and all old ReplicaSets is scaled to 0.  \nIf you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet\nas per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously\n-- it will add it to its list of old ReplicaSets and start scaling it down.  \nFor example, suppose you create a Deployment to create 5 replicas of `nginx:1.14.2`,\nbut then update the Deployment to create 5 replicas of `nginx:1.16.1`, when only 3\nreplicas of `nginx:1.14.2` had been created. In that case, the Deployment immediately starts\nkilling the 3 `nginx:1.14.2` Pods that it had created, and starts creating\n`nginx:1.16.1` Pods. It does not wait for the 5 replicas of `nginx:1.14.2` to be created\nbefore changing course.\n"
  },
  {
    "question": "What fields can I use with resourceFieldRef to manage CPU limits in my container?",
    "answer": "You can use `resource: limits.cpu` for a container's CPU limit and `resource: requests.cpu` for a container's CPU request.",
    "uuid": "3680aa14-032e-40b8-95f6-e57cfc5b282b",
    "question_with_context": "A user asked the following question:\nQuestion: What fields can I use with resourceFieldRef to manage CPU limits in my container?\nThis is about the following runbook:\nRunbook Title: Information available via `resourceFieldRef` {#downwardapi-resourceFieldRef}\nRunbook Content: Available fieldsInformation available via `resourceFieldRef` {#downwardapi-resourceFieldRef}These container-level fields allow you to provide information about\n[requests and limits](/docs/concepts/configuration/manage-resources-containers/#requests-and-limits)\nfor resources such as CPU and memory.  \n`resource: limits.cpu`\n: A container's CPU limit  \n`resource: requests.cpu`\n: A container's CPU request  \n`resource: limits.memory`\n: A container's memory limit  \n`resource: requests.memory`\n: A container's memory request  \n`resource: limits.hugepages-*`\n: A container's hugepages limit  \n`resource: requests.hugepages-*`\n: A container's hugepages request  \n`resource: limits.ephemeral-storage`\n: A container's ephemeral-storage limit  \n`resource: requests.ephemeral-storage`\n: A container's ephemeral-storage request  \n#### Fallback information for resource limits  \nIf CPU and memory limits are not specified for a container, and you use the\ndownward API to try to expose that information, then the\nkubelet defaults to exposing the maximum allocatable value for CPU and memory\nbased on the [node allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable)\ncalculation.\n"
  },
  {
    "question": "What happens to old Pods when I use the Recreate strategy?",
    "answer": "When you use the Recreate strategy, all existing Pods are killed before new ones are created. This means that during an upgrade, all Pods of the old revision will be terminated immediately before any new Pods are created.",
    "uuid": "70eae8f8-fae9-4520-8557-2a69b5bc576a",
    "question_with_context": "A user asked the following question:\nQuestion: What happens to old Pods when I use the Recreate strategy?\nThis is about the following runbook:\nRunbook Title: Strategy\nRunbook Content: Writing a Deployment SpecStrategy`.spec.strategy` specifies the strategy used to replace old Pods by new ones.\n`.spec.strategy.type` can be \"Recreate\" or \"RollingUpdate\". \"RollingUpdate\" is\nthe default value.  \n#### Recreate Deployment  \nAll existing Pods are killed before new ones are created when `.spec.strategy.type==Recreate`.  \n{{< note >}}\nThis will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods\nof the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new\nrevision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the\nreplacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an\n\"at most\" guarantee for your Pods, you should consider using a\n[StatefulSet](/docs/concepts/workloads/controllers/statefulset/).\n{{< /note >}}  \n#### Rolling Update Deployment  \nThe Deployment updates Pods in a rolling update\nfashion when `.spec.strategy.type==RollingUpdate`. You can specify `maxUnavailable` and `maxSurge` to control\nthe rolling update process.  \n##### Max Unavailable  \n`.spec.strategy.rollingUpdate.maxUnavailable` is an optional field that specifies the maximum number\nof Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)\nor a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by\nrounding down. The value cannot be 0 if `.spec.strategy.rollingUpdate.maxSurge` is 0. The default value is 25%.  \nFor example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired\nPods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled\ndown further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available\nat all times during the update is at least 70% of the desired Pods.  \n##### Max Surge  \n`.spec.strategy.rollingUpdate.maxSurge` is an optional field that specifies the maximum number of Pods\nthat can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a\npercentage of desired Pods (for example, 10%). The value cannot be 0 if `MaxUnavailable` is 0. The absolute number\nis calculated from the percentage by rounding up. The default value is 25%.  \nFor example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the\nrolling update starts, such that the total number of old and new Pods does not exceed 130% of desired\nPods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the\ntotal number of Pods running at any time during the update is at most 130% of desired Pods.  \nHere are some Rolling Update Deployment examples that use the `maxUnavailable` and `maxSurge`:  \n{{< tabs name=\"tab_with_md\" >}}\n{{% tab name=\"Max Unavailable\" %}}  \n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nlabels:\napp: nginx\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\nstrategy:\ntype: RollingUpdate\nrollingUpdate:\nmaxUnavailable: 1\n```  \n{{% /tab %}}\n{{% tab name=\"Max Surge\" %}}  \n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nlabels:\napp: nginx\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\nstrategy:\ntype: RollingUpdate\nrollingUpdate:\nmaxSurge: 1\n```  \n{{% /tab %}}\n{{% tab name=\"Hybrid\" %}}  \n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nlabels:\napp: nginx\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\nstrategy:\ntype: RollingUpdate\nrollingUpdate:\nmaxSurge: 1\nmaxUnavailable: 1\n```  \n{{% /tab %}}\n{{< /tabs >}}\n"
  },
  {
    "question": "How can I debug pods using ephemeral containers?",
    "answer": "You can learn how to debug pods using ephemeral containers by following the instructions in the documentation on debugging applications.",
    "uuid": "23914f3d-3b07-40a8-8966-a31b8ad1acd1",
    "question_with_context": "A user asked the following question:\nQuestion: How can I debug pods using ephemeral containers?\nThis is about the following runbook:\nRunbook Title: {{% heading \"whatsnext\" %}}\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn how to [debug pods using ephemeral containers](/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container).\n"
  },
  {
    "question": "How can I set the pod deletion cost for my ReplicaSet pods?",
    "answer": "You can set the pod deletion cost by using the `controller.kubernetes.io/pod-deletion-cost` annotation on the pod. The value should be in the range of [-2147483648, 2147483647], where lower values indicate a preference for deletion before higher values.",
    "uuid": "fa3e9bee-939f-4b73-81a0-8fdc39ef990e",
    "question_with_context": "A user asked the following question:\nQuestion: How can I set the pod deletion cost for my ReplicaSet pods?\nThis is about the following runbook:\nRunbook Title: Pod deletion cost\nRunbook Content: Working with ReplicaSetsPod deletion cost{{< feature-state for_k8s_version=\"v1.22\" state=\"beta\" >}}  \nUsing the [`controller.kubernetes.io/pod-deletion-cost`](/docs/reference/labels-annotations-taints/#pod-deletion-cost)\nannotation, users can set a preference regarding which pods to remove first when downscaling a ReplicaSet.  \nThe annotation should be set on the pod, the range is [-2147483648, 2147483647]. It represents the cost of\ndeleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletion\ncost are preferred to be deleted before pods with higher deletion cost.  \nThe implicit value for this annotation for pods that don't set it is 0; negative values are permitted.\nInvalid values will be rejected by the API server.  \nThis feature is beta and enabled by default. You can disable it using the\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)\n`PodDeletionCost` in both kube-apiserver and kube-controller-manager.  \n{{< note >}}\n- This is honored on a best-effort basis, so it does not offer any guarantees on pod deletion order.\n- Users should avoid updating the annotation frequently, such as updating it based on a metric value,\nbecause doing so will generate a significant number of pod updates on the apiserver.\n{{< /note >}}  \n#### Example Use Case  \nThe different pods of an application could have different utilization levels. On scale down, the application\nmay prefer to remove the pods with lower utilization. To avoid frequently updating the pods, the application\nshould update `controller.kubernetes.io/pod-deletion-cost` once before issuing a scale down (setting the\nannotation to a value proportional to pod utilization level). This works if the application itself controls\nthe down scaling; for example, the driver pod of a Spark deployment.\n"
  },
  {
    "question": "What do sidecar containers do in relation to app containers?",
    "answer": "Sidecar containers run alongside app containers in the same pod and provide supporting functionality to the main application, but they do not execute the primary application logic.",
    "uuid": "ac505511-45f5-441b-9191-697222c146d6",
    "question_with_context": "A user asked the following question:\nQuestion: What do sidecar containers do in relation to app containers?\nThis is about the following runbook:\nRunbook Title: Differences from application containers\nRunbook Content: Differences from application containersSidecar containers run alongside _app containers_ in the same pod. However, they do not\nexecute the primary application logic; instead, they provide supporting functionality to\nthe main application.  \nSidecar containers have their own independent lifecycles. They can be started, stopped,\nand restarted independently of app containers. This means you can update, scale, or\nmaintain sidecar containers without affecting the primary application.  \nSidecar containers share the same network and storage namespaces with the primary\ncontainer. This co-location allows them to interact closely and share resources.  \nFrom Kubernetes perspective, sidecars graceful termination is less important.\nWhen other containers took all alloted graceful termination time, sidecar containers\nwill receive the `SIGTERM` following with `SIGKILL` faster than may be expected.\nSo exit codes different from `0` (`0` indicates successful exit), for sidecar containers are normal\non Pod termination and should be generally ignored by the external tooling.\n"
  },
  {
    "question": "Can you give me an example of a Job with a sidecar container?",
    "answer": "Yes, you can refer to the example provided in the job-sidecar.yaml file for a Job with two containers, one of which is a sidecar.",
    "uuid": "cea747b2-14bf-46e4-8b81-84de7311f981",
    "question_with_context": "A user asked the following question:\nQuestion: Can you give me an example of a Job with a sidecar container?\nThis is about the following runbook:\nRunbook Title: Jobs with sidecar containers\nRunbook Content: Sidecar containers and Pod lifecycleJobs with sidecar containersIf you define a Job that uses sidecar using Kubernetes-style init containers,\nthe sidecar container in each Pod does not prevent the Job from completing after the\nmain container has finished.  \nHere's an example of a Job with two containers, one of which is a sidecar:  \n{{% code_sample language=\"yaml\" file=\"application/job/job-sidecar.yaml\" %}}\n"
  },
  {
    "question": "How do I create a PodDisruptionBudget for my application?",
    "answer": "As an application owner, you can create a PodDisruptionBudget (PDB) for each application to limit the number of Pods of a replicated application that are down simultaneously from voluntary disruptions.",
    "uuid": "406c8f76-ccbd-4a8a-a0a6-ebce8ce90ef2",
    "question_with_context": "A user asked the following question:\nQuestion: How do I create a PodDisruptionBudget for my application?\nThis is about the following runbook:\nRunbook Title: Pod disruption budgets\nRunbook Content: Pod disruption budgets{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}  \nKubernetes offers features to help you run highly available applications even when you\nintroduce frequent voluntary disruptions.  \nAs an application owner, you can create a PodDisruptionBudget (PDB) for each application.\nA PDB limits the number of Pods of a replicated application that are down simultaneously from\nvoluntary disruptions. For example, a quorum-based application would\nlike to ensure that the number of replicas running is never brought below the\nnumber needed for a quorum. A web front end might want to\nensure that the number of replicas serving load never falls below a certain\npercentage of the total.  \nCluster managers and hosting providers should use tools which\nrespect PodDisruptionBudgets by calling the [Eviction API](/docs/tasks/administer-cluster/safely-drain-node/#eviction-api)\ninstead of directly deleting pods or deployments.  \nFor example, the `kubectl drain` subcommand lets you mark a node as going out of\nservice. When you run `kubectl drain`, the tool tries to evict all of the Pods on\nthe Node you're taking out of service. The eviction request that `kubectl` submits on\nyour behalf may be temporarily rejected, so the tool periodically retries all failed\nrequests until all Pods on the target node are terminated, or until a configurable timeout\nis reached.  \nA PDB specifies the number of replicas that an application can tolerate having, relative to how\nmany it is intended to have.  For example, a Deployment which has a `.spec.replicas: 5` is\nsupposed to have 5 pods at any given time.  If its PDB allows for there to be 4 at a time,\nthen the Eviction API will allow voluntary disruption of one (but not two) pods at a time.  \nThe group of pods that comprise the application is specified using a label selector, the same\nas the one used by the application's controller (deployment, stateful-set, etc).  \nThe \"intended\" number of pods is computed from the `.spec.replicas` of the workload resource\nthat is managing those pods. The control plane discovers the owning workload resource by\nexamining the `.metadata.ownerReferences` of the Pod.  \n[Involuntary disruptions](#voluntary-and-involuntary-disruptions) cannot be prevented by PDBs; however they\ndo count against the budget.  \nPods which are deleted or unavailable due to a rolling upgrade to an application do count\nagainst the disruption budget, but workload resources (such as Deployment and StatefulSet)\nare not limited by PDBs when doing rolling upgrades. Instead, the handling of failures\nduring application updates is configured in the spec for the specific workload resource.  \nIt is recommended to set `AlwaysAllow` [Unhealthy Pod Eviction Policy](/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy)\nto your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain.\nThe default behavior is to wait for the application pods to become [healthy](/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod)\nbefore the drain can proceed.  \nWhen a pod is evicted using the eviction API, it is gracefully\n[terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination), honoring the\n`terminationGracePeriodSeconds` setting in its [PodSpec](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podspec-v1-core).\n"
  },
  {
    "question": "How does Kubernetes determine the QoS class for a Pod?",
    "answer": "Kubernetes determines the QoS class for a Pod based on the resource requests and limits of its component Containers.",
    "uuid": "70b34485-bcdf-4828-b047-262b36bcfc17",
    "question_with_context": "A user asked the following question:\nQuestion: How does Kubernetes determine the QoS class for a Pod?\nThis is about the following runbook:\nRunbook Title: Quality of Service classes\nRunbook Content: Quality of Service classesKubernetes classifies the Pods that you run and allocates each Pod into a specific\n_quality of service (QoS) class_. Kubernetes uses that classification to influence how different\npods are handled. Kubernetes does this classification based on the\n[resource requests](/docs/concepts/configuration/manage-resources-containers/)\nof the {{< glossary_tooltip text=\"Containers\" term_id=\"container\" >}} in that Pod, along with\nhow those requests relate to resource limits.\nThis is known as {{< glossary_tooltip text=\"Quality of Service\" term_id=\"qos-class\" >}}\n(QoS) class. Kubernetes assigns every Pod a QoS class based on the resource requests\nand limits of its component Containers. QoS classes are used by Kubernetes to decide\nwhich Pods to evict from a Node experiencing\n[Node Pressure](/docs/concepts/scheduling-eviction/node-pressure-eviction/). The possible\nQoS classes are `Guaranteed`, `Burstable`, and `BestEffort`. When a Node runs out of resources,\nKubernetes will first evict `BestEffort` Pods running on that Node, followed by `Burstable` and\nfinally `Guaranteed` Pods. When this eviction is due to resource pressure, only Pods exceeding\nresource requests are candidates for eviction.\n"
  },
  {
    "question": "What's the main difference between ReplicationController and ReplicaSet?",
    "answer": "ReplicaSets are the successors to ReplicationControllers, serving the same purpose but with ReplicaSets supporting set-based selector requirements, which ReplicationControllers do not.",
    "uuid": "411176d4-ec1e-4705-92d3-510e86344c18",
    "question_with_context": "A user asked the following question:\nQuestion: What's the main difference between ReplicationController and ReplicaSet?\nThis is about the following runbook:\nRunbook Title: ReplicationController\nRunbook Content: Alternatives to ReplicaSetReplicationControllerReplicaSets are the successors to [ReplicationControllers](/docs/concepts/workloads/controllers/replicationcontroller/).\nThe two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-based\nselector requirements as described in the [labels user guide](/docs/concepts/overview/working-with-objects/labels/#label-selectors).\nAs such, ReplicaSets are preferred over ReplicationControllers\n"
  },
  {
    "question": "Is there a way to get all the labels of a pod?",
    "answer": "Yes, you can get all of the pod's labels through a `downwardAPI` volume using `metadata.labels`, formatted as `label-key=\"escaped-label-value\"` with one label per line.",
    "uuid": "f0576ca8-efd3-4175-9fac-41df029ba1e4",
    "question_with_context": "A user asked the following question:\nQuestion: Is there a way to get all the labels of a pod?\nThis is about the following runbook:\nRunbook Title: Information available via `fieldRef` {#downwardapi-fieldRef}\nRunbook Content: Available fieldsInformation available via `fieldRef` {#downwardapi-fieldRef}For some Pod-level fields, you can provide them to a container either as\nan environment variable or using a `downwardAPI` volume. The fields available\nvia either mechanism are:  \n`metadata.name`\n: the pod's name  \n`metadata.namespace`\n: the pod's {{< glossary_tooltip text=\"namespace\" term_id=\"namespace\" >}}  \n`metadata.uid`\n: the pod's unique ID  \n`metadata.annotations['<KEY>']`\n: the value of the pod's {{< glossary_tooltip text=\"annotation\" term_id=\"annotation\" >}} named `<KEY>` (for example, `metadata.annotations['myannotation']`)  \n`metadata.labels['<KEY>']`\n: the text value of the pod's {{< glossary_tooltip text=\"label\" term_id=\"label\" >}} named `<KEY>` (for example, `metadata.labels['mylabel']`)  \nThe following information is available through environment variables\n**but not as a downwardAPI volume fieldRef**:  \n`spec.serviceAccountName`\n: the name of the pod's {{< glossary_tooltip text=\"service account\" term_id=\"service-account\" >}}  \n`spec.nodeName`\n: the name of the {{< glossary_tooltip term_id=\"node\" text=\"node\">}} where the Pod is executing  \n`status.hostIP`\n: the primary IP address of the node to which the Pod is assigned  \n`status.hostIPs`\n: the IP addresses is a dual-stack version of `status.hostIP`, the first is always the same as `status.hostIP`.  \n`status.podIP`\n: the pod's primary IP address (usually, its IPv4 address)  \n`status.podIPs`\n: the IP addresses is a dual-stack version of `status.podIP`, the first is always the same as `status.podIP`  \nThe following information is available through a `downwardAPI` volume\n`fieldRef`, **but not as environment variables**:  \n`metadata.labels`\n: all of the pod's labels, formatted as `label-key=\"escaped-label-value\"` with one label per line  \n`metadata.annotations`\n: all of the pod's annotations, formatted as `annotation-key=\"escaped-annotation-value\"` with one annotation per line\n"
  },
  {
    "question": "How should I name my Deployment to avoid issues with Pod hostnames?",
    "answer": "The name of a Deployment must be a valid DNS subdomain value, but for best compatibility, it should follow the more restrictive rules for a DNS label.",
    "uuid": "dee3bb7a-10d0-4d7f-9209-74c578904d5e",
    "question_with_context": "A user asked the following question:\nQuestion: How should I name my Deployment to avoid issues with Pod hostnames?\nThis is about the following runbook:\nRunbook Title: Writing a Deployment Spec\nRunbook Content: Writing a Deployment SpecAs with all other Kubernetes configs, a Deployment needs `.apiVersion`, `.kind`, and `.metadata` fields.\nFor general information about working with config files, see\n[deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/),\nconfiguring containers, and [using kubectl to manage resources](/docs/concepts/overview/working-with-objects/object-management/) documents.  \nWhen the control plane creates new Pods for a Deployment, the `.metadata.name` of the\nDeployment is part of the basis for naming those Pods. The name of a Deployment must be a valid\n[DNS subdomain](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names)\nvalue, but this can produce unexpected results for the Pod hostnames. For best compatibility,\nthe name should follow the more restrictive rules for a\n[DNS label](/docs/concepts/overview/working-with-objects/names#dns-label-names).  \nA Deployment also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).\n"
  },
  {
    "question": "Can I run multiple containers in a single Pod?",
    "answer": "Yes, a Pod can contain one or more application containers that are relatively tightly coupled and run in a shared context.",
    "uuid": "a39c0c31-b140-44c3-b649-76bed84a0747",
    "question_with_context": "A user asked the following question:\nQuestion: Can I run multiple containers in a single Pod?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- erictune\ntitle: Pods\napi_metadata:\n- apiVersion: \"v1\"\nkind: \"Pod\"\ncontent_type: concept\nweight: 10\nno_list: true\n---  \n<!-- overview -->  \n_Pods_ are the smallest deployable units of computing that you can create and manage in Kubernetes.  \nA _Pod_ (as in a pod of whales or pea pod) is a group of one or more\n{{< glossary_tooltip text=\"containers\" term_id=\"container\" >}}, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and\nco-scheduled, and run in a shared context. A Pod models an\napplication-specific \"logical host\": it contains one or more application\ncontainers which are relatively tightly coupled.\nIn non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.  \nAs well as application containers, a Pod can contain\n{{< glossary_tooltip text=\"init containers\" term_id=\"init-container\" >}} that run\nduring Pod startup. You can also inject\n{{< glossary_tooltip text=\"ephemeral containers\" term_id=\"ephemeral-container\" >}}\nfor debugging a running Pod.  \n<!-- body -->\n"
  },
  {
    "question": "How can I tell if my Kubernetes Deployment is progressing?",
    "answer": "You can tell if your Kubernetes Deployment is progressing when it creates a new ReplicaSet, scales up its newest ReplicaSet, scales down its older ReplicaSet(s), or when new Pods become ready or available for at least the specified MinReadySeconds.",
    "uuid": "792a42a4-df8b-464e-a996-78dff23c1c9f",
    "question_with_context": "A user asked the following question:\nQuestion: How can I tell if my Kubernetes Deployment is progressing?\nThis is about the following runbook:\nRunbook Title: Progressing Deployment\nRunbook Content: Deployment statusProgressing DeploymentKubernetes marks a Deployment as _progressing_ when one of the following tasks is performed:  \n* The Deployment creates a new ReplicaSet.\n* The Deployment is scaling up its newest ReplicaSet.\n* The Deployment is scaling down its older ReplicaSet(s).\n* New Pods become ready or available (ready for at least [MinReadySeconds](#min-ready-seconds)).  \nWhen the rollout becomes \u201cprogressing\u201d, the Deployment controller adds a condition with the following\nattributes to the Deployment's `.status.conditions`:  \n* `type: Progressing`\n* `status: \"True\"`\n* `reason: NewReplicaSetCreated` | `reason: FoundNewReplicaSet` | `reason: ReplicaSetUpdated`  \nYou can monitor the progress for a Deployment by using `kubectl rollout status`.\n"
  },
  {
    "question": "What happens if I delete a DaemonSet?",
    "answer": "Deleting a DaemonSet will clean up the Pods it created.",
    "uuid": "67620104-81bb-4054-a69a-5df93a8635f0",
    "question_with_context": "A user asked the following question:\nQuestion: What happens if I delete a DaemonSet?\nThis is about the following runbook:\nRunbook Title: \nRunbook Content: ---\nreviewers:\n- enisoc\n- erictune\n- foxish\n- janetkuo\n- kow3ns\ntitle: DaemonSet\napi_metadata:\n- apiVersion: \"apps/v1\"\nkind: \"DaemonSet\"\ndescription: >-\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.\ncontent_type: concept\nweight: 40\nhide_summary: true # Listed separately in section index\n---  \n<!-- overview -->  \nA _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the\ncluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage\ncollected.  Deleting a DaemonSet will clean up the Pods it created.  \nSome typical uses of a DaemonSet are:  \n- running a cluster storage daemon on every node\n- running a logs collection daemon on every node\n- running a node monitoring daemon on every node  \nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.\nA more complex setup might use multiple DaemonSets for a single type of daemon, but with\ndifferent flags and/or different memory and cpu requests for different hardware types.  \n<!-- body -->\n"
  }
]